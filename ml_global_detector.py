"""
ml_global_detector.py
ML-–¥–µ—Ç–µ–∫—Ç–æ—Ä —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LightGBM –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (5m)

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ:
- –í –ø–∞–∫–µ—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (trainer v2.1.1): –æ–∫–Ω–æ lookback √ó base_features, scaler, decision_policy (tau/delta/cooldown)
- –í legacy-—Ñ–æ—Ä–º–∞—Ç–µ (raw Booster): –æ–¥–Ω–æ-–±–∞—Ä—Ç–æ–≤—ã–π –≤—Ö–æ–¥ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)

–ö–ª–∞—Å—Å—ã:
- 0: FLAT (–Ω–µ—Ç —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞)
- 1: BUY reversal (—Ä–∞–∑–≤–æ—Ä–æ—Ç –≤–≤–µ—Ä—Ö)
- 2: SELL reversal (—Ä–∞–∑–≤–æ—Ä–æ—Ç –≤–Ω–∏–∑)
"""

from typing import Dict, Optional, Any, List
import numpy as np
import pandas as pd
from datetime import datetime
import os
import logging
from datetime import UTC
import lightgbm as lgb
import joblib

from iqts_standards import (
    DetectorSignal, Detector,
    normalize_signal, Timeframe
)

class MLGlobalDetector(Detector):
    """
    ML-–¥–µ—Ç–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LightGBM –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (5m)

    –†–∞–±–æ—Ç–∞–µ—Ç —Å –æ–∫–Ω–æ–º –∏—Å—Ç–æ—Ä–∏–∏ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ), —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º —Å –æ–±—É—á–µ–Ω–∏–µ–º:
    –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ [t0, t-1, ..., t-(lookback-1)] –ø–æ —Å–ø–∏—Å–∫—É base_feature_names.
    """

    def __init__(self, timeframe: Timeframe = "5m",
                 model_path: str = 'models/ml_global_5m_lgbm.joblib',
                 use_fallback: bool = False,
                 name: str = None, use_scaler: Optional[bool] = None):

        super().__init__(name or f"ml_global_{timeframe}")

        abs_path = os.path.abspath(model_path)
        self.logger.setLevel(logging.INFO)
        self.last_confidence = None
        self.timeframe = timeframe
        self.use_fallback = use_fallback
        self.model_path = model_path

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –º–æ–¥–µ–ª–∏
        self.model: Optional[lgb.Booster] = None
        self.use_scaler = use_scaler

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–∞–∫–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ (–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
        self.base_feature_names: List[str] = [
            'cmo_14', 'volume', 'trend_acceleration_ema7', 'regime_volatility',
            'bb_width', 'adx_14', 'plus_di_14', 'minus_di_14', 'atr_14_normalized',
            'volume_ratio_ema3', 'candle_relative_body', 'upper_shadow_ratio',
            'lower_shadow_ratio', 'price_vs_vwap', 'bb_position',
            'cusum_1m_quality_score', 'cusum_1m_trend_aligned', 'cusum_1m_price_move',
            'is_trend_pattern_1m', 'body_to_range_ratio_1m', 'close_position_in_range_1m'
        ]
        # lookback –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–±—É–¥–µ—Ç –∑–∞–º–µ–Ω—ë–Ω –∏–∑ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞)
        self.lookback: int = 1
        # –ü–æ–ª–Ω—ã–µ –∏–º–µ–Ω–∞ windowed-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏–∑ base_feature_names –∏ lookback)
        self.feature_names: List[str] = self._generate_windowed_feature_names()

        # –ü–æ—Ä–æ–≥ –∏ warmup ‚Äî –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª—å–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞
        self.min_confidence = 0.53
        self.scaler = None
        self.required_warmup = 20  # –æ–±—â–∏–π —Ç—ë–ø–ª—ã–π —Å—Ç–∞—Ä—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ, —á–µ–º lookback)

        # Decision policy (–∏–∑ trainer): tau/delta/cooldown/bars_per_day
        self.decision_policy: Optional[Dict[str, Any]] = None
        self._last_signal_ts: Optional[int] = None  # –¥–ª—è cooldown (ts –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.model_metadata = {
            'version': 'unknown',
            'instrument': 'ETH/USDT',
            'exchange': 'Binance',
            'timeframe': timeframe,
            'feature_count': len(self.feature_names),
            'trained_at': None,
            'training_samples': None,
            'val_accuracy': None
        }

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –ü–†–û–°–¢–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ë–ï–ó –†–ï–ö–£–†–°–ò–ò
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if model_path and os.path.exists(abs_path):
            try:
                self.load_model(abs_path)
                self.logger.info(f"‚úÖ ML –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {abs_path}")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                if not use_fallback:
                    raise
                else:
                    self.logger.warning("üîÑ –†–µ–∂–∏–º fallback –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        else:
            self.logger.error(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {abs_path}")
            if not use_fallback:
                raise FileNotFoundError(f"Model file not found: {abs_path}")
            else:
                self.logger.warning("üîÑ –†–µ–∂–∏–º fallback –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _generate_windowed_feature_names(self) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–∫–Ω–∞ lookback –≤ –ø–æ—Ä—è–¥–∫–µ:
        [feat_t0 for all base], [feat_t-1 for all base], ..., [feat_t-(lookback-1) ...]
        –°–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ—Ä—è–¥–∫–æ–º –≤ trainer (window[::-1].ravel()).
        """
        names: List[str] = []
        if self.lookback <= 1:
            # –±–µ–∑ –æ–∫–Ω–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –±–∞—Ä
            names = [f"{feat}_t0" for feat in self.base_feature_names]
        else:
            # t0 ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä –æ–∫–Ω–∞ (—Å–∞–º–∞—è —Å–≤–µ–∂–∞—è —Å–≤–µ—á–∞)
            for feat in self.base_feature_names:
                names.append(f"{feat}_t0")
            # t-1 ... t-(lookback-1)
            for lag in range(1, self.lookback):
                for feat in self.base_feature_names:
                    names.append(f"{feat}_t-{lag}")
        return names

    def _validate_features(self, features: np.ndarray) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç NaN –∏–ª–∏ Inf."""
        if features is None:
            self.logger.warning("[VALIDATOR] Features array is None")
            return False
        has_nan = np.isnan(features).any()
        has_inf = np.isinf(features).any()
        return not (has_nan or has_inf)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    def extract_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏:
        - –ü–∞–∫–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å (—Å –æ–∫–Ω–∞–º–∏): —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–∫–Ω–æ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö lookback –±–∞—Ä–æ–≤ –∏ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä [t0, t-1, ...]
        - Legacy-–º–æ–¥–µ–ª—å: –±–µ—Ä—ë—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–∫–æ–ª—å–∫–æ –±–∞—Ä–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –æ–∫–Ω–∞
        min_bars = max(1, self.lookback)
        if len(df) < min_bars:
            raise ValueError(f"Insufficient bars for window: need {min_bars}, got {len(df)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ –∫–æ–ª–æ–Ω–∫–∞–º df)
        missing_features = [col for col in self.base_feature_names if col not in df.columns]
        available_features = [col for col in self.base_feature_names if col in df.columns]
        if missing_features:
            self.logger.error(f"‚ùå MISSING FEATURES ({len(missing_features)}): {missing_features}")
            self.logger.info(f"‚úÖ AVAILABLE FEATURES ({len(available_features)}): {available_features}")
            # –ü–æ–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∏—á
            for feature in available_features[:5]:
                sample_value = df[feature].iloc[-1] if len(df) > 0 else "N/A"
                self.logger.info(f"   {feature}: {sample_value}")
            raise ValueError(f"Missing ML features: {missing_features}")

        # –û–∫–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º (lookback > 1) ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º (lookback, n_features) ‚Üí —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ [t0, t-1, ...]
        if self.lookback > 1:
            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback —Å—Ç—Ä–æ–∫ –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
            tail = df.iloc[-self.lookback:]
            # –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —Å—Ç—Ä–æ–∫–∏ ‚Äî –±–∞—Ä—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–æ—Ç —Å—Ç–∞—Ä–æ–≥–æ –∫ –Ω–æ–≤–æ–º—É), —Å—Ç–æ–ª–±—Ü—ã ‚Äî base_features
            window = tail[self.base_feature_names].to_numpy(dtype=float)  # shape: (lookback, n_features)
            # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∑–∞–º–µ–Ω–∏–º NaN/Inf
            window = np.nan_to_num(window, nan=0.0, posinf=0.0, neginf=0.0)

            # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–º —à—ë–ª t0 (–ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä –æ–∫–Ω–∞), –∑–∞—Ç–µ–º t-1, ... ‚Äî –∫–∞–∫ –≤ trainer
            window_ordered = window[::-1, :]  # shape: (lookback, n_features), –ø–µ—Ä–≤—ã–π —Ä—è–¥ ‚Äî t0
            # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –≤–µ–∫—Ç–æ—Ä: [t0_feat1..featN, t-1_feat1..featN, ...]
            features_array = window_ordered.reshape(1, -1).astype(np.float32)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not self._validate_features(features_array):
                self.logger.warning("Features contain NaN/Inf, cleaning...")
                features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)

            self.logger.info(f"‚úÖ ML FEATURE DIAGNOSTIC (windowed) - OK | "
                             f"window_shape={window.shape}, vector_dim={features_array.shape[1]}")
            return features_array

        # –ò–Ω–∞—á–µ ‚Äî legacy —Ä–µ–∂–∏–º (–æ–¥–∏–Ω –±–∞—Ä)
        features = []
        for feature_name in self.base_feature_names:
            value = df[feature_name].iloc[-1]
            if pd.isna(value):
                self.logger.warning(f"Feature '{feature_name}' is NaN, replacing with 0.0")
                value = 0.0
            features.append(float(value))
        features_array = np.array(features, dtype=float).reshape(1, -1)

        if not self._validate_features(features_array):
            self.logger.warning("Features contain NaN/Inf, cleaning...")
            features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)

        self.logger.info("‚úÖ ML FEATURE DIAGNOSTIC (legacy) - OK")
        return features_array

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –ê–ù–ê–õ–ò–ó–ê
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    async def analyze(self, data: Dict[Timeframe, pd.DataFrame]) -> DetectorSignal:
        """
        –ò–Ω—Ñ–µ—Ä–µ–Ω—Å LightGBM –ø–æ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–∫–æ–Ω).
        """
        self.logger.info(f"üîÑ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º LightGBM ")
        # 1) –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤—Ö–æ–¥–∞
        if not data or not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid data structure: {type(data)}")
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "invalid_data_structure",
                "metadata": {"detector": "ml", "timeframe": self.timeframe}
            })

        # 2) –ù–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω–æ–≥–æ –¢–§
        if self.timeframe not in data:
            self.logger.error(f"‚ùå Missing timeframe {self.timeframe} in data. Available: {list(data.keys())}")
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "missing_timeframe",
                "metadata": {"detector": "ml", "missing_tf": self.timeframe, "available_tfs": list(data.keys())}
            })

        df = data[self.timeframe]

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ DataFrame
        self.logger.info(f"üîç ML DETECTOR DIAGNOSTIC:")
        self.logger.info(f"  DataFrame shape: {df.shape}")
        self.logger.info(f"  Columns (first 15): {df.columns.tolist()[:15]}")
        last_ts = None
        if 'ts' in df.columns:
            last_ts = int(df['ts'].iloc[-1])
            self.logger.info(f"  last ts: {last_ts}")
        elif 'timestamp' in df.columns:
            last_ts = int(df['timestamp'].iloc[-1])
            self.logger.info(f"  last timestamp: {last_ts}")

        # 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if df.empty:
            self.logger.error(f"‚ùå DataFrame for {self.timeframe} is empty")
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "empty_dataframe",
                "metadata": {"detector": "ml", "timeframe": self.timeframe}
            })

        # 4) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ (ts -> timestamp) ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        if 'ts' in df.columns and 'timestamp' not in df.columns:
            df = df.rename(columns={'ts': 'timestamp'})
            data[self.timeframe] = df  # –æ–±–Ω–æ–≤–ª—è–µ–º –≤ data
            last_ts = int(df['timestamp'].iloc[-1])

        # 5) –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ OHLCV
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            self.logger.error(f"‚ùå Missing required columns: {missing_cols}")
            return normalize_signal({
                "ok": False,
                "direction": 0,
                "confidence": 0.0,
                "reason": "missing_required_columns",
                "metadata": {"detector": "ml", "missing_cols": missing_cols}
            })

        # 6) Warmup: –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º lookback –±–∞—Ä–æ–≤
        min_bars = max(self.required_warmup, self.lookback)
        if len(df) < min_bars:
            self.logger.warning(f"‚ö†Ô∏è Insufficient data: {len(df)} < {min_bars} "
                                f"(required_warmup={self.required_warmup}, lookback={self.lookback})")
            return normalize_signal({
                "ok": False,
                "direction": 0,
                "confidence": 0.0,
                "reason": "insufficient_warmup",
                "metadata": {
                    "detector": "ml",
                    "required": int(min_bars),
                    "actual": int(len(df))
                }
            })
        else:
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞
            self.logger.info(f"üéØ Starting ML analysis: {len(df)} candles available (last={last_ts})")

        # 7) –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞?
        if self.model is None:
            self.logger.error("‚ùå Model not loaded! Call load_model() first.")
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "model_not_loaded",
                "metadata": {"detector": "ml"}
            })

        self.logger.info(f"‚úÖ All basic validations passed for {self.timeframe}")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            X = self.extract_features(df)  # shape: (1, lookback * n_features) –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        except Exception as e:
            self.logger.error(f"‚ùå Feature extraction failed: {e}", exc_info=True)
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "feature_extraction_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            if self.use_scaler and self.scaler is not None:
                X_scaled = self.scaler.transform(X)
                self.logger.debug("üîç Using StandardScaler")
            else:
                X_scaled = X
                self.logger.debug("üîç Using RAW features")
        except Exception as e:
            self.logger.error(f"‚ùå Feature scaling failed: {e}")
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "scaling_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ò –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ü–û–õ–ò–¢–ò–ö–ò –ü–û–†–û–ì–û–í (tau/delta/cooldown)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            probabilities = self.model.predict(X_scaled)[0]  # [p0, p1, p2]
            flat_p, buy_p, sell_p = float(probabilities[0]), float(probabilities[1]), float(probabilities[2])

            # –ë–∞–∑–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            prediction_idx = int(np.argmax(probabilities))
            predicted_class_confidence = float(probabilities[prediction_idx])
            direction_map = {0: 0, 1: 1, 2: -1}
            predicted_direction = direction_map.get(prediction_idx, 0)

            # ‚úÖ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ü–ï–†–ï–ú–ï–ù–ù–´–• –î–û –£–°–õ–û–í–ò–ô
            ok = False
            reason = "no_trend_signal"

            policy = self.decision_policy or self.model_metadata.get("decision_policy")
            if policy:
                tau = float(policy.get("tau", 0.5))
                delta = float(policy.get("delta", 0.08))
                cooldown_bars = int(policy.get("cooldown_bars", 0))

                # –û–Ω-–∞–∫—Ç –ª–æ–≥–∏–∫–∞ –∫–∞–∫ –≤ trainer: maxp –∏ margin
                maxp = max(buy_p, sell_p)
                margin = abs(buy_p - sell_p)

                act = (maxp >= tau) and (margin >= delta)

                # ‚úÖ –û–ü–†–ï–î–ï–õ–Ø–ï–ú ok –ò reason –ù–ê –û–°–ù–û–í–ï act
                if predicted_direction == 0:
                    ok = True
                    reason = "no_trend_signal"
                else:
                    ok = act
                    if ok:
                        reason = "trend_confirmed"
                    elif reason == "cooldown_active":
                        # –£–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ
                        pass
                    else:
                        reason = "weak_trend_signal"  # ‚úÖ –í–∞–ª–∏–¥–Ω—ã–π ReasonCode

                # –ï—Å–ª–∏ —Å—Ä–∞–±–æ—Ç–∞–ª —Å–∏–≥–Ω–∞–ª ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º ts
                if ok and predicted_direction != 0 and last_ts is not None:
                    self._last_signal_ts = last_ts

            else:
                # Fallback: –±–µ–∑ –ø–æ–ª–∏—Ç–∏–∫–∏ ‚Äî –ø—Ä–æ—Å—Ç–æ–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                if predicted_direction == 0:
                    ok = True
                    reason = "no_trend_signal"
                else:
                    ok = (predicted_class_confidence >= self.min_confidence)
                    reason = "trend_confirmed" if ok else "weak_trend_signal"

            self.last_confidence = predicted_class_confidence

            self.logger.info(
                f"üîÑ ML —Ä–µ–∑—É–ª—å—Ç–∞—Ç: dir={predicted_direction} | conf={predicted_class_confidence:.3f} | "
                f"BUY={buy_p:.3f} | SELL={sell_p:.3f} | FLAT={flat_p:.3f} | "
                f"policy={'on' if policy else 'off'} | ok={ok} | reason={reason}"
            )

            return normalize_signal({
                "ok": ok,
                "direction": predicted_direction,
                "confidence": predicted_class_confidence,
                "reason": reason,
                "metadata": {
                    "detector": "ml",
                    "timeframe": self.timeframe,
                    "probabilities": {"FLAT": flat_p, "BUY": buy_p, "SELL": sell_p},
                    "predicted_class_confidence": predicted_class_confidence,
                    "feature_count": int(X.shape[1]),
                    "model_version": self.model_metadata.get("version", "unknown"),
                    "lookback": int(self.lookback),
                    "base_feature_count": int(len(self.base_feature_names)),
                    "decision_policy": policy or {},
                }
            })

        except Exception as e:
            self.logger.error(f"‚ùå Prediction failed: {e}", exc_info=True)
            return normalize_signal({
                "ok": False,
                "direction": 0,  # FLAT
                "confidence": 0.0,
                "reason": "prediction_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

    def load_model(self, path: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ trainer'–∞ (v2.1.1)
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")

        try:
            self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {path}...")
            loaded_data = joblib.load(path)

            # –°–û–í–†–ï–ú–ï–ù–ù–´–ô –§–û–†–ú–ê–¢ (–∏–∑ trainer)
            if isinstance(loaded_data, dict):

                self.model = loaded_data.get("model")
                if self.model is None:
                    raise ValueError("Dictionary does not contain 'model' key")

                self.scaler = loaded_data.get("scaler")
                self.model_metadata = loaded_data.get("metadata", {})

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –º–æ–¥–µ–ª—å–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞
                self.timeframe = loaded_data.get("timeframe", self.timeframe)
                self.min_confidence = loaded_data.get("min_confidence", self.min_confidence)
                self.required_warmup = loaded_data.get("required_warmup", self.required_warmup)

                # Decision policy
                self.decision_policy = self.model_metadata.get("decision_policy")

                # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –æ–∫–Ω–æ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –æ–∫–æ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞)
                self.base_feature_names = loaded_data.get("base_feature_names", self.base_feature_names)
                self.lookback = int(loaded_data.get("lookback", max(1, self.lookback)))
                # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ –∏–º–µ–Ω–∞ –æ–∫–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
                self.feature_names = self._generate_windowed_feature_names()

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∫–µ–π–ª–µ—Ä–∞
                scaler_used = self.model_metadata.get("scaler_used", False)
                if hasattr(self, "use_scaler") and getattr(self, "use_scaler") is None:
                    self.use_scaler = scaler_used

                self.logger.info(
                    f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: timeframe={self.timeframe}, "
                    f"scaler={'‚úì' if self.scaler else '‚úó'}, "
                    f"lookback={self.lookback}, base_features={len(self.base_feature_names)}, "
                    f"vector_dim={len(self.feature_names)}, "
                    f"policy={'‚úì' if self.decision_policy else '‚úó'}"
                )

                # –í–ê–õ–ò–î–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
                if not isinstance(self.model, lgb.Booster):
                    raise TypeError(f"Model must be lgb.Booster, got {type(self.model).__name__}")

            # LEGACY –§–û–†–ú–ê–¢ (–±–µ–∑ –æ–∫–Ω–∞, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
            elif isinstance(loaded_data, lgb.Booster):
                self.model = loaded_data
                self.scaler = None
                self.model_metadata = {
                    "version": "legacy",
                    "loaded_at": datetime.now(UTC).isoformat(),
                    "format": "raw_booster",
                    "scaler_used": False,
                }
                self.use_scaler = False
                self.lookback = 1
                self.feature_names = self._generate_windowed_feature_names()
                self.decision_policy = None
                self.logger.info("‚úÖ Legacy –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (RAW features, single-bar)")

            else:
                raise TypeError(f"Unsupported model format: {type(loaded_data)}")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            raise

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ú–ï–¢–û–î–´ –ò–ù–¢–ï–†–§–ï–ô–°–ê DETECTOR
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_required_bars(self) -> Dict[str, int]:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—É—á–∏—Ç—ã–≤–∞–µ—Ç lookback)."""
        return {self.timeframe: max(int(self.required_warmup), int(self.lookback))}