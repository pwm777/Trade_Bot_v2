# train_ml_global_v2_windowed.py
"""
–û–±—É—á–µ–Ω–∏–µ LightGBM –º–æ–¥–µ–ª–∏ —Å –û–ö–ù–û–ú –ò–°–¢–û–†–ò–ò (30 –±–∞—Ä–æ–≤)
–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É: –º–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –≤–∏–¥–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞—Ä–æ–≤, –∞ –Ω–µ –æ–¥–∏–Ω –±–∞—Ä

–ê–≤—Ç–æ—Ä: pwm777
–î–∞—Ç–∞: 2025-11-17
–í–µ—Ä—Å–∏—è: 2.1 (windowed training)

–ò–∑–º–µ–Ω–µ–Ω–∏—è:
- –î–æ–±–∞–≤–ª–µ–Ω lookback window = 30 –±–∞—Ä–æ–≤
- –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä = –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –±–∞—Ä–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏
- 22 –ø—Ä–∏–∑–Ω–∞–∫–∞ √ó 30 –±–∞—Ä–æ–≤ = 660 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—Ö–æ–¥
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –í–°–Ø —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑ v2.0: tau tuning, diagnostics, plots
"""

import sys
import logging
from sqlalchemy import create_engine, text
from datetime import datetime
import json
from typing import Tuple
import warnings
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import joblib
from pathlib import Path

warnings.filterwarnings('ignore')
import re
import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize, StandardScaler
from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LOOKBACK_WINDOW = 11  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
TIMEFRAME_TO_BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –°–ü–ò–°–û–ö –ë–ê–ó–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í (–∏–∑ –æ–¥–Ω–æ–≥–æ –±–∞—Ä–∞)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BASE_FEATURE_NAMES = [
    'cmo_14',
    'volume',
    'trend_acceleration_ema7',
    'regime_volatility',
    'bb_width',
    'adx_14',
    'plus_di_14',
    'minus_di_14',
    'atr_14_normalized',
    'volume_ratio_ema3',
    'candle_relative_body',
    'upper_shadow_ratio',
    'lower_shadow_ratio',
    'price_vs_vwap',
    'bb_position',
    #'cusum_1m_recent',
    'cusum_1m_quality_score',
    'cusum_1m_trend_aligned',
    'cusum_1m_price_move',
    'is_trend_pattern_1m',
    'body_to_range_ratio_1m',
    'close_position_in_range_1m'
]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –£–¢–ò–õ–ò–¢–´
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _infer_bars_per_day_from_run_id(run_id: str, default: int = 288) -> int:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑ run_id —Ñ–æ—Ä–º–∞—Ç–∞ ..._<tf>_...
    –∏ –≤–µ—Ä–Ω—É—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –≤ —Å—É—Ç–∫–∞—Ö.
    """
    m = re.search(r"_(\d+[mh])_", str(run_id).lower())
    if not m:
        return default
    tf = m.group(1)
    TIMEFRAME_TO_BARS_LOCAL = {
        "1m": 1440,
        "3m": 480,
        "5m": 288,
        "15m": 96,
        "30m": 48,
        "1h": 24,
        "2h": 12,
        "4h": 6,
        "6h": 4,
        "12h": 2,
        "1d": 1,
    }
    return TIMEFRAME_TO_BARS_LOCAL.get(tf, default)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–û–õ–õ–ë–≠–ö ¬´–¢–ï–†–ú–û–ú–ï–¢–† –ü–†–û–ì–†–ï–°–°–ê¬ª –î–õ–Ø LIGHTGBM
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def thermometer_progress_callback(logger: logging.Logger, width: int = 30, period: int = 10):
    """–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º –±—É—Å—Ç–∏–Ω–≥–∞"""
    import sys

    def _cb(env):
        begin = getattr(env, 'begin_iteration', 0) or 0
        end = getattr(env, 'end_iteration', None)
        if end is None or end <= begin:
            end = begin + int(env.params.get('num_boost_round', 0) or 0)
            if end <= begin:
                end = begin + 1

        total = max(1, end - begin)
        iter_now = int(getattr(env, 'iteration', 0) or 0)
        done = max(0, iter_now - begin + 1)
        pct = min(1.0, max(0.0, done / total))
        filled = int(round(pct * width))
        bar = '‚ñà' * filled + '‚ñë' * (width - filled)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_metric_name = None
        val_metric_value = None
        evals = getattr(env, 'evaluation_result_list', None)
        if evals:
            for res in evals:
                if isinstance(res, (list, tuple)) and len(res) >= 3:
                    data_name, metric_name, metric_val = res[0], res[1], res[2]
                    if str(data_name).startswith(('valid', 'val')):
                        val_metric_name = str(metric_name)
                        try:
                            val_metric_value = float(metric_val)
                        except Exception:
                            val_metric_value = None
                        break

        should_print = (iter_now > 0 and iter_now % period == 0) or (done >= total)

        if should_print:
            if val_metric_name is not None and val_metric_value is not None:
                msg = f"[{iter_now:4d}/{total}] {val_metric_name}:{val_metric_value:.5f} | {bar} {int(pct * 100):3d}%"
            else:
                msg = f"[{iter_now:4d}/{total}] {bar} {int(pct * 100):3d}%"

            if done >= total:
                print(f"\r{msg}")
                sys.stdout.flush()
            else:
                print(f"\r{msg}", end='', flush=True)

    return _cb


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–õ–ê–°–° –î–õ–Ø –†–ê–ë–û–¢–´ –° –ë–ê–ó–û–ô –î–ê–ù–ù–´–•
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class DataLoader:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –±–∞–∑—ã ml_labeling_tool_v3.py"""

    def __init__(self, db_dsn: str = MARKET_DB_DSN, symbol: str = "ETHUSDT"):
        self.db_dsn = db_dsn
        self.db_path = DATA_DIR / "market_data.sqlite"
        self.symbol = symbol
        self.engine = None

    def connect(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.db_path}")
        self.engine = create_engine(self.db_dsn)
        logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î: {self.db_path}")

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.engine:
            self.engine.dispose()
            logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

    def load_market_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ candles_5m"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM candles_5m 
            WHERE symbol = :symbol 
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"symbol": self.symbol})

        if df.empty:
            raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        if 'ts' in df.columns and 'datetime' not in df.columns:
            df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π –∏–∑ candles_5m")
        return df

    def load_training_dataset(self, run_id: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ training_dataset"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM training_dataset
            WHERE run_id = :run_id
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"run_id": run_id})

        if df.empty:
            raise ValueError(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è run_id={run_id}")

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ training_dataset")
        logger.info(f"   –ö–ª–∞—Å—Å—ã: {df['reversal_label'].value_counts().to_dict()}")
        return df


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° ModelTrainer –° –û–ö–ù–û–ú –ò–°–¢–û–†–ò–ò
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ModelTrainer:
    def __init__(self, db_dsn: str, symbol: str, lookback: int = LOOKBACK_WINDOW):
        self.db_dsn = db_dsn
        self.symbol = symbol
        self.lookback = lookback
        self.timeframe = "5m"  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.data_loader = DataLoader(db_dsn, symbol)
        self.base_feature_names = BASE_FEATURE_NAMES

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ª–∞–≥–∞–º–∏
        self.feature_names = self._generate_windowed_feature_names()
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ "
                    f"({len(self.base_feature_names)} √ó {lookback} –±–∞—Ä–æ–≤)")

    def _generate_windowed_feature_names(self) -> list:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ª–∞–≥–æ–≤
        –ù–∞–ø—Ä–∏–º–µ—Ä: cmo_14_t0, cmo_14_t-1, ..., cmo_14_t-29
        """
        names = []
        # t0 - —Ç–µ–∫—É—â–∏–π –±–∞—Ä (—Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π)
        for feat in self.base_feature_names:
            names.append(f"{feat}_t0")

        # t-1, t-2, ..., t-(lookback-1) - –∏—Å—Ç–æ—Ä–∏—è
        for lag in range(1, self.lookback):
            for feat in self.base_feature_names:
                names.append(f"{feat}_t-{lag}")

        return names

    def prepare_training_data(self, run_id: str) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–∫–Ω–æ–º –∏—Å—Ç–æ—Ä–∏–∏ (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é numpy –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        """
        df = self.data_loader.load_training_dataset(run_id)

        logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –∏—Å—Ç–æ—Ä–∏–∏ (lookback={self.lookback})...")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å 3 –°–†–ê–ó–£
        df_filtered = df[df['reversal_label'] != 3].copy()
        logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df) - len(df_filtered)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º 3")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        feature_matrix = df_filtered[self.base_feature_names].values  # shape: (n, 22)
        labels = df_filtered['reversal_label'].values
        weights = df_filtered['sample_weight'].values

        n_samples = len(df_filtered)
        n_features = len(self.base_feature_names)

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π)
        n_valid = n_samples - (self.lookback - 1)

        if n_valid <= 0:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è lookback={self.lookback}")

        logger.info(f"   –°–æ–∑–¥–∞–Ω–∏–µ {n_valid} –æ–∫–æ–Ω –∏–∑ {n_samples} –æ–±—Ä–∞–∑—Ü–æ–≤...")

        # –ü—Ä–µ–¥–∞–ª–ª–æ–∫–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        # Shape: (n_valid, lookback * n_features)
        X_windowed = np.zeros((n_valid, self.lookback * n_features), dtype=np.float32)

        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
        for i in range(n_valid):
            start_idx = i
            end_idx = i + self.lookback

            # –ë–µ—Ä—ë–º –æ–∫–Ω–æ [start_idx:end_idx] –∏ "—Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º" –≤ 1D
            window = feature_matrix[start_idx:end_idx, :]  # shape: (lookback, n_features)

            # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–∏: —Å–Ω–∞—á–∞–ª–∞ —Ç–µ–∫—É—â–∏–π –±–∞—Ä (t0), –ø–æ—Ç–æ–º –ª–∞–≥–∏
            # t0 –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–º –±–∞—Ä–æ–º –æ–∫–Ω–∞
            window_reversed = window[::-1]  # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º: [t-29, t-28, ..., t0]

            # Flatten –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ: t0_feat1, t0_feat2, ..., t-1_feat1, ...
            X_windowed[i] = window_reversed.ravel()

        # –ú–µ—Ç–∫–∏ –∏ –≤–µ—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ü–û–°–õ–ï–î–ù–ï–ú–£ –±–∞—Ä—É –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
        y_windowed = labels[self.lookback - 1:]
        w_windowed = weights[self.lookback - 1:]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        X_df = pd.DataFrame(X_windowed, columns=self.feature_names)
        y_series = pd.Series(y_windowed, name='label')
        w_series = pd.Series(w_windowed, name='weight')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing = X_df.isnull().sum()
        if missing.any():
            logger.warning(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:\n{missing[missing > 0].head(10)}")
            logger.warning(f"   –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏...")
            X_df = X_df.fillna(0)

        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(X_df)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y_series.value_counts().to_dict()}")
        logger.info(f"   –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {self.lookback} –±–∞—Ä–æ–≤")
        logger.info(f"   –ë–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.base_feature_names)}")

        return X_df, y_series, w_series

    def tune_tau_for_spd_range(
            self,
            y_val: np.ndarray,
            proba: np.ndarray,  # shape (n,3) ‚Äî [HOLD, BUY, SELL]
            bars_per_day: int,
            spd_min: float = 30.0,
            spd_max: float = 50.0,
            precision_min: float = 0.60,
            delta: float = 0.08,
            cooldown_bars: int = 2,
    ):
        """
        –ü–æ–¥–±–∏—Ä–∞–µ—Ç tau (–ø–æ—Ä–æ–≥ max(p_buy,p_sell)) —Ç–∞–∫, —á—Ç–æ–±—ã SPD –±—ã–ª –≤ [spd_min, spd_max].
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (tau, stats_dict). –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä—ë—Ç tau —Å SPD –±–ª–∏–∂–∞–π—à–∏–º –∫ —Ü–µ–Ω—Ç—Ä—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞.
        """
        # –ø—Ä–µ–¥—Ä–∞—Å—á—ë—Ç—ã
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        taus = np.quantile(maxp, np.linspace(0.30, 0.95, 48))

        best_in = None  # (cand, key)  ‚Äî –ª—É—á—à–∏–π –≤–Ω—É—Ç—Ä–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        best_near = None  # (cand, keyn) ‚Äî –±–ª–∏–∂–∞–π—à–∏–π –∫ —Ü–µ–Ω—Ç—Ä—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        target = 0.5 * (spd_min + spd_max)
        n = len(y_val)

        for tau_cand in sorted(taus):
            # –µ–¥–∏–Ω—ã–π —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫/act —á–µ—Ä–µ–∑ helper
            stats = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(tau_cand),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd = stats['spd']
            prec = stats['precision_macro_buy_sell']
            rec = stats['recall_macro_buy_sell']
            f1 = stats['f1_macro_buy_sell']
            # –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ SPD (–ø–æ—Å–ª–µ cooldown)
            signals = int(round(spd * max(1, n) / max(1, bars_per_day)))

            cand = (float(tau_cand), float(spd), float(prec), float(rec), float(f1), int(signals))

            # –≤–Ω—É—Ç—Ä–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º precision, –∑–∞—Ç–µ–º F1, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ
            if spd_min <= spd <= spd_max and prec >= precision_min:
                key = (prec, f1, -abs(spd - target), float(tau_cand))
                if (best_in is None) or (key > best_in[1]):
                    best_in = (cand, key)

            # –±–ª–∏–∂–∞–π—à–∏–π –∫ —Ü–µ–Ω—Ç—Ä—É ‚Äî –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö
            gap = abs(spd - target)
            keyn = (-gap, prec, f1)  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º gap, –∑–∞—Ç–µ–º –º–∞–∫—Å. precision/F1
            if (best_near is None) or (keyn > best_near[1]):
                best_near = (cand, keyn)

        # –æ—Å–Ω–æ–≤–Ω–æ–π –≤—ã–±–æ—Ä
        chosen = (best_in[0] if best_in is not None else best_near[0])
        tau_chosen, spd, prec, rec, f1, signals = chosen

        # ‚îÄ‚îÄ –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—ä—ë–º œÑ –≤–≤–µ—Ä—Ö (–µ—Å–ª–∏ –Ω–µ —Ö—É–∂–µ –∏ –æ—Å—Ç–∞—ë–º—Å—è –≤ —Ç–µ–∫—É—â–µ–º –æ–∫–Ω–µ SPD) ‚îÄ‚îÄ
        upper = min(float(tau_chosen) + 0.05, 0.999)
        ref_grid = np.linspace(float(tau_chosen), upper, 31)  # —à–∞–≥ ‚âà0.0017

        best_ref = None  # (key_ref, t, stats_ref)
        for t in ref_grid:
            stats_ref = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(t),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd_r = stats_ref['spd']
            prec_r = stats_ref['precision_macro_buy_sell']
            f1_r = stats_ref['f1_macro_buy_sell']

            # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞; –æ–∫–Ω–æ SPD –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ (spd_min..spd_max) –∏ —Ç–µ–∫—É—â–∏–π precision_min
            if (spd_min <= spd_r <= spd_max) and (prec_r >= precision_min):
                # –∫–ª—é—á: –º–∞–∫—Å F1, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É SPD
                key_ref = (f1_r, float(t), -abs(spd_r - target))
                if (best_ref is None) or (key_ref > best_ref[0]):
                    best_ref = (key_ref, float(t), stats_ref)

        # –ø—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ
        if best_ref is not None:
            _, tau_new, sref = best_ref
            tau_chosen = tau_new
            spd = float(sref['spd'])
            prec = float(sref['precision_macro_buy_sell'])
            rec = float(sref['recall_macro_buy_sell'])
            f1 = float(sref['f1_macro_buy_sell'])
            signals = int(round(spd * max(1, len(y_val)) / max(1, bars_per_day)))

        # hit_range –¥–æ–ª–∂–µ–Ω –æ—Ç—Ä–∞–∂–∞—Ç—å —Ñ–∞–∫—Ç –ø–æ–ø–∞–¥–∞–Ω–∏—è –ò–¢–û–ì–û–í–û–ì–û –≤—ã–±–æ—Ä–∞ –≤ –æ–∫–Ω–æ –∏ –ø–æ precision
        in_range = (spd_min <= spd <= spd_max) and (prec >= precision_min)

        return float(tau_chosen), {
            "spd": float(spd),
            "precision_macro_buy_sell": float(prec),
            "recall_macro_buy_sell": float(rec),
            "f1_macro_buy_sell": float(f1),
            "signals": int(signals),
            "delta": float(delta),
            "cooldown_bars": int(cooldown_bars),
            "range": [float(spd_min), float(spd_max)],
            "hit_range": bool(in_range),
        }

    @staticmethod
    def _eval_decision_metrics(y_true: np.ndarray,
                               proba: np.ndarray,  # shape (n,3) [HOLD, BUY, SELL]
                               tau: float,
                               delta: float,
                               cooldown_bars: int,
                               bars_per_day: int) -> dict:
        """
        –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç act/–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π/–º–µ—Ç—Ä–∏–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∏ –≤ —Ç—é–Ω–µ—Ä–µ, –∏ –≤ sensitivity.
        –í–ê–ñ–ù–û: –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –¢–û–õ–¨–ö–û –Ω–∞ –∏–Ω–¥–µ–∫—Å–µ act=True (–∫–∞–∫ –≤ —Ç—é–Ω–µ—Ä–µ), labels=[1,2].
        """
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        act = (maxp >= tau) & (margin >= delta)

        # cooldown –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è 0/1/2 –ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ
        pred = np.zeros(len(proba), dtype=int)  # HOLD=0
        buy_ge_sell = p_buy >= p_sell
        pred[act & buy_ge_sell] = 1
        pred[act & (~buy_ge_sell)] = 2

        # SPD
        spd_val = act.sum() * bars_per_day / max(1, len(y_true))

        # –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –∞–∫—Ç–∏–≤–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ (–∫–∞–∫ –≤ —Ç—é–Ω–µ—Ä–µ)
        if np.any(act):
            pm, rm, fm, _ = precision_recall_fscore_support(
                y_true[act], pred[act], labels=[1, 2], average='macro', zero_division=0
            )
        else:
            pm = rm = fm = 0.0

        return {
            'spd': float(spd_val),
            'precision_macro_buy_sell': float(pm),
            'recall_macro_buy_sell': float(rm),
            'f1_macro_buy_sell': float(fm),
            # –¥—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            'tau': float(tau),
            'delta': float(delta),
            'cooldown_bars': int(cooldown_bars),
        }

    @staticmethod
    def decide(proba, tau, delta=0.08, cooldown_bars=2):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)
        act = (maxp >= tau) & (margin >= delta)

        # Apply cooldown
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        pred = np.zeros(len(proba), dtype=int)
        pred[act] = np.where(p_buy[act] >= p_sell[act], 1, 2)
        return pred

    def train_model(self, run_id: str, use_scaler: bool = False) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–∫–Ω–æ–º –∏—Å—Ç–æ—Ä–∏–∏ + –ø–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"""

        logger.info("\n" + "=" * 60)
        logger.info("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò LIGHTGBM (WINDOWED)")
        logger.info("=" * 60)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y, w = self.prepare_training_data(run_id)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (80/20)
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        w_train, w_val = w.iloc[:split_idx], w.iloc[split_idx:]

        NUM_CLASS = 3
        REPORT_LABELS = [1, 2, 0]  # BUY, SELL, HOLD
        REPORT_NAMES = ['BUY', 'SELL', 'HOLD']

        logger.info(f"üìä Train: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤, Val: {len(X_val)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        logger.info("‚öñÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –∏–∑ training_dataset")

        # –î–∞—Ç–∞—Å–µ—Ç—ã LightGBM
        train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # SCALER (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        scaler = None
        if use_scaler:
            logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ StandardScaler –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            scaler = StandardScaler()
            scaler.fit(X_train)

            X_train_scaled = scaler.transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            train_data = lgb.Dataset(X_train_scaled, label=y_train, weight=w_train)
            val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)

            logger.info(f"‚úÖ Scaler –æ–±—É—á–µ–Ω –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω –Ω–∞ {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö")
        else:
            logger.info("‚ö†Ô∏è  Scaler –æ—Ç–∫–ª—é—á–µ–Ω - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ RAW –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        params = {
            'objective': 'multiclass',
            'num_class': NUM_CLASS,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.01,
            'feature_fraction': 0.6,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'min_child_samples': 20,
            'max_depth': 8,
            'lambda_l1': 1.0,
            'lambda_l2': 1.0,
            'min_gain_to_split': 0.1,
            'boost_from_average': False,
            'seed': 42,
            'bagging_seed': 42,
            'feature_fraction_seed': 42,
        }

        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")

        # –û–±—É—á–µ–Ω–∏–µ
        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            valid_names=['valid_0'],
            num_boost_round=2200,
            callbacks=[
                thermometer_progress_callback(logger, width=30, period=10),
                lgb.early_stopping(stopping_rounds=150, first_metric_only=True),
            ],
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if use_scaler and scaler is not None:
            y_val_pred_proba = model.predict(X_val_scaled)
        else:
            y_val_pred_proba = model.predict(X_val)

        y_val_pred = y_val_pred_proba.argmax(axis=1)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–∏–≥–Ω–∞–ª–æ–≤ –∏ –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–æ–≤
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        TF2BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}
        tf = str(getattr(self, "timeframe", "5m")).lower()
        bars_per_day = TF2BARS.get(tf, 288)

        # –ü–µ—Ä–µ–±–æ—Ä precision_min
        precision_grid = [0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.9]
        candidates = []
        for pm in precision_grid:
            try:
                tau_i, tstats_i = self.tune_tau_for_spd_range(
                    y_val=np.asarray(y_val),
                    proba=np.asarray(y_val_pred_proba),
                    bars_per_day=bars_per_day,
                    spd_min=4.0,
                    spd_max=10.0,
                    precision_min=pm,
                    delta=0.08,
                    cooldown_bars=2,
                )
                candidates.append({
                    'precision_min': pm,
                    'tau': float(tau_i),
                    'spd': float(tstats_i.get('spd', float('nan'))),
                    'precision_macro_buy_sell': float(tstats_i.get('precision_macro_buy_sell', float('nan'))),
                    'f1_macro_buy_sell': float(tstats_i.get('f1_macro_buy_sell', float('nan'))),
                    'hit_range': bool(tstats_i.get('hit_range', False)),
                    'delta': float(tstats_i.get('delta', 0.08)),
                    'cooldown_bars': int(tstats_i.get('cooldown_bars', 2)),
                    '_tstats': tstats_i,
                })
            except Exception as e:
                logging.warning(f"precision_min={pm:.2f}: sweep failed with error: {e}")

        def _key(c):
            return (1 if c.get('hit_range') else 0,
                    c.get('precision_macro_buy_sell', float('-inf')),
                    c.get('f1_macro_buy_sell', float('-inf')),
                    -c.get('tau', float('inf')))

        if not candidates:
            raise RuntimeError("Precision sweep failed: no candidates collected")

        best = max(candidates, key=_key)
        tau = best['tau']
        tstats = best['_tstats']
        delta = best.get('delta', 0.08)
        cooldown_bars = best.get('cooldown_bars', 2)

        logging.info("üîß Precision sweep results:")
        for c in candidates:
            logging.info(f"  pm={c['precision_min']:.2f}, tau={c['tau']:.3f}, spd‚âà{c['spd']:.1f}, "
                         f"prec‚âà{c['precision_macro_buy_sell']:.3f}, f1‚âà{c['f1_macro_buy_sell']:.3f}, "
                         f"hit={c['hit_range']}")
        logging.info(f"‚úÖ Picked precision_min={best['precision_min']:.2f} ‚Üí "
                     f"tau={tau:.3f}, spd‚âà{best['spd']:.1f}")

        logger.info(
            "üîß Tuned thresholds: tau=%.3f, delta=%.2f, cooldown=%d ‚Üí spd‚âà%.1f/day, "
            "precision‚âà%.3f, recall‚âà%.3f, f1‚âà%.3f (hit_range=%s)"
            % (tau, tstats['delta'], tstats['cooldown_bars'],
               tstats['spd'], tstats['precision_macro_buy_sell'],
               tstats['recall_macro_buy_sell'], tstats['f1_macro_buy_sell'],
               tstats['hit_range'])
        )

        # Sensitivity –∞–Ω–∞–ª–∏–∑
        _tau_offsets = [-0.05, -0.03, -0.02, 0.0, 0.02, 0.03, 0.05]
        _delta_offsets = [-0.02, 0.0, 0.02]

        tau_sensitivity = []
        for off in _tau_offsets:
            tau_x = float(np.clip(tau + off, 0.0, 1.0))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(y_val_pred_proba),
                tau=tau_x,
                delta=delta,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            tau_sensitivity.append(r)

        delta_sensitivity = []
        for off in _delta_offsets:
            delta_x = float(max(0.0, delta + off))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(y_val_pred_proba),
                tau=tau,
                delta=delta_x,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            delta_sensitivity.append(r)

        _tau_sorted = sorted(tau_sensitivity, key=lambda r: abs(r['tau'] - float(tau)))[:3]
        _tau_sorted = sorted(_tau_sorted, key=lambda r: r['tau'])

        logging.info("üîç Sensitivity (tau near current):")
        for r in _tau_sorted:
            logging.info(f"  tau={r['tau']:.3f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        logging.info("üîç Sensitivity (delta¬±0.02):")
        for r in delta_sensitivity:
            logging.info(f"  delta={r['delta']:.2f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        # –ú–µ—Ç—Ä–∏–∫–∏
        val_acc = accuracy_score(y_val, y_val_pred)
        train_dist = Counter(y_train)
        val_dist = Counter(y_val)
        pred_dist = Counter(y_val_pred)

        logger.info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"  Train: {dict(train_dist)}")
        logger.info(f"  Val:   {dict(val_dist)}")
        logger.info(f"  Pred:  {dict(pred_dist)}")

        prec, rec, f1, _ = precision_recall_fscore_support(
            y_val, y_val_pred,
            labels=REPORT_LABELS,
            average=None,
            zero_division=0
        )
        cm = confusion_matrix(y_val, y_val_pred, labels=REPORT_LABELS)

        decision_policy = {
            'tau': tau,
            'delta': tstats['delta'],
            'cooldown_bars': tstats['cooldown_bars'],
            'bars_per_day': bars_per_day,
            'val_spd': tstats['spd'],
            'val_precision_macro_buy_sell': tstats['precision_macro_buy_sell'],
            'val_recall_macro_buy_sell': tstats['recall_macro_buy_sell'],
            'val_f1_macro_buy_sell': tstats['f1_macro_buy_sell'],
            'target_spd_range': tstats['range'],
            'hit_range': tstats['hit_range'],
            'precision_min': best.get('precision_min', 0.60),
        }

        precision_min_sweep = [
            {
                'precision_min': c['precision_min'],
                'tau': c['tau'],
                'spd': c['spd'],
                'precision_macro_buy_sell': c['precision_macro_buy_sell'],
                'f1_macro_buy_sell': c['f1_macro_buy_sell'],
                'hit_range': c['hit_range'],
            }
            for c in candidates
        ]

        metrics = {
            'decision_policy': decision_policy,
            'precision_min_sweep': precision_min_sweep,
            'val_accuracy': float(val_acc),
            'precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec)},
            'recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec)},
            'f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1)},
            'confusion_matrix': cm.tolist(),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'class_distribution': {
                'train': {int(k): int(v) for k, v in train_dist.items()},
                'val': {int(k): int(v) for k, v in val_dist.items()},
                'pred': {int(k): int(v) for k, v in pred_dist.items()}
            },
            'tau_sensitivity': tau_sensitivity,
            'delta_sensitivity': delta_sensitivity,
            'lookback_window': self.lookback,
            'base_features_count': len(self.base_feature_names),
            'total_features_count': len(self.feature_names),
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        os.makedirs("models", exist_ok=True)
        model_filename = f"models/ml_windowed_{self.symbol.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"

        model_metadata = {
            'version': '2.1',
            'format': 'windowed_lgb',
            'instrument': self.symbol,
            'exchange': 'Binance',
            'timeframe': '5m',
            'lookback_window': self.lookback,
            'base_feature_count': len(self.base_feature_names),
            'total_feature_count': len(self.feature_names),
            'trained_at': datetime.now().isoformat(),
            'training_samples': len(X_train),
            'val_samples': len(X_val),
            'val_accuracy': float(val_acc),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'run_id': run_id,
            'decision_policy': decision_policy,
            'scaler_used': use_scaler,
        }

        model_package = {
            'model': model,
            'scaler': scaler,
            'metadata': model_metadata,
            'base_feature_names': self.base_feature_names,
            'lookback': self.lookback,
            'timeframe': '5m',
            'min_confidence': 0.65,
            'required_warmup': 60
        }

        joblib.dump(model_package, model_filename)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
        logger.info(f"   - Lookback: {self.lookback} –±–∞—Ä–æ–≤")
        logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names)}")
        logger.info(f"   - Scaler: {'StandardScaler' if scaler else 'None'}")

        # Tau curves
        try:
            tau_left = max(0.0, float(tau) - 0.05)
            tau_right = min(0.999, float(tau) + 0.05)
            tau_grid = np.arange(tau_left, tau_right + 1e-9, 0.002)

            spd_curve = []
            f1_curve = []
            for tcur in tau_grid:
                s = self._eval_decision_metrics(
                    y_true=np.asarray(y_val),
                    proba=np.asarray(y_val_pred_proba),
                    tau=float(tcur),
                    delta=float(delta),
                    cooldown_bars=int(cooldown_bars),
                    bars_per_day=int(bars_per_day),
                )
                spd_curve.append(s['spd'])
                f1_curve.append(s['f1_macro_buy_sell'])

            os.makedirs("models/training_logs", exist_ok=True)
            curve_prefix = str(Path("models/training_logs") / Path(model_filename).with_suffix('').name)

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, spd_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('SPD vs tau')
            plt.xlabel('tau')
            plt.ylabel('signals per day')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_spd.png")
            plt.close()

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, f1_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('F1 (macro BUY/SELL on act) vs tau')
            plt.xlabel('tau')
            plt.ylabel('F1 macro (BUY/SELL)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_f1.png")
            plt.close()

        except Exception as _e:
            logging.warning(f"tau curves plotting skipped: {_e}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        self.save_training_report(metrics, model_filename)

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        diag_prefix = Path("models/training_logs") / Path(model_filename).with_suffix('').name
        self.post_training_diagnostics(
            model=model,
            X_val=X_val,
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            prefix_path=str(diag_prefix),
            bars_per_day=bars_per_day
        )

        return metrics

    def plot_precision_spd_curve(self, y_val, y_val_pred_proba, bars_per_day: int,
                                 delta: float = 0.08, cooldown_bars: int = 2,
                                 prefix_path: str = "models/training_logs/diag") -> None:
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ SPD(œÑ) –∏ Precision/Recall/F1 –æ—Ç SPD
        """
        proba = np.asarray(y_val_pred_proba)
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        def _apply_cooldown(mask: np.ndarray, cd: int) -> np.ndarray:
            if cd <= 0:
                return mask
            idx = np.where(mask)[0]
            if idx.size == 0:
                return mask
            keep = [idx[0]]
            last = idx[0]
            for i in idx[1:]:
                if i - last >= cd:
                    keep.append(i)
                    last = i
            out = np.zeros_like(mask, dtype=bool)
            out[np.array(keep, dtype=int)] = True
            return out

        taus = np.linspace(0.45, 0.70, 26)
        rows = []
        n = len(y_val)

        for tau in taus:
            act = (maxp >= tau) & (margin >= delta)
            act = _apply_cooldown(act, cooldown_bars)

            signals = int(act.sum())
            spd = signals * bars_per_day / max(1, n)

            if signals == 0:
                prec = rec = f1 = 0.0
            else:
                pred_dir = np.where(p_buy[act] >= p_sell[act], 1, 2)
                true_dir = y_val[act]
                prec = precision_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                rec = recall_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                f1 = f1_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)

            rows.append((tau, spd, prec, rec, f1, signals))

        df = pd.DataFrame(rows, columns=['tau', 'spd_per_day', 'precision', 'recall', 'f1', 'signals'])
        csv_path = f"{prefix_path}_tau_sweep.csv"
        df.to_csv(csv_path, index=False)

        # –ì—Ä–∞—Ñ–∏–∫ SPD(œÑ)
        plt.figure(figsize=(8, 5))
        plt.plot(df['tau'], df['spd_per_day'], marker='o')
        plt.xlabel('tau')
        plt.ylabel('SPD (signals/day)')
        plt.title('Signals per day vs tau')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_spd_vs_tau.png")
        plt.close()

        # –ì—Ä–∞—Ñ–∏–∫ Precision/Recall/F1 vs SPD
        plt.figure(figsize=(8, 5))
        plt.plot(df['spd_per_day'], df['precision'], marker='o', label='Precision (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['recall'], marker='o', label='Recall (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['f1'], marker='o', label='F1 (macro BUY/SELL)')
        plt.xlabel('SPD (signals/day)')
        plt.ylabel('score')
        plt.title('Precision / Recall / F1 vs SPD')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_prf_vs_spd.png")
        plt.close()

    def post_training_diagnostics(self, model, X_val, y_val, y_val_pred_proba,
                                  prefix_path: str, bars_per_day: int = 288):
        """
        –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:
        - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        - PR-–∫—Ä–∏–≤—ã–µ
        - SPD curves
        """
        os.makedirs(os.path.dirname(prefix_path), exist_ok=True)

        try:
            bars_per_day = int(bars_per_day) if bars_per_day is not None else 288
        except Exception:
            bars_per_day = 288

        proba = np.asarray(y_val_pred_proba)
        if proba.ndim == 1:
            tmp = np.zeros((len(proba), 3), dtype=float)
            tmp[np.arange(len(proba)), np.clip(proba.astype(int), 0, 2)] = 1.0
            proba = tmp

        p_hold = proba[:, 0]
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]

        feat_names = self.feature_names if hasattr(self, 'feature_names') else [f"f{i}" for i in range(X_val.shape[1])]

        # === 1) Feature Importance ===
        try:
            gain = model.feature_importance(importance_type='gain')
            df_imp = (pd.DataFrame({'feature': feat_names, 'gain': gain})
                      .sort_values('gain', ascending=False)
                      .head(30))
            plt.figure(figsize=(10, max(8, 0.3 * len(df_imp))))
            sns.barplot(data=df_imp, x='gain', y='feature')
            plt.title('Feature Importance (gain) ‚Äî top 30')
            plt.tight_layout()
            plt.savefig(f"{prefix_path}_feat_importance.png")
            plt.close()

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å CSV —Å –í–°–ï–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
            pd.DataFrame({'feature': feat_names, 'gain': gain}).sort_values('gain', ascending=False).to_csv(
                f"{prefix_path}_feat_importance.csv", index=False
            )

            # –î–û–ë–ê–í–õ–ï–ù–û: –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è 22 –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å)
            base_feat_importance = {}
            for feature, importance in zip(feat_names, gain):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ (—É–±–∏—Ä–∞–µ–º _t0, _t-1 –∏ —Ç.–¥.)
                if '_t-' in feature:
                    base_feat = feature.split('_t-')[0]  # cmo_14_t-1 -> cmo_14
                elif '_t0' in feature:
                    base_feat = feature.replace('_t0', '')  # cmo_14_t0 -> cmo_14
                else:
                    base_feat = feature  # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫

                base_feat_importance[base_feat] = base_feat_importance.get(base_feat, 0) + importance

            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
            df_base_imp = pd.DataFrame({
                'base_feature': list(base_feat_importance.keys()),
                'total_gain': list(base_feat_importance.values())
            }).sort_values('total_gain', ascending=False)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Ç–∞–±–ª–∏—Ü—É —Å 22 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            df_base_imp.to_csv(f"{prefix_path}_feat_importance_22_base.csv", index=False)

            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ 22 –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞
            logger.info("üéØ –í–ê–ñ–ù–û–°–¢–¨ 22 –ë–ê–ó–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –≤—Å–µ–º –ª–∞–≥–∞–º):")
            for i, row in df_base_imp.iterrows():
                logger.info(f"   {i + 1:2d}. {row['base_feature']}: {row['total_gain']:.0f}")

        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

        # === 2) –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã ===
        y_pred = proba.argmax(axis=1)

        def hist_one(prob, true_class, name, fname):
            mask_pos = (y_val == true_class)
            mask_pred_pos = (y_pred == true_class)

            tp = prob[mask_pos & mask_pred_pos]
            fp = prob[(~mask_pos) & mask_pred_pos]
            fn = prob[mask_pos & (~mask_pred_pos)]
            tn = prob[(~mask_pos) & (~mask_pred_pos)]

            plt.figure(figsize=(8, 5))
            bins = 30
            if len(tp) > 0:
                sns.histplot(tp, bins=bins, stat='density', label='TP', alpha=0.6)
            if len(fp) > 0:
                sns.histplot(fp, bins=bins, stat='density', label='FP', alpha=0.6)
            if len(fn) > 0:
                sns.histplot(fn, bins=bins, stat='density', label='FN', alpha=0.6)
            if len(tn) > 0:
                sns.histplot(tn, bins=bins, stat='density', label='TN', alpha=0.6)
            plt.legend()
            plt.xlabel(f"p({name})")
            plt.ylabel("density")
            plt.title(f"Distributions for {name}")
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()

        hist_one(p_buy, 1, "BUY", f"{prefix_path}_proba_hist_BUY.png")
        hist_one(p_sell, 2, "SELL", f"{prefix_path}_proba_hist_SELL.png")

        # === 3) Max-proba scatter ===
        maxp = proba.max(axis=1)
        plt.figure(figsize=(8, 5))
        sns.scatterplot(x=np.arange(len(maxp)), y=maxp,
                        hue=[{0: 'HOLD', 1: 'BUY', 2: 'SELL'}.get(c, 'UNK') for c in y_val],
                        s=12, linewidth=0)
        plt.title("Max class probability vs true class (val order)")
        plt.xlabel("index in validation set (chronological)")
        plt.ylabel("max proba")
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_maxproba_scatter.png")
        plt.close()

        # === 4) PR curves ===
        Y_bin = label_binarize(y_val, classes=[0, 1, 2])
        curves = [
            ("BUY", Y_bin[:, 1], p_buy),
            ("SELL", Y_bin[:, 2], p_sell),
            ("HOLD", Y_bin[:, 0], p_hold),
        ]
        plt.figure(figsize=(8, 6))
        for name, y_true_bin, y_score in curves:
            precision, recall, _ = precision_recall_curve(y_true_bin, y_score)
            ap = average_precision_score(y_true_bin, y_score)
            plt.plot(recall, precision, label=f"{name} (AP={ap:.3f})")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision‚ÄìRecall curves (one-vs-rest)")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_pr_curves.png")
        plt.close()

        # === 5) SPD curve ===
        self.plot_precision_spd_curve(
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            bars_per_day=bars_per_day,
            delta=0.08,
            cooldown_bars=2,
            prefix_path=prefix_path
        )

    def save_training_report(self, metrics: dict, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –æ–±—É—á–µ–Ω–∏–∏"""
        os.makedirs("models/training_logs", exist_ok=True)

        report = {
            'training_date': datetime.now().isoformat(),
            'symbol': self.symbol,
            'db_dsn': self.db_dsn,
            'model_path': model_path,
            'lookback_window': self.lookback,
            'metrics': metrics,
            'base_feature_names': self.base_feature_names,
            'total_feature_names_count': len(self.feature_names),
        }

        report_filename = model_path.replace('.joblib', '_report.json')
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)

        # Confusion matrix
        try:
            cm = np.array(metrics.get('confusion_matrix', []))
            if cm.size > 0:
                n = cm.shape[0]
                prec_map = metrics.get('precision', {})
                labels = list(prec_map.keys()) if isinstance(prec_map, dict) else None

                if not labels or len(labels) != n:
                    if n == 3:
                        labels = ['BUY', 'SELL', 'HOLD']
                    else:
                        labels = [f"class_{i}" for i in range(n)]

                plt.figure(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d',
                            xticklabels=labels, yticklabels=labels)
                plt.title('Confusion Matrix')
                plt.tight_layout()
                plt.savefig(report_filename.replace('.json', '_cm.png'))
                plt.close()
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é CM: {e}")

        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")

    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ"""
        self.data_loader.close()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò (v2.1 - WINDOWED)")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ë–î
    db_file = DATA_DIR / "market_data.sqlite"
    if not db_file.exists():
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {db_file} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   1. ml_data_preparation.py")
        print("   2. ml_labeling_tool_v3.py")
        return 1

    trainer = None
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        db_dsn = MARKET_DB_DSN
        symbol = "ETHUSDT"
        lookback = LOOKBACK_WINDOW

        # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π run_id
        engine = create_engine(MARKET_DB_DSN)
        with engine.connect() as conn:
            result = conn.execute(
                text("SELECT run_id FROM training_dataset_meta ORDER BY created_at DESC LIMIT 1")
            )
            row = result.fetchone()
        engine.dispose()

        if not row:
            print("‚ùå –ù–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö snapshot –≤ training_dataset_meta!")
            print("   –°–æ–∑–¥–∞–π—Ç–µ snapshot —á–µ—Ä–µ–∑ [14] –≤ ml_labeling_tool_v3.py")
            return 1

        run_id = row[0]

        print(f"üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DATA_DIR / 'market_data.sqlite'}")
        print(f"üéØ –°–∏–º–≤–æ–ª: {symbol}")
        print(f"üì¶ Run ID: {run_id}")
        print(f"ü™ü Lookback Window: {lookback} –±–∞—Ä–æ–≤")
        print("=" * 50)

        # –û–±—É—á–µ–Ω–∏–µ
        trainer = ModelTrainer(db_dsn, symbol, lookback=lookback)

        use_scaler = False  # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å True
        metrics = trainer.train_model(run_id, use_scaler=use_scaler)

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['val_accuracy']:.4f}")
        print(f"   Precision BUY/SELL/HOLD: "
              f"{metrics['precision']['BUY']:.4f}/"
              f"{metrics['precision']['SELL']:.4f}/"
              f"{metrics['precision']['HOLD']:.4f}")
        print(f"   Recall BUY/SELL/HOLD: "
              f"{metrics['recall']['BUY']:.4f}/"
              f"{metrics['recall']['SELL']:.4f}/"
              f"{metrics['recall']['HOLD']:.4f}/")
        return 0

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1
    finally:
        if trainer:
            trainer.close()


if __name__ == '__main__':
    sys.exit(main())