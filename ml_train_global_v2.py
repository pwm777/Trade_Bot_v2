# train_ml_global_v2.py
"""
–û–±—É—á–µ–Ω–∏–µ LightGBM –º–æ–¥–µ–ª–∏ —Å –û–ö–ù–û–ú –ò–°–¢–û–†–ò–ò

–ê–≤—Ç–æ—Ä: pwm777
–î–∞—Ç–∞: 2025-11-17
–í–µ—Ä—Å–∏—è: 2.1.1 (windowed training)

–ò–∑–º–µ–Ω–µ–Ω–∏—è:
- Lookback —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π LOOKBACK_WINDOW (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 11 –±–∞—Ä–æ–≤)
- –ü—Ä–∏–º–µ—Ä = –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –±–∞—Ä–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ (–æ–∫–Ω–æ), –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–º–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤ –ø–æ—Ä—è–¥–æ–∫ [t0, t-1, ..., t-(N-1)]
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: tau tuning, diagnostics, plots, –æ—Ç—á—ë—Ç—ã
"""

import sys
import logging
from sqlalchemy import create_engine, text
from datetime import datetime
import json
from typing import Tuple
import warnings
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import joblib
from pathlib import Path
from config import BASE_FEATURE_NAMES
warnings.filterwarnings('ignore')
import re
import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize, StandardScaler
from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LOOKBACK_WINDOW = 11  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
TIMEFRAME_TO_BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –£–¢–ò–õ–ò–¢–´
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _infer_bars_per_day_from_run_id(run_id: str, default: int = 288) -> int:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑ run_id —Ñ–æ—Ä–º–∞—Ç–∞ ..._<tf>_...
    –∏ –≤–µ—Ä–Ω—É—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –≤ —Å—É—Ç–∫–∞—Ö.
    """
    m = re.search(r"_(\d+[mh])_", str(run_id).lower())
    if not m:
        return default
    tf = m.group(1)
    timeframe_to_bars_local = {
        "1m": 1440,
        "3m": 480,
        "5m": 288,
        "15m": 96,
        "30m": 48,
        "1h": 24,
        "2h": 12,
        "4h": 6,
        "6h": 4,
        "12h": 2,
        "1d": 1,
    }
    return timeframe_to_bars_local.get(tf, default)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–û–õ–õ–ë–≠–ö ¬´–¢–ï–†–ú–û–ú–ï–¢–† –ü–†–û–ì–†–ï–°–°–ê¬ª –î–õ–Ø LIGHTGBM
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def thermometer_progress_callback(logger: logging.Logger, width: int = 30, period: int = 10):
    """–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º –±—É—Å—Ç–∏–Ω–≥–∞"""
    import sys

    def _cb(env):
        begin = getattr(env, 'begin_iteration', 0) or 0
        end = getattr(env, 'end_iteration', None)
        if end is None or end <= begin:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ü–µ–Ω–∏—Ç—å –æ–±—â–µ–µ —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–≤ params num_boost_round –æ–±—ã—á–Ω–æ –Ω–µ—Ç)
            total = max(1, (getattr(env, 'iteration', 0) or 0) + 1)
        else:
            total = max(1, end - begin)

        iter_now = int(getattr(env, 'iteration', 0) or 0)
        done = max(0, iter_now - begin + 1)
        pct = min(1.0, max(0.0, done / total))
        filled = int(round(pct * width))
        bar = '‚ñà' * filled + '‚ñë' * (width - filled)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_metric_name = None
        val_metric_value = None
        evals = getattr(env, 'evaluation_result_list', None)
        if evals:
            for res in evals:
                if isinstance(res, (list, tuple)) and len(res) >= 3:
                    data_name, metric_name, metric_val = res[0], res[1], res[2]
                    if str(data_name).startswith(('valid', 'val')):
                        val_metric_name = str(metric_name)
                        try:
                            val_metric_value = float(metric_val)
                        except Exception:
                            val_metric_value = None
                        break

        should_print = (iter_now > 0 and iter_now % period == 0) or (done >= total)

        if should_print:
            if val_metric_name is not None and val_metric_value is not None:
                msg = f"[{iter_now:4d}/{total}] {val_metric_name}:{val_metric_value:.5f} | {bar} {int(pct * 100):3d}%"
            else:
                msg = f"[{iter_now:4d}/{total}] {bar} {int(pct * 100):3d}%"

            if done >= total:
                print(f"\r{msg}")
                sys.stdout.flush()
            else:
                print(f"\r{msg}", end='', flush=True)

    return _cb


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–õ–ê–°–° –î–õ–Ø –†–ê–ë–û–¢–´ –° –ë–ê–ó–û–ô –î–ê–ù–ù–´–•
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class DataLoader:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –±–∞–∑—ã ml_labeling_tool_v3.py"""

    def __init__(self, db_dsn: str = MARKET_DB_DSN, symbol: str = "ETHUSDT"):
        self.db_dsn = db_dsn
        self.db_path = DATA_DIR / "market_data.sqlite"
        self.symbol = symbol
        self.engine = None

    def connect(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.db_path}")
        self.engine = create_engine(self.db_dsn)
        logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î: {self.db_path}")

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.engine:
            self.engine.dispose()
            logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

    def load_market_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ candles_5m"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM candles_5m 
            WHERE symbol = :symbol 
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"symbol": self.symbol})

        if df.empty:
            raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        if 'ts' in df.columns and 'datetime' not in df.columns:
            df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π –∏–∑ candles_5m")
        return df

    def load_training_dataset(self, run_id: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ training_dataset"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM training_dataset
            WHERE run_id = :run_id
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"run_id": run_id})

        if df.empty:
            raise ValueError(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è run_id={run_id}")

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ training_dataset")
        logger.info(f"   –ö–ª–∞—Å—Å—ã: {df['reversal_label'].value_counts().to_dict()}")
        return df


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° ModelTrainer –° –û–ö–ù–û–ú –ò–°–¢–û–†–ò–ò
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ModelTrainer:
    def __init__(self, db_dsn: str, symbol: str, lookback: int = LOOKBACK_WINDOW):
        self.db_dsn = db_dsn
        self.symbol = symbol
        self.lookback = lookback
        self.timeframe = "5m"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.data_loader = DataLoader(db_dsn, symbol)
        self.base_feature_names = BASE_FEATURE_NAMES

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ª–∞–≥–∞–º–∏
        self.feature_names = self._generate_windowed_feature_names()
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ "
                    f"({len(self.base_feature_names)} √ó {lookback} –±–∞—Ä–æ–≤)")

    def _generate_windowed_feature_names(self) -> list:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ª–∞–≥–æ–≤
        –ù–∞–ø—Ä–∏–º–µ—Ä: cmo_14_t0, cmo_14_t-1, ..., cmo_14_t-(lookback-1)
        """
        names = []
        # t0 - —Ç–µ–∫—É—â–∏–π –±–∞—Ä (—Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π)
        for feat in self.base_feature_names:
            names.append(f"{feat}_t0")

        # t-1, t-2, ..., t-(lookback-1) - –∏—Å—Ç–æ—Ä–∏—è
        for lag in range(1, self.lookback):
            for feat in self.base_feature_names:
                names.append(f"{feat}_t-{lag}")

        return names

    def prepare_training_data(self, run_id: str) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–∫–Ω–æ–º –∏—Å—Ç–æ—Ä–∏–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
        """
        df = self.data_loader.load_training_dataset(run_id)

        logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –∏—Å—Ç–æ—Ä–∏–∏ (lookback={self.lookback})...")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å 3 –°–†–ê–ó–£
        df_filtered = df[df['reversal_label'] != 3].copy()
        skipped = len(df) - len(df_filtered)

        if skipped > 0:
            logger.info(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º 3")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        feature_matrix = df_filtered[self.base_feature_names].values
        labels = df_filtered['reversal_label'].values
        weights = df_filtered['sample_weight'].values

        n_samples = len(df_filtered)
        n_features = len(self.base_feature_names)

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–∫–Ω–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ù–ê –º–µ—Ç–∫–µ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)
        # –î–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–∏ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ i –Ω—É–∂–Ω–æ lookback –±–∞—Ä–æ–≤ –í–ö–õ–Æ–ß–ê–Ø i
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –º–µ—Ç–∫–∏: lookback-1 (—á—Ç–æ–±—ã –±—ã–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏)
        n_valid = n_samples - (self.lookback - 1)

        if n_valid <= 0:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è lookback={self.lookback}")

        logger.info(f"   –°–æ–∑–¥–∞–Ω–∏–µ {n_valid} –æ–∫–æ–Ω –∏–∑ {n_samples} –æ–±—Ä–∞–∑—Ü–æ–≤...")

        # –ü—Ä–µ–¥–∞–ª–ª–æ–∫–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        X_windowed = np.zeros((n_valid, self.lookback * n_features), dtype=np.float32)

        # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê:
        # –ú–µ—Ç–∫–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ label_idx (—ç—Ç–æ extreme_timestamp)
        # –û–∫–Ω–æ: [label_idx - (lookback-1), .. ., label_idx-1, label_idx]
        # –¢–æ –µ—Å—Ç—å lookback –±–∞—Ä–æ–≤, –ó–ê–ö–ê–ù–ß–ò–í–ê–Æ–©–ò–•–°–Ø –Ω–∞ label_idx

        for i in range(n_valid):
            label_idx = i + (self.lookback - 1)  # –ü–æ–∑–∏—Ü–∏—è –º–µ—Ç–∫–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—Å–∏–≤–µ
            start_idx = label_idx - (self.lookback - 1)  # –ù–∞—á–∞–ª–æ –æ–∫–Ω–∞
            end_idx = label_idx + 1  # –ö–æ–Ω–µ—Ü –æ–∫–Ω–∞ (—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ)

            # –û–∫–Ω–æ: [start_idx : end_idx] = lookback –±–∞—Ä–æ–≤
            window = feature_matrix[start_idx:end_idx, :]  # shape: (lookback, n_features)

            # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: [t0, t-1, t-2, ..., t-(lookback-1)]
            # –≥–¥–µ t0 = label_idx (—Ç–µ–∫—É—â–∏–π –±–∞—Ä —Å –º–µ—Ç–∫–æ–π)
            window_ordered = window[::-1]  # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º: –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø–µ—Ä–≤—ã–º

            X_windowed[i] = window_ordered.ravel()

        # ‚úÖ –ú–µ—Ç–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç label_idx –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
        y_windowed = labels[self.lookback - 1:]
        w_windowed = weights[self.lookback - 1:]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        X_df = pd.DataFrame(X_windowed, columns=self.feature_names)
        y_series = pd.Series(y_windowed, name='label')
        w_series = pd.Series(w_windowed, name='weight')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing = X_df.isnull().sum()
        if missing.any():
            logger.warning(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏...")
            X_df = X_df.fillna(0)

        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(X_df)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y_series.value_counts().to_dict()}")

        return X_df, y_series, w_series

    def tune_tau_for_spd_range(
            self,
            y_val: np.ndarray,
            proba: np.ndarray,
            bars_per_day: int,
            spd_min: float = 20.0,  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
            spd_max: float = 35.0,
            precision_min: float = 0.70,
            delta: float = 0.06,
            cooldown_bars: int = 2,
            log_stats: bool = True,  # ‚Üê –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ maxp —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
    ):
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)

        # –î–∏–∞–ø–∞–∑–æ–Ω tau –∏–∑ –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        taus = np.linspace(0.65, 0.92, 50)

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ maxp —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–ª–∞–≥—É
        if log_stats:
            logging.info(f"üìä Max probability stats: "
                         f"mean={maxp.mean():.3f}, "
                         f"50%={np.percentile(maxp, 50):.3f}, "
                         f"90%={np.percentile(maxp, 90):.3f}")

        best_in = None  # (cand, key)
        best_near = None  # (cand, keyn)
        target = 0.5 * (spd_min + spd_max)
        n = len(y_val)

        for tau_cand in sorted(taus):
            # –µ–¥–∏–Ω—ã–π —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫/act —á–µ—Ä–µ–∑ helper
            stats = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(tau_cand),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd = stats['spd']
            prec = stats['precision_macro_buy_sell']
            rec = stats['recall_macro_buy_sell']
            f1 = stats['f1_macro_buy_sell']
            # –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ SPD (–ø–æ—Å–ª–µ cooldown)
            signals = int(round(spd * max(1, n) / max(1, bars_per_day)))

            cand = (float(tau_cand), float(spd), float(prec), float(rec), float(f1), int(signals))

            # –≤–Ω—É—Ç—Ä–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º precision, –∑–∞—Ç–µ–º F1, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ
            if spd_min <= spd <= spd_max and prec >= precision_min:
                key = (prec, f1, -abs(spd - target), float(tau_cand))
                if (best_in is None) or (key > best_in[1]):
                    best_in = (cand, key)

            # –±–ª–∏–∂–∞–π—à–∏–π –∫ —Ü–µ–Ω—Ç—Ä—É ‚Äî –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö
            gap = abs(spd - target)
            keyn = (-gap, prec, f1)  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º gap, –∑–∞—Ç–µ–º –º–∞–∫—Å. precision/F1
            if (best_near is None) or (keyn > best_near[1]):
                best_near = (cand, keyn)

        # –æ—Å–Ω–æ–≤–Ω–æ–π –≤—ã–±–æ—Ä
        chosen = (best_in[0] if best_in is not None else best_near[0])
        tau_chosen, spd, prec, rec, f1, signals = chosen

        # ‚îÄ‚îÄ –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—ä—ë–º œÑ –≤–≤–µ—Ä—Ö (–µ—Å–ª–∏ –Ω–µ —Ö—É–∂–µ –∏ –æ—Å—Ç–∞—ë–º—Å—è –≤ —Ç–µ–∫—É—â–µ–º –æ–∫–Ω–µ SPD) ‚îÄ‚îÄ
        upper = min(float(tau_chosen) + 0.05, 0.999)
        ref_grid = np.linspace(float(tau_chosen), upper, 31)  # —à–∞–≥ ‚âà0.0017

        best_ref = None  # (key_ref, t, stats_ref)
        for t in ref_grid:
            stats_ref = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(t),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd_r = stats_ref['spd']
            prec_r = stats_ref['precision_macro_buy_sell']
            f1_r = stats_ref['f1_macro_buy_sell']

            # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞; –æ–∫–Ω–æ SPD –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ (spd_min..spd_max) –∏ —Ç–µ–∫—É—â–∏–π precision_min
            if (spd_min <= spd_r <= spd_max) and (prec_r >= precision_min):
                # –∫–ª—é—á: –º–∞–∫—Å F1, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É SPD
                key_ref = (f1_r, float(t), -abs(spd_r - target))
                if (best_ref is None) or (key_ref > best_ref[0]):
                    best_ref = (key_ref, float(t), stats_ref)

        # –ø—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ
        if best_ref is not None:
            _, tau_new, sref = best_ref
            tau_chosen = tau_new
            spd = float(sref['spd'])
            prec = float(sref['precision_macro_buy_sell'])
            rec = float(sref['recall_macro_buy_sell'])
            f1 = float(sref['f1_macro_buy_sell'])
            signals = int(round(spd * max(1, len(y_val)) / max(1, bars_per_day)))

        # hit_range –¥–æ–ª–∂–µ–Ω –æ—Ç—Ä–∞–∂–∞—Ç—å —Ñ–∞–∫—Ç –ø–æ–ø–∞–¥–∞–Ω–∏—è –ò–¢–û–ì–û–í–û–ì–û –≤—ã–±–æ—Ä–∞ –≤ –æ–∫–Ω–æ –∏ –ø–æ precision
        in_range = (spd_min <= spd <= spd_max) and (prec >= precision_min)

        return float(tau_chosen), {
            "spd": float(spd),
            "precision_macro_buy_sell": float(prec),
            "recall_macro_buy_sell": float(rec),
            "f1_macro_buy_sell": float(f1),
            "signals": int(signals),
            "delta": float(delta),
            "cooldown_bars": int(cooldown_bars),
            "range": [float(spd_min), float(spd_max)],
            "hit_range": bool(in_range),
        }

    @staticmethod
    def _eval_decision_metrics(y_true: np.ndarray,
                               proba: np.ndarray,
                               tau: float,
                               delta: float,
                               cooldown_bars: int,
                               bars_per_day: int) -> dict:
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        act = (maxp >= tau) & (margin >= delta)

        # cooldown –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö samples
        active_count = np.sum(act)
        if active_count > 0:
            logging.debug(f"Active samples: {active_count}, tau={tau:.3f}")

        pred = np.zeros(len(proba), dtype=int)
        buy_ge_sell = p_buy >= p_sell
        pred[act & buy_ge_sell] = 1
        pred[act & (~buy_ge_sell)] = 2

        # SPD
        spd_val = act.sum() * bars_per_day / max(1, len(y_true))

        # –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –∞–∫—Ç–∏–≤–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ (–¢–û–õ–¨–ö–û –∫–ª–∞—Å—Å—ã 1 –∏ 2)
        if np.any(act):
            y_true_active = y_true[act]
            pred_active = pred[act]

            # –§–ò–õ–¨–¢–†–£–ï–ú: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã 1 –∏ 2 (BUY/SELL)
            mask_buy_sell = (y_true_active == 1) | (y_true_active == 2)
            y_true_bs = y_true_active[mask_buy_sell]
            pred_bs = pred_active[mask_buy_sell]

            if len(y_true_bs) > 0:
                pm, rm, fm, _ = precision_recall_fscore_support(
                    y_true_bs, pred_bs, labels=[1, 2], average='macro', zero_division=0
                )

                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                correct_bs = np.sum(y_true_bs == pred_bs)
                accuracy_bs = correct_bs / len(y_true_bs) if len(y_true_bs) > 0 else 0
                logging.debug(f"BUY/SELL metrics: {len(y_true_bs)} samples, accuracy={accuracy_bs:.3f}")
            else:
                pm = rm = fm = 0.0
                logging.debug("No BUY/SELL samples in active set")
        else:
            pm = rm = fm = 0.0

        return {
            'spd': float(spd_val),
            'precision_macro_buy_sell': float(pm),
            'recall_macro_buy_sell': float(rm),
            'f1_macro_buy_sell': float(fm),
            'tau': float(tau),
            'delta': float(delta),
            'cooldown_bars': int(cooldown_bars),
            '_debug_active_count': int(active_count),  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        }

    @staticmethod
    def decide(proba, tau, delta=0.08, cooldown_bars=2):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)
        act = (maxp >= tau) & (margin >= delta)

        # Apply cooldown
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        pred = np.zeros(len(proba), dtype=int)
        pred[act] = np.where(p_buy[act] >= p_sell[act], 1, 2)
        return pred

    def train_model(self, run_id: str, use_scaler: bool = False) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–∫–Ω–æ–º –∏—Å—Ç–æ—Ä–∏–∏ + –ø–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"""

        logger.info("\n" + "=" * 60)
        logger.info("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò LIGHTGBM (WINDOWED)")
        logger.info("=" * 60)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y, w = self.prepare_training_data(run_id)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (70/15/15)
        n = len(X)
        train_end = int(n * 0.70)
        val_end = int(n * 0.85)

        X_train = X.iloc[:train_end]
        X_val = X.iloc[train_end:val_end]
        X_test = X.iloc[val_end:]

        y_train = y.iloc[:train_end]
        y_val = y.iloc[train_end:val_end]
        y_test = y.iloc[val_end:]

        w_train = w.iloc[:train_end]
        w_val = w.iloc[train_end:val_end]
        w_test = w.iloc[val_end:]

        NUM_CLASS = 3
        REPORT_LABELS = [1, 2, 0]  # BUY, SELL, HOLD
        REPORT_NAMES = ['BUY', 'SELL', 'HOLD']

        logger.info(f"üìä Train: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤, Val: {len(X_val)} –ø—Ä–∏–º–µ—Ä–æ–≤, Test: {len(X_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        logger.info("‚öñÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –∏–∑ training_dataset")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # SCALER (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        scaler = None
        X_train_processed = X_train
        X_val_processed = X_val
        X_test_processed = X_test

        if use_scaler:
            logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ StandardScaler –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            scaler = StandardScaler()
            X_train_processed = scaler.fit_transform(X_train)
            X_val_processed = scaler.transform(X_val)
            X_test_processed = scaler.transform(X_test)
            logger.info(f"‚úÖ Scaler –æ–±—É—á–µ–Ω –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω –Ω–∞ {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö")
        else:
            logger.info("‚ö†Ô∏è  Scaler –æ—Ç–∫–ª—é—á–µ–Ω - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ RAW –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")

        # –î–∞—Ç–∞—Å–µ—Ç—ã LightGBM
        train_data = lgb.Dataset(X_train_processed, label=y_train, weight=w_train)
        val_data = lgb.Dataset(X_val_processed, label=y_val, reference=train_data)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        params = {
            'objective': 'multiclass',
            'num_class': NUM_CLASS,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 16,  # –ë—ã–ª–æ 31 - —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —É–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            'max_depth': 4,  # –ë—ã–ª–æ 8 - —É–±—Ä–∞–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –ø—É—Å—Ç—å —É–ø—Ä–∞–≤–ª—è–µ—Ç num_leaves
            'min_child_samples': 100,  # –ë—ã–ª–æ 20 - —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            'learning_rate': 0.03,  # –ë—ã–ª–æ 0.01 - —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            'n_estimators': 1000,  # –ë—ã–ª–æ –Ω–µ—è–≤–Ω–æ 1600 - —É–º–µ–Ω—å—à–µ–Ω–æ –∏–∑-–∑–∞ –±–æ–ª—å—à–µ–≥–æ learning_rate
            'feature_fraction': 0.5,
            'bagging_fraction': 0.6,
            'bagging_freq': 5,
            'lambda_l1': 1.5,
            'lambda_l2': 1.5,
            'min_gain_to_split': 0.15,
            'boost_from_average': False,
            'verbose': -1,
            'seed': 42,
            'bagging_seed': 42,
            'feature_fraction_seed': 42,
            'extra_trees': True,  # –†–µ–∂–∏–º Extremely Randomized Trees (–º–æ–∂–Ω–æ True –¥–ª—è –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
            'path_smooth': 0.1,  # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö)
            'max_bin': 65,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º
            'min_data_in_bin': 5,  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –±–∏–Ω–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
            'bin_construct_sample_cnt': 200000,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º
            'extra_seed': 42,
        }

        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")

        # –û–±—É—á–µ–Ω–∏–µ
        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            valid_names=['valid_0'],
            num_boost_round=1600,
            callbacks=[
                thermometer_progress_callback(logger, width=30, period=10),
                lgb.early_stopping(stopping_rounds=20, first_metric_only=True),
            ],
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ val
        y_val_pred_proba = model.predict(X_val_processed if (use_scaler and scaler is not None) else X_val)
        y_val_pred = y_val_pred_proba.argmax(axis=1)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ test (–¥–ª—è tuning)
        y_test_pred_proba = model.predict(X_test_processed if (use_scaler and scaler is not None) else X_test)
        y_test_pred = y_test_pred_proba.argmax(axis=1)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –£–¢–ï–ß–ö–ò –î–ê–ù–ù–´–•
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        logger.info("\nüîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –£–¢–ï–ß–ö–ò –î–ê–ù–ù–´–•:")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ train (–±–µ–∑ –ø–æ—Ä–æ–≥–æ–≤)
        y_train_pred_proba_diag = model.predict(X_train_processed if (use_scaler and scaler is not None) else X_train)
        y_train_pred_diag = y_train_pred_proba_diag.argmax(axis=1)
        train_acc = accuracy_score(y_train, y_train_pred_diag)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ val/test (–±–µ–∑ –ø–æ—Ä–æ–≥–æ–≤)
        val_acc = accuracy_score(y_val, y_val_pred)
        test_acc = accuracy_score(y_test, y_test_pred)

        logger.info(f"   Train accuracy (–±–µ–∑ –ø–æ—Ä–æ–≥–æ–≤): {train_acc:.4f}")
        logger.info(f"   Val accuracy (–±–µ–∑ –ø–æ—Ä–æ–≥–æ–≤): {val_acc:.4f}")
        logger.info(f"   Test accuracy (–±–µ–∑ –ø–æ—Ä–æ–≥–æ–≤): {test_acc:.4f}")
        logger.info(f"   Gap (train-val): {train_acc - val_acc:.4f}")
        logger.info(f"   Gap (train-test): {train_acc - test_acc:.4f}")

        if train_acc > 0.95:
            logger.error("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –£–¢–ï–ß–ö–ê: train accuracy >95%!")
            logger.error("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏/—Ä–∞–∑–º–µ—Ç–∫—É –Ω–∞ forward-looking –¥–∞–Ω–Ω—ã–µ!")

        if abs(train_acc - val_acc) > 0.20:
            logger.warning(f"‚ö†Ô∏è  –°–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: gap={train_acc - val_acc:.2%}")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–∏–≥–Ω–∞–ª–æ–≤ –∏ –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–æ–≤ (–ù–ê TEST!)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # bars_per_day –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑ run_id (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
        bars_per_day = _infer_bars_per_day_from_run_id(run_id, default=TIMEFRAME_TO_BARS.get(str(self.timeframe).lower(), 288))
        candidates = []
        # –ü–µ—Ä–µ–±–æ—Ä precision_min –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï
        precision_grid = [0.70, 0.75, 0.80, 0.85, 0.90]

        for idx, pm in enumerate(precision_grid):
            try:
                tau_i, tstats_i = self.tune_tau_for_spd_range(
                    y_val=np.asarray(y_test),
                    proba=np.asarray(y_test_pred_proba),
                    bars_per_day=bars_per_day,
                    spd_min=20.0,
                    spd_max=35.0,
                    precision_min=pm,
                    delta=0.06,
                    cooldown_bars=2,
                    log_stats=(idx == 0),  # –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å max-proba stats —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                )
                candidates.append({
                    'precision_min': pm,
                    'tau': float(tau_i),
                    'spd': float(tstats_i.get('spd', float('nan'))),
                    'precision_macro_buy_sell': float(tstats_i.get('precision_macro_buy_sell', float('nan'))),
                    'f1_macro_buy_sell': float(tstats_i.get('f1_macro_buy_sell', float('nan'))),
                    'hit_range': bool(tstats_i.get('hit_range', False)),
                    'delta': float(tstats_i.get('delta', 0.08)),
                    'cooldown_bars': int(tstats_i.get('cooldown_bars', 2)),
                    '_tstats': tstats_i,
                })
            except Exception as e:
                logging.warning(f"precision_min={pm:.2f}: sweep failed with error: {e}")

        def _key(c):
            return (1 if c.get('hit_range') else 0,
                    c.get('precision_macro_buy_sell', float('-inf')),
                    c.get('f1_macro_buy_sell', float('-inf')),
                    -c.get('tau', float('inf')))

        if not candidates:
            raise RuntimeError("Precision sweep failed: no candidates collected")

        best = max(candidates, key=_key)
        tau = best['tau']
        tstats = best['_tstats']
        delta = best.get('delta', 0.08)
        cooldown_bars = best.get('cooldown_bars', 2)

        logging.info("üîß Precision sweep results (–Ω–∞ TEST –Ω–∞–±–æ—Ä–µ):")
        for c in candidates:
            logging.info(f"  pm={c['precision_min']:.2f}, tau={c['tau']:.3f}, spd‚âà{c['spd']:.1f}, "
                         f"prec‚âà{c['precision_macro_buy_sell']:.3f}, f1‚âà{c['f1_macro_buy_sell']:.3f}, "
                         f"hit={c['hit_range']}")
        logging.info(f"‚úÖ Picked precision_min={best['precision_min']:.2f} ‚Üí "
                     f"tau={tau:.3f}, spd‚âà{best['spd']:.1f}")

        logger.info(
            "üîß Tuned thresholds: tau=%.3f, delta=%.2f, cooldown=%d ‚Üí spd‚âà%.1f/day, "
            "precision‚âà%.3f, recall‚âà%.3f, f1‚âà%.3f (hit_range=%s)"
            % (tau, tstats['delta'], tstats['cooldown_bars'],
               tstats['spd'], tstats['precision_macro_buy_sell'],
               tstats['recall_macro_buy_sell'], tstats['f1_macro_buy_sell'],
               tstats['hit_range'])
        )

        # Sensitivity –∞–Ω–∞–ª–∏–∑ (–ù–ê TEST)
        _tau_offsets = [-0.05, -0.03, -0.02, 0.0, 0.02, 0.03, 0.05]
        _delta_offsets = [-0.02, 0.0, 0.02]

        tau_sensitivity = []
        for off in _tau_offsets:
            tau_x = float(np.clip(tau + off, 0.0, 1.0))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_test),
                proba=np.asarray(y_test_pred_proba),
                tau=tau_x,
                delta=delta,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            tau_sensitivity.append(r)

        delta_sensitivity = []
        for off in _delta_offsets:
            delta_x = float(max(0.0, delta + off))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_test),
                proba=np.asarray(y_test_pred_proba),
                tau=tau,
                delta=delta_x,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            delta_sensitivity.append(r)

        _tau_sorted = sorted(tau_sensitivity, key=lambda r: abs(r['tau'] - float(tau)))[:3]
        _tau_sorted = sorted(_tau_sorted, key=lambda r: r['tau'])

        logging.info("üîç Sensitivity (tau near current):")
        for r in _tau_sorted:
            logging.info(f"  tau={r['tau']:.3f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        logging.info("üîç Sensitivity (delta¬±0.02):")
        for r in delta_sensitivity:
            logging.info(f"  delta={r['delta']:.2f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –Ω–∞–±–æ—Ä–æ–≤
        train_dist = Counter(y_train)
        val_dist = Counter(y_val)
        test_dist = Counter(y_test)
        pred_val_dist = Counter(y_val_pred)
        pred_test_dist = Counter(y_test_pred)

        logger.info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"  Train:     {dict(train_dist)}")
        logger.info(f"  Val:       {dict(val_dist)}")
        logger.info(f"  Test:      {dict(test_dist)}")
        logger.info(f"  Pred Val:  {dict(pred_val_dist)}")
        logger.info(f"  Pred Test: {dict(pred_test_dist)}")

        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ
        prec_val, rec_val, f1_val, _ = precision_recall_fscore_support(
            y_val, y_val_pred,
            labels=REPORT_LABELS,
            average=None,
            zero_division=0
        )
        cm_val = confusion_matrix(y_val, y_val_pred, labels=REPORT_LABELS)

        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ (–¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)
        prec_test, rec_test, f1_test, _ = precision_recall_fscore_support(
            y_test, y_test_pred,
            labels=REPORT_LABELS,
            average=None,
            zero_division=0
        )
        cm_test = confusion_matrix(y_test, y_test_pred, labels=REPORT_LABELS)

        decision_policy = {
            'tau': tau,
            'delta': tstats['delta'],
            'cooldown_bars': tstats['cooldown_bars'],
            'bars_per_day': bars_per_day,
            'test_spd': tstats['spd'],
            'test_precision_macro_buy_sell': tstats['precision_macro_buy_sell'],
            'test_recall_macro_buy_sell': tstats['recall_macro_buy_sell'],
            'test_f1_macro_buy_sell': tstats['f1_macro_buy_sell'],
            'target_spd_range': tstats['range'],
            'hit_range': tstats['hit_range'],
            'precision_min': best.get('precision_min', 0.60),
        }

        precision_min_sweep = [
            {
                'precision_min': c['precision_min'],
                'tau': c['tau'],
                'spd': c['spd'],
                'precision_macro_buy_sell': c['precision_macro_buy_sell'],
                'f1_macro_buy_sell': c['f1_macro_buy_sell'],
                'hit_range': c['hit_range'],
            }
            for c in candidates
        ]

        metrics = {
            'decision_policy': decision_policy,
            'precision_min_sweep': precision_min_sweep,

            # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            'val_accuracy': float(val_acc),
            'val_precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec_val)},
            'val_recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec_val)},
            'val_f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1_val)},
            'val_confusion_matrix': cm_val.tolist(),

            # –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            'test_accuracy': float(test_acc),
            'test_precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec_test)},
            'test_recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec_test)},
            'test_f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1_test)},
            'test_confusion_matrix': cm_test.tolist(),

            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'class_distribution': {
                'train': {int(k): int(v) for k, v in train_dist.items()},
                'val': {int(k): int(v) for k, v in val_dist.items()},
                'test': {int(k): int(v) for k, v in test_dist.items()},
            },
            'tau_sensitivity': tau_sensitivity,
            'delta_sensitivity': delta_sensitivity,
            'lookback_window': self.lookback,
            'base_features_count': len(self.base_feature_names),
            'total_features_count': len(self.feature_names),
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        os.makedirs("models", exist_ok=True)
        model_filename = f"models/ml_windowed_{self.symbol.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"

        model_metadata = {
            'version': '2.1.1',
            'format': 'windowed_lgb',
            'instrument': self.symbol,
            'exchange': 'Binance',
            'timeframe': '5m',
            'lookback_window': self.lookback,
            'base_feature_count': len(self.base_feature_names),
            'total_feature_count': len(self.feature_names),
            'trained_at': datetime.now().isoformat(),
            'training_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test),
            'val_accuracy': float(val_acc),
            'test_accuracy': float(test_acc),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'run_id': run_id,
            'decision_policy': decision_policy,
            'scaler_used': use_scaler,
        }

        model_package = {
            'model': model,
            'scaler': scaler,
            'metadata': model_metadata,
            'base_feature_names': self.base_feature_names,
            'lookback': self.lookback,
            'timeframe': '5m',
            'min_confidence': 0.65,
            'required_warmup': 20
        }

        joblib.dump(model_package, model_filename)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
        logger.info(f"   - Lookback: {self.lookback} –±–∞—Ä–æ–≤")
        logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names)}")
        logger.info(f"   - Scaler: {'StandardScaler' if scaler else 'None'}")

        # Tau curves (–ù–ê TEST)
        try:
            tau_left = max(0.0, float(tau) - 0.05)
            tau_right = min(0.999, float(tau) + 0.05)
            tau_grid = np.arange(tau_left, tau_right + 1e-9, 0.002)

            spd_curve = []
            f1_curve = []
            for tcur in tau_grid:
                s = self._eval_decision_metrics(
                    y_true=np.asarray(y_test),
                    proba=np.asarray(y_test_pred_proba),
                    tau=float(tcur),
                    delta=float(delta),
                    cooldown_bars=int(cooldown_bars),
                    bars_per_day=int(bars_per_day),
                )
                spd_curve.append(s['spd'])
                f1_curve.append(s['f1_macro_buy_sell'])

            os.makedirs("models/training_logs", exist_ok=True)
            curve_prefix = str(Path("models/training_logs") / Path(model_filename).with_suffix('').name)

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, spd_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('SPD vs tau (–Ω–∞ TEST –Ω–∞–±–æ—Ä–µ)')
            plt.xlabel('tau')
            plt.ylabel('signals per day')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_spd.png")
            plt.close()

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, f1_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('F1 (macro BUY/SELL on act) vs tau (–Ω–∞ TEST –Ω–∞–±–æ—Ä–µ)')
            plt.xlabel('tau')
            plt.ylabel('F1 macro (BUY/SELL)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_f1.png")
            plt.close()

        except Exception as _e:
            logging.warning(f"tau curves plotting skipped: {_e}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        self.save_training_report(metrics, model_filename)

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º TEST –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)
        diag_prefix = Path("models/training_logs") / Path(model_filename).with_suffix('').name
        self.post_training_diagnostics(
            model=model,
            X_val=X_test,
            y_val=y_test,
            y_val_pred_proba=y_test_pred_proba,
            prefix_path=str(diag_prefix),
            bars_per_day=bars_per_day
        )

        return metrics

    def plot_precision_spd_curve(self, y_val, y_val_pred_proba, bars_per_day: int,
                                 delta: float = 0.08, cooldown_bars: int = 2,
                                 prefix_path: str = "models/training_logs/diag") -> None:
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ SPD(œÑ) –∏ Precision/Recall/F1 –æ—Ç SPD
        """
        proba = np.asarray(y_val_pred_proba)
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        def _apply_cooldown(mask: np.ndarray, cd: int) -> np.ndarray:
            if cd <= 0:
                return mask
            idx = np.where(mask)[0]
            if idx.size == 0:
                return mask
            keep = [idx[0]]
            last = idx[0]
            for i in idx[1:]:
                if i - last >= cd:
                    keep.append(i)
                    last = i
            out = np.zeros_like(mask, dtype=bool)
            out[np.array(keep, dtype=int)] = True
            return out

        taus = np.linspace(0.45, 0.70, 26)
        rows = []
        n = len(y_val)

        for tau in taus:
            act = (maxp >= tau) & (margin >= delta)
            act = _apply_cooldown(act, cooldown_bars)

            signals = int(act.sum())
            spd = signals * bars_per_day / max(1, n)

            if signals == 0:
                prec = rec = f1 = 0.0
            else:
                pred_dir = np.where(p_buy[act] >= p_sell[act], 1, 2)
                true_dir = y_val[act]
                prec = precision_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                rec = recall_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                f1 = f1_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)

            rows.append((tau, spd, prec, rec, f1, signals))

        df = pd.DataFrame(rows, columns=['tau', 'spd_per_day', 'precision', 'recall', 'f1', 'signals'])
        csv_path = f"{prefix_path}_tau_sweep.csv"
        df.to_csv(csv_path, index=False)

        # –ì—Ä–∞—Ñ–∏–∫ SPD(œÑ)
        plt.figure(figsize=(8, 5))
        plt.plot(df['tau'], df['spd_per_day'], marker='o')
        plt.xlabel('tau')
        plt.ylabel('SPD (signals/day)')
        plt.title('Signals per day vs tau')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_spd_vs_tau.png")
        plt.close()

        # –ì—Ä–∞—Ñ–∏–∫ Precision/Recall/F1 vs SPD
        plt.figure(figsize=(8, 5))
        plt.plot(df['spd_per_day'], df['precision'], marker='o', label='Precision (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['recall'], marker='o', label='Recall (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['f1'], marker='o', label='F1 (macro BUY/SELL)')
        plt.xlabel('SPD (signals/day)')
        plt.ylabel('score')
        plt.title('Precision / Recall / F1 vs SPD')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_prf_vs_spd.png")
        plt.close()

    def post_training_diagnostics(self, model, X_val, y_val, y_val_pred_proba,
                                  prefix_path: str, bars_per_day: int = 288):
        """
        –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:
        - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        - PR-–∫—Ä–∏–≤—ã–µ
        - SPD curves
        """
        os.makedirs(os.path.dirname(prefix_path), exist_ok=True)

        try:
            bars_per_day = int(bars_per_day) if bars_per_day is not None else 288
        except Exception:
            bars_per_day = 288

        proba = np.asarray(y_val_pred_proba)
        if proba.ndim == 1:
            tmp = np.zeros((len(proba), 3), dtype=float)
            tmp[np.arange(len(proba)), np.clip(proba.astype(int), 0, 2)] = 1.0
            proba = tmp

        p_hold = proba[:, 0]
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]

        feat_names = self.feature_names if hasattr(self, 'feature_names') else [f"f{i}" for i in range(X_val.shape[1])]

        # === 1) Feature Importance ===
        try:
            gain = model.feature_importance(importance_type='gain')
            df_imp = (pd.DataFrame({'feature': feat_names, 'gain': gain})
                      .sort_values('gain', ascending=False)
                      .head(30))
            plt.figure(figsize=(10, max(8, 0.3 * len(df_imp))))
            sns.barplot(data=df_imp, x='gain', y='feature')
            plt.title('Feature Importance (gain) ‚Äî top 30')
            plt.tight_layout()
            plt.savefig(f"{prefix_path}_feat_importance.png")
            plt.close()

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å CSV —Å –í–°–ï–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
            pd.DataFrame({'feature': feat_names, 'gain': gain}).sort_values('gain', ascending=False).to_csv(
                f"{prefix_path}_feat_importance.csv", index=False
            )

            # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ  –±–∞–∑–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–ø–æ –≤—Å–µ–º –ª–∞–≥–∞–º)
            base_feat_importance = {}
            for feature, importance in zip(feat_names, gain):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ (—É–±–∏—Ä–∞–µ–º _t0, _t-1 –∏ —Ç.–¥.)
                if '_t-' in feature:
                    base_feat = feature.split('_t-')[0]  # cmo_14_t-1 -> cmo_14
                elif '_t0' in feature:
                    base_feat = feature.replace('_t0', '')  # cmo_14_t0 -> cmo_14
                else:
                    base_feat = feature  # fallback

                base_feat_importance[base_feat] = base_feat_importance.get(base_feat, 0) + importance

            df_base_imp = pd.DataFrame({
                'base_feature': list(base_feat_importance.keys()),
                'total_gain': list(base_feat_importance.values())
            }).sort_values('total_gain', ascending=False)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Ç–∞–±–ª–∏—Ü—É —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
            df_base_imp.to_csv(f"{prefix_path}_feat_importance_base_aggregated.csv", index=False)

            # –õ–æ–≥–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            logger.info(f"üéØ –í–ê–ñ–ù–û–°–¢–¨ {len(self.base_feature_names)} –ë–ê–ó–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –≤—Å–µ–º –ª–∞–≥–∞–º):")
            for i, row in df_base_imp.iterrows():
                logger.info(f"   {i + 1:2d}. {row['base_feature']}: {row['total_gain']:.0f}")

        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

        # === 2) –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã ===
        y_pred = proba.argmax(axis=1)

        def hist_one(prob, true_class, name, fname):
            mask_pos = (y_val == true_class)
            mask_pred_pos = (y_pred == true_class)

            tp = prob[mask_pos & mask_pred_pos]
            fp = prob[(~mask_pos) & mask_pred_pos]
            fn = prob[mask_pos & (~mask_pred_pos)]
            tn = prob[(~mask_pos) & (~mask_pred_pos)]

            plt.figure(figsize=(8, 5))
            bins = 30
            if len(tp) > 0:
                sns.histplot(tp, bins=bins, stat='density', label='TP', alpha=0.6)
            if len(fp) > 0:
                sns.histplot(fp, bins=bins, stat='density', label='FP', alpha=0.6)
            if len(fn) > 0:
                sns.histplot(fn, bins=bins, stat='density', label='FN', alpha=0.6)
            if len(tn) > 0:
                sns.histplot(tn, bins=bins, stat='density', label='TN', alpha=0.6)
            plt.legend()
            plt.xlabel(f"p({name})")
            plt.ylabel("density")
            plt.title(f"Distributions for {name}")
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()

        hist_one(p_buy, 1, "BUY", f"{prefix_path}_proba_hist_BUY.png")
        hist_one(p_sell, 2, "SELL", f"{prefix_path}_proba_hist_SELL.png")

        # === 3) Max-proba scatter ===
        maxp = proba.max(axis=1)
        plt.figure(figsize=(8, 5))
        sns.scatterplot(x=np.arange(len(maxp)), y=maxp,
                        hue=[{0: 'HOLD', 1: 'BUY', 2: 'SELL'}.get(c, 'UNK') for c in y_val],
                        s=12, linewidth=0)
        plt.title("Max class probability vs true class (val order)")
        plt.xlabel("index in validation set (chronological)")
        plt.ylabel("max proba")
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_maxproba_scatter.png")
        plt.close()

        # === 4) PR curves ===
        Y_bin = label_binarize(y_val, classes=[0, 1, 2])
        curves = [
            ("BUY", Y_bin[:, 1], p_buy),
            ("SELL", Y_bin[:, 2], p_sell),
            ("HOLD", Y_bin[:, 0], p_hold),
        ]
        plt.figure(figsize=(8, 6))
        for name, y_true_bin, y_score in curves:
            precision, recall, _ = precision_recall_curve(y_true_bin, y_score)
            ap = average_precision_score(y_true_bin, y_score)
            plt.plot(recall, precision, label=f"{name} (AP={ap:.3f})")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision‚ÄìRecall curves (one-vs-rest)")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_pr_curves.png")
        plt.close()

        # === 5) SPD curve ===
        self.plot_precision_spd_curve(
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            bars_per_day=bars_per_day,
            delta=0.08,
            cooldown_bars=2,
            prefix_path=prefix_path
        )

    def save_training_report(self, metrics: dict, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –æ–±—É—á–µ–Ω–∏–∏"""
        os.makedirs("models/training_logs", exist_ok=True)

        report = {
            'training_date': datetime.now().isoformat(),
            'symbol': self.symbol,
            'db_dsn': self.db_dsn,
            'model_path': model_path,
            'lookback_window': self.lookback,
            'metrics': metrics,
            'base_feature_names': self.base_feature_names,
            'total_feature_names_count': len(self.feature_names),
        }

        report_filename = model_path.replace('.joblib', '_report.json')
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)

        # Confusion matrices (val –∏ test)
        try:
            labels = ['BUY', 'SELL', 'HOLD']

            cm_val = np.array(metrics.get('val_confusion_matrix', []))
            if cm_val.size > 0:
                plt.figure(figsize=(8, 6))
                sns.heatmap(cm_val, annot=True, fmt='d',
                            xticklabels=labels, yticklabels=labels)
                plt.title('Validation Confusion Matrix')
                plt.tight_layout()
                plt.savefig(report_filename.replace('.json', '_cm_val.png'))
                plt.close()

            cm_test = np.array(metrics.get('test_confusion_matrix', []))
            if cm_test.size > 0:
                plt.figure(figsize=(8, 6))
                sns.heatmap(cm_test, annot=True, fmt='d',
                            xticklabels=labels, yticklabels=labels)
                plt.title('Test Confusion Matrix')
                plt.tight_layout()
                plt.savefig(report_filename.replace('.json', '_cm_test.png'))
                plt.close()
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é CM: {e}")

        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")

    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ"""
        self.data_loader.close()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò (v2.1 - WINDOWED)")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ë–î
    db_file = DATA_DIR / "market_data.sqlite"
    if not db_file.exists():
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {db_file} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   1. ml_data_preparation.py")
        print("   2. ml_labeling_tool_v3.py")
        return 1

    trainer = None
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        db_dsn = MARKET_DB_DSN
        symbol = "ETHUSDT"
        lookback = LOOKBACK_WINDOW

        # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π run_id –î–õ–Ø –ö–û–ù–ö–†–ï–¢–ù–û–ì–û symbol/timeframe
        engine = create_engine(MARKET_DB_DSN)
        with engine.connect() as conn:
            row = conn.execute(
                text("""
                    SELECT run_id 
                      FROM training_dataset_meta 
                     WHERE status='READY' AND symbol=:symbol AND timeframe='5m'
                     ORDER BY created_at DESC LIMIT 1
                """),
                {"symbol": symbol}
            ).fetchone()
        engine.dispose()

        if not row:
            print("‚ùå –ù–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö snapshot –≤ training_dataset_meta –¥–ª—è ETHUSDT/5m!")
            print("   –°–æ–∑–¥–∞–π—Ç–µ snapshot —á–µ—Ä–µ–∑ [14] –≤ ml_labeling_tool_v3.py")
            return 1

        run_id = row[0]

        print(f"üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DATA_DIR / 'market_data.sqlite'}")
        print(f"üéØ –°–∏–º–≤–æ–ª: {symbol}")
        print(f"üì¶ Run ID: {run_id}")
        print(f"ü™ü Lookback Window: {lookback} –±–∞—Ä–æ–≤")
        print("=" * 50)

        # –û–±—É—á–µ–Ω–∏–µ
        trainer = ModelTrainer(db_dsn, symbol, lookback=lookback)

        use_scaler = False  # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å True
        metrics = trainer.train_model(run_id, use_scaler=use_scaler)

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   Val Accuracy:  {metrics['val_accuracy']:.4f}")
        print(f"   Test Accuracy: {metrics['test_accuracy']:.4f}")

        print(f"\n   VAL Precision BUY/SELL/HOLD: "
              f"{metrics['val_precision']['BUY']:.4f}/"
              f"{metrics['val_precision']['SELL']:.4f}/"
              f"{metrics['val_precision']['HOLD']:.4f}")
        print(f"   TEST Precision BUY/SELL/HOLD: "
              f"{metrics['test_precision']['BUY']:.4f}/"
              f"{metrics['test_precision']['SELL']:.4f}/"
              f"{metrics['test_precision']['HOLD']:.4f}")

        print(f"\n   VAL Recall BUY/SELL/HOLD: "
              f"{metrics['val_recall']['BUY']:.4f}/"
              f"{metrics['val_recall']['SELL']:.4f}/"
              f"{metrics['val_recall']['HOLD']:.4f}")
        print(f"   TEST Recall BUY/SELL/HOLD: "
              f"{metrics['test_recall']['BUY']:.4f}/"
              f"{metrics['test_recall']['SELL']:.4f}/"
              f"{metrics['test_recall']['HOLD']:.4f}")
        return 0

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1
    finally:
        if trainer:
            trainer.close()


if __name__ == '__main__':
    sys.exit(main())