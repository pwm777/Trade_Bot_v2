# train_ml_global_v2.py
"""
ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LightGBM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞĞšĞĞĞœ Ğ˜Ğ¡Ğ¢ĞĞ Ğ˜Ğ˜

ĞĞ²Ñ‚Ğ¾Ñ€: pwm777
Ğ”Ğ°Ñ‚Ğ°: 2025-11-17
Ğ’ĞµÑ€ÑĞ¸Ñ: 2.1.1 (windowed training)

Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:
- Lookback ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ¾Ğ¹ LOOKBACK_WINDOW (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 11 Ğ±Ğ°Ñ€Ğ¾Ğ²)
- ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ = Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N Ğ±Ğ°Ñ€Ğ¾Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (Ğ¾ĞºĞ½Ğ¾), Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº [t0, t-1, ..., t-(N-1)]
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: tau tuning, diagnostics, plots, Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹
"""

import sys
import logging
from sqlalchemy import create_engine, text
from datetime import datetime
import json
from typing import Tuple
import warnings
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import joblib
from pathlib import Path
from config import BASE_FEATURE_NAMES
warnings.filterwarnings('ignore')
import re
import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize, StandardScaler
from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOOKBACK_WINDOW = 11  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±Ğ°Ñ€Ğ¾Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°
TIMEFRAME_TO_BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ£Ğ¢Ğ˜Ğ›Ğ˜Ğ¢Ğ«
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _infer_bars_per_day_from_run_id(run_id: str, default: int = 288) -> int:
    """
    ĞŸÑ‹Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹Ñ‚Ğ°Ñ‰Ğ¸Ñ‚ÑŒ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼ Ğ¸Ğ· run_id Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° ..._<tf>_...
    Ğ¸ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±Ğ°Ñ€Ğ¾Ğ² Ğ² ÑÑƒÑ‚ĞºĞ°Ñ….
    """
    m = re.search(r"_(\d+[mh])_", str(run_id).lower())
    if not m:
        return default
    tf = m.group(1)
    timeframe_to_bars_local = {
        "1m": 1440,
        "3m": 480,
        "5m": 288,
        "15m": 96,
        "30m": 48,
        "1h": 24,
        "2h": 12,
        "4h": 6,
        "6h": 4,
        "12h": 2,
        "1d": 1,
    }
    return timeframe_to_bars_local.get(tf, default)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞĞ›Ğ›Ğ‘Ğ­Ğš Â«Ğ¢Ğ•Ğ ĞœĞĞœĞ•Ğ¢Ğ  ĞŸĞ ĞĞ“Ğ Ğ•Ğ¡Ğ¡ĞÂ» Ğ”Ğ›Ğ¯ LIGHTGBM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def thermometer_progress_callback(logger: logging.Logger, width: int = 30, period: int = 10):
    """ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ Ğ¿Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ±ÑƒÑÑ‚Ğ¸Ğ½Ğ³Ğ°"""
    import sys

    def _cb(env):
        begin = getattr(env, 'begin_iteration', 0) or 0
        end = getattr(env, 'end_iteration', None)
        if end is None or end <= begin:
            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ‰ĞµĞµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (Ğ² params num_boost_round Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ½ĞµÑ‚)
            total = max(1, (getattr(env, 'iteration', 0) or 0) + 1)
        else:
            total = max(1, end - begin)

        iter_now = int(getattr(env, 'iteration', 0) or 0)
        done = max(0, iter_now - begin + 1)
        pct = min(1.0, max(0.0, done / total))
        filled = int(round(pct * width))
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        val_metric_name = None
        val_metric_value = None
        evals = getattr(env, 'evaluation_result_list', None)
        if evals:
            for res in evals:
                if isinstance(res, (list, tuple)) and len(res) >= 3:
                    data_name, metric_name, metric_val = res[0], res[1], res[2]
                    if str(data_name).startswith(('valid', 'val')):
                        val_metric_name = str(metric_name)
                        try:
                            val_metric_value = float(metric_val)
                        except Exception:
                            val_metric_value = None
                        break

        should_print = (iter_now > 0 and iter_now % period == 0) or (done >= total)

        if should_print:
            if val_metric_name is not None and val_metric_value is not None:
                msg = f"[{iter_now:4d}/{total}] {val_metric_name}:{val_metric_value:.5f} | {bar} {int(pct * 100):3d}%"
            else:
                msg = f"[{iter_now:4d}/{total}] {bar} {int(pct * 100):3d}%"

            if done >= total:
                print(f"\r{msg}")
                sys.stdout.flush()
            else:
                print(f"\r{msg}", end='', flush=True)

    return _cb


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞ›ĞĞ¡Ğ¡ Ğ”Ğ›Ğ¯ Ğ ĞĞ‘ĞĞ¢Ğ« Ğ¡ Ğ‘ĞĞ—ĞĞ™ Ğ”ĞĞĞĞ«Ğ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DataLoader:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· SQLite Ğ±Ğ°Ğ·Ñ‹ ml_labeling_tool_v3.py"""

    def __init__(self, db_dsn: str = MARKET_DB_DSN, symbol: str = "ETHUSDT"):
        self.db_dsn = db_dsn
        self.db_path = DATA_DIR / "market_data.sqlite"
        self.symbol = symbol
        self.engine = None

    def connect(self):
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ‘Ğ”"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {self.db_path}")
        self.engine = create_engine(self.db_dsn)
        logger.info(f"âœ… ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾ Ğº Ğ‘Ğ”: {self.db_path}")

    def close(self):
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ"""
        if self.engine:
            self.engine.dispose()
            logger.info("âœ… Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ‘Ğ” Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾")

    def load_market_data(self) -> pd.DataFrame:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑĞ²ĞµÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· candles_5m"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM candles_5m 
            WHERE symbol = :symbol 
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"symbol": self.symbol})

        if df.empty:
            raise ValueError(f"ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ° {self.symbol}")

        # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
        if 'ts' in df.columns and 'datetime' not in df.columns:
            df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

        logger.info(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑĞ²ĞµÑ‡ĞµĞ¹ Ğ¸Ğ· candles_5m")
        return df

    def load_training_dataset(self, run_id: str) -> pd.DataFrame:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ· training_dataset"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM training_dataset
            WHERE run_id = :run_id
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"run_id": run_id})

        if df.empty:
            raise ValueError(f"âŒ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ run_id={run_id}")

        logger.info(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· training_dataset")
        logger.info(f"   ĞšĞ»Ğ°ÑÑÑ‹: {df['reversal_label'].value_counts().to_dict()}")
        return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ ĞšĞ›ĞĞ¡Ğ¡ ModelTrainer Ğ¡ ĞĞšĞĞĞœ Ğ˜Ğ¡Ğ¢ĞĞ Ğ˜Ğ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ModelTrainer:
    def __init__(self, db_dsn: str, symbol: str, lookback: int = LOOKBACK_WINDOW):
        self.db_dsn = db_dsn
        self.symbol = symbol
        self.lookback = lookback
        self.timeframe = "5m"  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        self.data_loader = DataLoader(db_dsn, symbol)
        self.base_feature_names = BASE_FEATURE_NAMES

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ»Ğ°Ğ³Ğ°Ğ¼Ğ¸
        self.feature_names = self._generate_windowed_feature_names()
        logger.info(f"ğŸ“Š Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(self.feature_names)} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² "
                    f"({len(self.base_feature_names)} Ã— {lookback} Ğ±Ğ°Ñ€Ğ¾Ğ²)")

    def _generate_windowed_feature_names(self) -> list:
        """
        Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ¼ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ»Ğ°Ğ³Ğ¾Ğ²
        ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: cmo_14_t0, cmo_14_t-1, ..., cmo_14_t-(lookback-1)
        """
        names = []
        # t0 - Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ±Ğ°Ñ€ (ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹)
        for feat in self.base_feature_names:
            names.append(f"{feat}_t0")

        # t-1, t-2, ..., t-(lookback-1) - Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        for lag in range(1, self.lookback):
            for feat in self.base_feature_names:
                names.append(f"{feat}_t-{lag}")

        return names

    def prepare_training_data(self, run_id: str) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """
        ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ ĞĞŸĞ¢Ğ˜ĞœĞ˜Ğ—Ğ˜Ğ ĞĞ’ĞĞĞĞĞ¯ Ğ’Ğ•Ğ Ğ¡Ğ˜Ğ¯)
        """
        df = self.data_loader.load_training_dataset(run_id)

        logger.info(f"ğŸ”„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºĞ¾Ğ½ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (lookback={self.lookback})...")

        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ ĞºĞ»Ğ°ÑÑ 3 Ğ¡Ğ ĞĞ—Ğ£
        df_filtered = df[df['reversal_label'] != 3].copy()
        skipped = len(df) - len(df_filtered)

        if skipped > 0:
            logger.info(f"âš ï¸  ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ {skipped} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ¼ 3")

        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² numpy array Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸
        feature_matrix = df_filtered[self.base_feature_names].values
        labels = df_filtered['reversal_label'].values
        weights = df_filtered['sample_weight'].values

        n_samples = len(df_filtered)
        n_features = len(self.base_feature_names)

        # âœ… Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ: ĞĞºĞ½Ğ¾ Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞĞ Ğ¼ĞµÑ‚ĞºĞµ (Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾)
        # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ i Ğ½ÑƒĞ¶Ğ½Ğ¾ lookback Ğ±Ğ°Ñ€Ğ¾Ğ² Ğ’ĞšĞ›Ğ®Ğ§ĞĞ¯ i
        # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚ĞºĞ¸: lookback-1 (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ±Ñ‹Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸)
        n_valid = n_samples - (self.lookback - 1)

        if n_valid <= 0:
            raise ValueError(f"ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ lookback={self.lookback}")

        logger.info(f"   Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ {n_valid} Ğ¾ĞºĞ¾Ğ½ Ğ¸Ğ· {n_samples} Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ²...")

        # ĞŸÑ€ĞµĞ´Ğ°Ğ»Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        X_windowed = np.zeros((n_valid, self.lookback * n_features), dtype=np.float32)

        # âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ:
        # ĞœĞµÑ‚ĞºĞ° Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ label_idx (ÑÑ‚Ğ¾ extreme_timestamp)
        # ĞĞºĞ½Ğ¾: [label_idx - (lookback-1), .. ., label_idx-1, label_idx]
        # Ğ¢Ğ¾ ĞµÑÑ‚ÑŒ lookback Ğ±Ğ°Ñ€Ğ¾Ğ², Ğ—ĞĞšĞĞĞ§Ğ˜Ğ’ĞĞ®Ğ©Ğ˜Ğ¥Ğ¡Ğ¯ Ğ½Ğ° label_idx

        for i in range(n_valid):
            label_idx = i + (self.lookback - 1)  # ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğµ
            start_idx = label_idx - (self.lookback - 1)  # ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾ĞºĞ½Ğ°
            end_idx = label_idx + 1  # ĞšĞ¾Ğ½ĞµÑ† Ğ¾ĞºĞ½Ğ° (ÑĞºÑĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ğ¾)

            # ĞĞºĞ½Ğ¾: [start_idx : end_idx] = lookback Ğ±Ğ°Ñ€Ğ¾Ğ²
            window = feature_matrix[start_idx:end_idx, :]  # shape: (lookback, n_features)

            # ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: [t0, t-1, t-2, ..., t-(lookback-1)]
            # Ğ³Ğ´Ğµ t0 = label_idx (Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ±Ğ°Ñ€ Ñ Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹)
            window_ordered = window[::-1]  # Ğ Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼: Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ±Ğ°Ñ€ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼

            X_windowed[i] = window_ordered.ravel()

        # âœ… ĞœĞµÑ‚ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ label_idx Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°
        y_windowed = labels[self.lookback - 1:]
        w_windowed = weights[self.lookback - 1:]

        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² DataFrame
        X_df = pd.DataFrame(X_windowed, columns=self.feature_names)
        y_series = pd.Series(y_windowed, name='label')
        w_series = pd.Series(w_windowed, name='weight')

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ²
        missing = X_df.isnull().sum()
        if missing.any():
            logger.warning(f"âš ï¸  ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¸, Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ½ÑƒĞ»ÑĞ¼Ğ¸...")
            X_df = X_df.fillna(0)

        logger.info(f"âœ… ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: {len(X_df)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², {len(self.feature_names)} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²")
        logger.info(f"   Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ²: {y_series.value_counts().to_dict()}")

        return X_df, y_series, w_series

    def tune_tau_for_spd_range(
            self,
            y_val: np.ndarray,
            proba: np.ndarray,
            bars_per_day: int,
            spd_min: float = 20.0,  # Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½
            spd_max: float = 35.0,
            precision_min: float = 0.70,
            delta: float = 0.06,
            cooldown_bars: int = 2,
            log_stats: bool = True,  # â† Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ maxp Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·
    ):
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)

        # Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ tau Ğ¸Ğ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ĞµĞ¹
        taus = np.quantile(maxp, np.linspace(0.05, 0.95, 100))

        # Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ: Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ maxp Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ñ„Ğ»Ğ°Ğ³Ñƒ
        if log_stats:
            logging.info(f"ğŸ“Š Max probability stats: "
                         f"mean={maxp.mean():.3f}, "
                         f"50%={np.percentile(maxp, 50):.3f}, "
                         f"90%={np.percentile(maxp, 90):.3f}")

        best_in = None  # (cand, key)
        best_near = None  # (cand, keyn)
        target = 0.5 * (spd_min + spd_max)
        n = len(y_val)

        for tau_cand in sorted(taus):
            # ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº/act Ñ‡ĞµÑ€ĞµĞ· helper
            stats = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(tau_cand),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd = stats['spd']
            prec = stats['precision_macro_buy_sell']
            rec = stats['recall_macro_buy_sell']
            f1 = stats['f1_macro_buy_sell']
            # Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· SPD (Ğ¿Ğ¾ÑĞ»Ğµ cooldown)
            signals = int(round(spd * max(1, n) / max(1, bars_per_day)))

            cand = (float(tau_cand), float(spd), float(prec), float(rec), float(f1), int(signals))

            # Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° â€” Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ precision, Ğ·Ğ°Ñ‚ĞµĞ¼ F1, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ñ†ĞµĞ½Ñ‚Ñ€Ñƒ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ï„
            if spd_min <= spd <= spd_max and prec >= precision_min:
                key = (prec, f1, -abs(spd - target), float(tau_cand))
                if (best_in is None) or (key > best_in[1]):
                    best_in = (cand, key)

            # Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğ¹ Ğº Ñ†ĞµĞ½Ñ‚Ñ€Ñƒ â€” Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹, ĞµÑĞ»Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ Ğ½ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…
            gap = abs(spd - target)
            keyn = (-gap, prec, f1)  # Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ gap, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ°ĞºÑ. precision/F1
            if (best_near is None) or (keyn > best_near[1]):
                best_near = (cand, keyn)

        # Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€
        chosen = (best_in[0] if best_in is not None else best_near[0])
        tau_chosen, spd, prec, rec, f1, signals = chosen

        # â”€â”€ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´ÑŠÑ‘Ğ¼ Ï„ Ğ²Ğ²ĞµÑ€Ñ… (ĞµÑĞ»Ğ¸ Ğ½Ğµ Ñ…ÑƒĞ¶Ğµ Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‘Ğ¼ÑÑ Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ Ğ¾ĞºĞ½Ğµ SPD) â”€â”€
        upper = min(float(tau_chosen) + 0.05, 0.999)
        ref_grid = np.linspace(float(tau_chosen), upper, 31)  # ÑˆĞ°Ğ³ â‰ˆ0.0017

        best_ref = None  # (key_ref, t, stats_ref)
        for t in ref_grid:
            stats_ref = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(t),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd_r = stats_ref['spd']
            prec_r = stats_ref['precision_macro_buy_sell']
            f1_r = stats_ref['f1_macro_buy_sell']

            # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°; Ğ¾ĞºĞ½Ğ¾ SPD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ (spd_min..spd_max) Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ precision_min
            if (spd_min <= spd_r <= spd_max) and (prec_r >= precision_min):
                # ĞºĞ»ÑÑ‡: Ğ¼Ğ°ĞºÑ F1, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ï„, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ñ†ĞµĞ½Ñ‚Ñ€Ñƒ SPD
                key_ref = (f1_r, float(t), -abs(spd_r - target))
                if (best_ref is None) or (key_ref > best_ref[0]):
                    best_ref = (key_ref, float(t), stats_ref)

        # Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ, ĞµÑĞ»Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾
        if best_ref is not None:
            _, tau_new, sref = best_ref
            tau_chosen = tau_new
            spd = float(sref['spd'])
            prec = float(sref['precision_macro_buy_sell'])
            rec = float(sref['recall_macro_buy_sell'])
            f1 = float(sref['f1_macro_buy_sell'])
            signals = int(round(spd * max(1, len(y_val)) / max(1, bars_per_day)))

        # hit_range Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ“Ğ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ² Ğ¾ĞºĞ½Ğ¾ Ğ¸ Ğ¿Ğ¾ precision
        in_range = (spd_min <= spd <= spd_max) and (prec >= precision_min)

        return float(tau_chosen), {
            "spd": float(spd),
            "precision_macro_buy_sell": float(prec),
            "recall_macro_buy_sell": float(rec),
            "f1_macro_buy_sell": float(f1),
            "signals": int(signals),
            "delta": float(delta),
            "cooldown_bars": int(cooldown_bars),
            "range": [float(spd_min), float(spd_max)],
            "hit_range": bool(in_range),
        }

    @staticmethod
    def _eval_decision_metrics(y_true: np.ndarray,
                               proba: np.ndarray,
                               tau: float,
                               delta: float,
                               cooldown_bars: int,
                               bars_per_day: int) -> dict:
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        act = (maxp >= tau) & (margin >= delta)

        # cooldown Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ğ¼ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        # Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ: Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… samples
        active_count = np.sum(act)
        if active_count > 0:
            logging.debug(f"Active samples: {active_count}, tau={tau:.3f}")

        pred = np.zeros(len(proba), dtype=int)
        buy_ge_sell = p_buy >= p_sell
        pred[act & buy_ge_sell] = 1
        pred[act & (~buy_ge_sell)] = 2

        # SPD
        spd_val = act.sum() * bars_per_day / max(1, len(y_true))

        # Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ (Ğ¢ĞĞ›Ğ¬ĞšĞ ĞºĞ»Ğ°ÑÑÑ‹ 1 Ğ¸ 2)
        if np.any(act):
            y_true_active = y_true[act]
            pred_active = pred[act]

            # Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ Ğ£Ğ•Ğœ: Ğ±ĞµÑ€ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»Ğ°ÑÑÑ‹ 1 Ğ¸ 2 (BUY/SELL)
            mask_buy_sell = (y_true_active == 1) | (y_true_active == 2)
            y_true_bs = y_true_active[mask_buy_sell]
            pred_bs = pred_active[mask_buy_sell]

            if len(y_true_bs) > 0:
                pm, rm, fm, _ = precision_recall_fscore_support(
                    y_true_bs, pred_bs, labels=[1, 2], average='macro', zero_division=0
                )

                # Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ: Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
                correct_bs = np.sum(y_true_bs == pred_bs)
                accuracy_bs = correct_bs / len(y_true_bs) if len(y_true_bs) > 0 else 0
                logging.debug(f"BUY/SELL metrics: {len(y_true_bs)} samples, accuracy={accuracy_bs:.3f}")
            else:
                pm = rm = fm = 0.0
                logging.debug("No BUY/SELL samples in active set")
        else:
            pm = rm = fm = 0.0

        return {
            'spd': float(spd_val),
            'precision_macro_buy_sell': float(pm),
            'recall_macro_buy_sell': float(rm),
            'f1_macro_buy_sell': float(fm),
            'tau': float(tau),
            'delta': float(delta),
            'cooldown_bars': int(cooldown_bars),
            '_debug_active_count': int(active_count),  # Ğ”Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
        }

    @staticmethod
    def decide(proba, tau, delta=0.08, cooldown_bars=2):
        """Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ)"""
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)
        act = (maxp >= tau) & (margin >= delta)

        # Apply cooldown
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        pred = np.zeros(len(proba), dtype=int)
        pred[act] = np.where(p_buy[act] >= p_sell[act], 1, 2)
        return pred

    def train_model(self, run_id: str, use_scaler: bool = False) -> dict:
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ + Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°"""

        logger.info("\n" + "=" * 60)
        logger.info("ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ LIGHTGBM (WINDOWED)")
        logger.info("=" * 60)

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        X, y, w = self.prepare_training_data(run_id)

        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° train/val/test Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (70/15/15)
        n = len(X)
        train_end = int(n * 0.70)
        val_end = int(n * 0.85)

        X_train = X.iloc[:train_end]
        X_val = X.iloc[train_end:val_end]
        X_test = X.iloc[val_end:]

        y_train = y.iloc[:train_end]
        y_val = y.iloc[train_end:val_end]
        y_test = y.iloc[val_end:]

        w_train = w.iloc[:train_end]
        w_val = w.iloc[train_end:val_end]
        w_test = w.iloc[val_end:]

        NUM_CLASS = 3
        REPORT_LABELS = [1, 2, 0]  # BUY, SELL, HOLD
        REPORT_NAMES = ['BUY', 'SELL', 'HOLD']

        logger.info(f"ğŸ“Š Train: {len(X_train)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Val: {len(X_val)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Test: {len(X_test)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²")
        logger.info("âš–ï¸  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ²ĞµÑĞ° Ğ¸Ğ· training_dataset")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SCALER (ĞĞŸĞ¦Ğ˜ĞĞĞĞ›Ğ¬ĞĞ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        scaler = None
        X_train_processed = X_train
        X_val_processed = X_val
        X_test_processed = X_test

        if use_scaler:
            logger.info("ğŸ“Š Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ StandardScaler Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
            scaler = StandardScaler()
            X_train_processed = scaler.fit_transform(X_train)
            X_val_processed = scaler.transform(X_val)
            X_test_processed = scaler.transform(X_test)
            logger.info(f"âœ… Scaler Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ½Ğ° {len(X_train)} Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ñ…")
        else:
            logger.info("âš ï¸  Scaler Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½ - Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° RAW Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…")

        # Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ LightGBM
        train_data = lgb.Dataset(X_train_processed, label=y_train, weight=w_train)
        val_data = lgb.Dataset(X_val_processed, label=y_val, reference=train_data)

        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        params = {
            'objective': 'multiclass',
            'num_class': NUM_CLASS,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.01,
            'feature_fraction': 0.6,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'min_child_samples': 20,
            'max_depth': 8,
            'lambda_l1': 1.5,
            'lambda_l2': 1.5,
            'min_gain_to_split': 0.15,
            'boost_from_average': False,
            'seed': 42,
            'bagging_seed': 42,
            'feature_fraction_seed': 42,
        }

        logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...")

        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            valid_names=['valid_0'],
            num_boost_round=1600,
            callbacks=[
                thermometer_progress_callback(logger, width=30, period=10),
                lgb.early_stopping(stopping_rounds=20, first_metric_only=True),
            ],
        )

        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° val
        y_val_pred_proba = model.predict(X_val_processed if (use_scaler and scaler is not None) else X_val)
        y_val_pred = y_val_pred_proba.argmax(axis=1)

        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° test (Ğ´Ğ»Ñ tuning)
        y_test_pred_proba = model.predict(X_test_processed if (use_scaler and scaler is not None) else X_test)
        y_test_pred = y_test_pred_proba.argmax(axis=1)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ” Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ Ğ£Ğ¢Ğ•Ğ§ĞšĞ˜ Ğ”ĞĞĞĞ«Ğ¥
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("\nğŸ” Ğ”Ğ˜ĞĞ“ĞĞĞ¡Ğ¢Ğ˜ĞšĞ Ğ£Ğ¢Ğ•Ğ§ĞšĞ˜ Ğ”ĞĞĞĞ«Ğ¥:")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° train (Ğ±ĞµĞ· Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²)
        y_train_pred_proba_diag = model.predict(X_train_processed if (use_scaler and scaler is not None) else X_train)
        y_train_pred_diag = y_train_pred_proba_diag.argmax(axis=1)
        train_acc = accuracy_score(y_train, y_train_pred_diag)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° val/test (Ğ±ĞµĞ· Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²)
        val_acc = accuracy_score(y_val, y_val_pred)
        test_acc = accuracy_score(y_test, y_test_pred)

        logger.info(f"   Train accuracy (Ğ±ĞµĞ· Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²): {train_acc:.4f}")
        logger.info(f"   Val accuracy (Ğ±ĞµĞ· Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²): {val_acc:.4f}")
        logger.info(f"   Test accuracy (Ğ±ĞµĞ· Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²): {test_acc:.4f}")
        logger.info(f"   Gap (train-val): {train_acc - val_acc:.4f}")
        logger.info(f"   Gap (train-test): {train_acc - test_acc:.4f}")

        if train_acc > 0.95:
            logger.error("ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ£Ğ¢Ğ•Ğ§ĞšĞ: train accuracy >95%!")
            logger.error("   ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸/Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ½Ğ° forward-looking Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ!")

        if abs(train_acc - val_acc) > 0.20:
            logger.warning(f"âš ï¸  Ğ¡Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: gap={train_acc - val_acc:.2%}")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ” Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ² (ĞĞ TEST!)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # bars_per_day Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¸Ğ· run_id (ĞµÑĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾)
        bars_per_day = _infer_bars_per_day_from_run_id(run_id, default=TIMEFRAME_TO_BARS.get(str(self.timeframe).lower(), 288))
        candidates = []
        # ĞŸĞµÑ€ĞµĞ±Ğ¾Ñ€ precision_min ĞĞ Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’ĞĞœ ĞĞĞ‘ĞĞ Ğ•
        precision_grid = [0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]

        for idx, pm in enumerate(precision_grid):
            try:
                tau_i, tstats_i = self.tune_tau_for_spd_range(
                    y_val=np.asarray(y_test),
                    proba=np.asarray(y_test_pred_proba),
                    bars_per_day=bars_per_day,
                    spd_min=20.0,
                    spd_max=35.0,
                    precision_min=pm,
                    delta=0.06,
                    cooldown_bars=2,
                    log_stats=(idx == 0),  # Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ max-proba stats Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
                )
                candidates.append({
                    'precision_min': pm,
                    'tau': float(tau_i),
                    'spd': float(tstats_i.get('spd', float('nan'))),
                    'precision_macro_buy_sell': float(tstats_i.get('precision_macro_buy_sell', float('nan'))),
                    'f1_macro_buy_sell': float(tstats_i.get('f1_macro_buy_sell', float('nan'))),
                    'hit_range': bool(tstats_i.get('hit_range', False)),
                    'delta': float(tstats_i.get('delta', 0.08)),
                    'cooldown_bars': int(tstats_i.get('cooldown_bars', 2)),
                    '_tstats': tstats_i,
                })
            except Exception as e:
                logging.warning(f"precision_min={pm:.2f}: sweep failed with error: {e}")

        def _key(c):
            return (1 if c.get('hit_range') else 0,
                    c.get('precision_macro_buy_sell', float('-inf')),
                    c.get('f1_macro_buy_sell', float('-inf')),
                    -c.get('tau', float('inf')))

        if not candidates:
            raise RuntimeError("Precision sweep failed: no candidates collected")

        best = max(candidates, key=_key)
        tau = best['tau']
        tstats = best['_tstats']
        delta = best.get('delta', 0.08)
        cooldown_bars = best.get('cooldown_bars', 2)

        logging.info("ğŸ”§ Precision sweep results (Ğ½Ğ° TEST Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ):")
        for c in candidates:
            logging.info(f"  pm={c['precision_min']:.2f}, tau={c['tau']:.3f}, spdâ‰ˆ{c['spd']:.1f}, "
                         f"precâ‰ˆ{c['precision_macro_buy_sell']:.3f}, f1â‰ˆ{c['f1_macro_buy_sell']:.3f}, "
                         f"hit={c['hit_range']}")
        logging.info(f"âœ… Picked precision_min={best['precision_min']:.2f} â†’ "
                     f"tau={tau:.3f}, spdâ‰ˆ{best['spd']:.1f}")

        logger.info(
            "ğŸ”§ Tuned thresholds: tau=%.3f, delta=%.2f, cooldown=%d â†’ spdâ‰ˆ%.1f/day, "
            "precisionâ‰ˆ%.3f, recallâ‰ˆ%.3f, f1â‰ˆ%.3f (hit_range=%s)"
            % (tau, tstats['delta'], tstats['cooldown_bars'],
               tstats['spd'], tstats['precision_macro_buy_sell'],
               tstats['recall_macro_buy_sell'], tstats['f1_macro_buy_sell'],
               tstats['hit_range'])
        )

        # Sensitivity Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (ĞĞ TEST)
        _tau_offsets = [-0.05, -0.03, -0.02, 0.0, 0.02, 0.03, 0.05]
        _delta_offsets = [-0.02, 0.0, 0.02]

        tau_sensitivity = []
        for off in _tau_offsets:
            tau_x = float(np.clip(tau + off, 0.0, 1.0))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_test),
                proba=np.asarray(y_test_pred_proba),
                tau=tau_x,
                delta=delta,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            tau_sensitivity.append(r)

        delta_sensitivity = []
        for off in _delta_offsets:
            delta_x = float(max(0.0, delta + off))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_test),
                proba=np.asarray(y_test_pred_proba),
                tau=tau,
                delta=delta_x,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            delta_sensitivity.append(r)

        _tau_sorted = sorted(tau_sensitivity, key=lambda r: abs(r['tau'] - float(tau)))[:3]
        _tau_sorted = sorted(_tau_sorted, key=lambda r: r['tau'])

        logging.info("ğŸ” Sensitivity (tau near current):")
        for r in _tau_sorted:
            logging.info(f"  tau={r['tau']:.3f} â†’ spdâ‰ˆ{r['spd']:.1f}, f1â‰ˆ{r['f1_macro_buy_sell']:.3f}")

        logging.info("ğŸ” Sensitivity (deltaÂ±0.02):")
        for r in delta_sensitivity:
            logging.info(f"  delta={r['delta']:.2f} â†’ spdâ‰ˆ{r['spd']:.1f}, f1â‰ˆ{r['f1_macro_buy_sell']:.3f}")

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²
        train_dist = Counter(y_train)
        val_dist = Counter(y_val)
        test_dist = Counter(y_test)
        pred_val_dist = Counter(y_val_pred)
        pred_test_dist = Counter(y_test_pred)

        logger.info(f"\nğŸ“Š Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ²:")
        logger.info(f"  Train:     {dict(train_dist)}")
        logger.info(f"  Val:       {dict(val_dist)}")
        logger.info(f"  Test:      {dict(test_dist)}")
        logger.info(f"  Pred Val:  {dict(pred_val_dist)}")
        logger.info(f"  Pred Test: {dict(pred_test_dist)}")

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ
        prec_val, rec_val, f1_val, _ = precision_recall_fscore_support(
            y_val, y_val_pred,
            labels=REPORT_LABELS,
            average=None,
            zero_division=0
        )
        cm_val = confusion_matrix(y_val, y_val_pred, labels=REPORT_LABELS)

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ (Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸)
        prec_test, rec_test, f1_test, _ = precision_recall_fscore_support(
            y_test, y_test_pred,
            labels=REPORT_LABELS,
            average=None,
            zero_division=0
        )
        cm_test = confusion_matrix(y_test, y_test_pred, labels=REPORT_LABELS)

        decision_policy = {
            'tau': tau,
            'delta': tstats['delta'],
            'cooldown_bars': tstats['cooldown_bars'],
            'bars_per_day': bars_per_day,
            'test_spd': tstats['spd'],
            'test_precision_macro_buy_sell': tstats['precision_macro_buy_sell'],
            'test_recall_macro_buy_sell': tstats['recall_macro_buy_sell'],
            'test_f1_macro_buy_sell': tstats['f1_macro_buy_sell'],
            'target_spd_range': tstats['range'],
            'hit_range': tstats['hit_range'],
            'precision_min': best.get('precision_min', 0.60),
        }

        precision_min_sweep = [
            {
                'precision_min': c['precision_min'],
                'tau': c['tau'],
                'spd': c['spd'],
                'precision_macro_buy_sell': c['precision_macro_buy_sell'],
                'f1_macro_buy_sell': c['f1_macro_buy_sell'],
                'hit_range': c['hit_range'],
            }
            for c in candidates
        ]

        metrics = {
            'decision_policy': decision_policy,
            'precision_min_sweep': precision_min_sweep,

            # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            'val_accuracy': float(val_acc),
            'val_precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec_val)},
            'val_recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec_val)},
            'val_f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1_val)},
            'val_confusion_matrix': cm_val.tolist(),

            # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            'test_accuracy': float(test_acc),
            'test_precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec_test)},
            'test_recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec_test)},
            'test_f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1_test)},
            'test_confusion_matrix': cm_test.tolist(),

            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'class_distribution': {
                'train': {int(k): int(v) for k, v in train_dist.items()},
                'val': {int(k): int(v) for k, v in val_dist.items()},
                'test': {int(k): int(v) for k, v in test_dist.items()},
            },
            'tau_sensitivity': tau_sensitivity,
            'delta_sensitivity': delta_sensitivity,
            'lookback_window': self.lookback,
            'base_features_count': len(self.base_feature_names),
            'total_features_count': len(self.feature_names),
        }

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        os.makedirs("models", exist_ok=True)
        model_filename = f"models/ml_windowed_{self.symbol.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"

        model_metadata = {
            'version': '2.1.1',
            'format': 'windowed_lgb',
            'instrument': self.symbol,
            'exchange': 'Binance',
            'timeframe': '5m',
            'lookback_window': self.lookback,
            'base_feature_count': len(self.base_feature_names),
            'total_feature_count': len(self.feature_names),
            'trained_at': datetime.now().isoformat(),
            'training_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test),
            'val_accuracy': float(val_acc),
            'test_accuracy': float(test_acc),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'run_id': run_id,
            'decision_policy': decision_policy,
            'scaler_used': use_scaler,
        }

        model_package = {
            'model': model,
            'scaler': scaler,
            'metadata': model_metadata,
            'base_feature_names': self.base_feature_names,
            'lookback': self.lookback,
            'timeframe': '5m',
            'min_confidence': 0.65,
            'required_warmup': 20
        }

        joblib.dump(model_package, model_filename)
        logger.info(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {model_filename}")
        logger.info(f"   - Lookback: {self.lookback} Ğ±Ğ°Ñ€Ğ¾Ğ²")
        logger.info(f"   - ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {len(self.feature_names)}")
        logger.info(f"   - Scaler: {'StandardScaler' if scaler else 'None'}")

        # Tau curves (ĞĞ TEST)
        try:
            tau_left = max(0.0, float(tau) - 0.05)
            tau_right = min(0.999, float(tau) + 0.05)
            tau_grid = np.arange(tau_left, tau_right + 1e-9, 0.002)

            spd_curve = []
            f1_curve = []
            for tcur in tau_grid:
                s = self._eval_decision_metrics(
                    y_true=np.asarray(y_test),
                    proba=np.asarray(y_test_pred_proba),
                    tau=float(tcur),
                    delta=float(delta),
                    cooldown_bars=int(cooldown_bars),
                    bars_per_day=int(bars_per_day),
                )
                spd_curve.append(s['spd'])
                f1_curve.append(s['f1_macro_buy_sell'])

            os.makedirs("models/training_logs", exist_ok=True)
            curve_prefix = str(Path("models/training_logs") / Path(model_filename).with_suffix('').name)

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, spd_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('SPD vs tau (Ğ½Ğ° TEST Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ)')
            plt.xlabel('tau')
            plt.ylabel('signals per day')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_spd.png")
            plt.close()

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, f1_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--', color='red', label=f'tau={tau:.3f}')
            plt.title('F1 (macro BUY/SELL on act) vs tau (Ğ½Ğ° TEST Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ)')
            plt.xlabel('tau')
            plt.ylabel('F1 macro (BUY/SELL)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_f1.png")
            plt.close()

        except Exception as _e:
            logging.warning(f"tau curves plotting skipped: {_e}")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
        self.save_training_report(metrics, model_filename)

        # Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ TEST Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸)
        diag_prefix = Path("models/training_logs") / Path(model_filename).with_suffix('').name
        self.post_training_diagnostics(
            model=model,
            X_val=X_test,
            y_val=y_test,
            y_val_pred_proba=y_test_pred_proba,
            prefix_path=str(diag_prefix),
            bars_per_day=bars_per_day
        )

        return metrics

    def plot_precision_spd_curve(self, y_val, y_val_pred_proba, bars_per_day: int,
                                 delta: float = 0.08, cooldown_bars: int = 2,
                                 prefix_path: str = "models/training_logs/diag") -> None:
        """
        Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ SPD(Ï„) Ğ¸ Precision/Recall/F1 Ğ¾Ñ‚ SPD
        """
        proba = np.asarray(y_val_pred_proba)
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        def _apply_cooldown(mask: np.ndarray, cd: int) -> np.ndarray:
            if cd <= 0:
                return mask
            idx = np.where(mask)[0]
            if idx.size == 0:
                return mask
            keep = [idx[0]]
            last = idx[0]
            for i in idx[1:]:
                if i - last >= cd:
                    keep.append(i)
                    last = i
            out = np.zeros_like(mask, dtype=bool)
            out[np.array(keep, dtype=int)] = True
            return out

        taus = np.linspace(0.45, 0.70, 26)
        rows = []
        n = len(y_val)

        for tau in taus:
            act = (maxp >= tau) & (margin >= delta)
            act = _apply_cooldown(act, cooldown_bars)

            signals = int(act.sum())
            spd = signals * bars_per_day / max(1, n)

            if signals == 0:
                prec = rec = f1 = 0.0
            else:
                pred_dir = np.where(p_buy[act] >= p_sell[act], 1, 2)
                true_dir = y_val[act]
                prec = precision_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                rec = recall_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                f1 = f1_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)

            rows.append((tau, spd, prec, rec, f1, signals))

        df = pd.DataFrame(rows, columns=['tau', 'spd_per_day', 'precision', 'recall', 'f1', 'signals'])
        csv_path = f"{prefix_path}_tau_sweep.csv"
        df.to_csv(csv_path, index=False)

        # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº SPD(Ï„)
        plt.figure(figsize=(8, 5))
        plt.plot(df['tau'], df['spd_per_day'], marker='o')
        plt.xlabel('tau')
        plt.ylabel('SPD (signals/day)')
        plt.title('Signals per day vs tau')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_spd_vs_tau.png")
        plt.close()

        # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Precision/Recall/F1 vs SPD
        plt.figure(figsize=(8, 5))
        plt.plot(df['spd_per_day'], df['precision'], marker='o', label='Precision (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['recall'], marker='o', label='Recall (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['f1'], marker='o', label='F1 (macro BUY/SELL)')
        plt.xlabel('SPD (signals/day)')
        plt.ylabel('score')
        plt.title('Precision / Recall / F1 vs SPD')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_prf_vs_spd.png")
        plt.close()

    def post_training_diagnostics(self, model, X_val, y_val, y_val_pred_proba,
                                  prefix_path: str, bars_per_day: int = 288):
        """
        Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:
        - Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        - Ğ³Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹
        - PR-ĞºÑ€Ğ¸Ğ²Ñ‹Ğµ
        - SPD curves
        """
        os.makedirs(os.path.dirname(prefix_path), exist_ok=True)

        try:
            bars_per_day = int(bars_per_day) if bars_per_day is not None else 288
        except Exception:
            bars_per_day = 288

        proba = np.asarray(y_val_pred_proba)
        if proba.ndim == 1:
            tmp = np.zeros((len(proba), 3), dtype=float)
            tmp[np.arange(len(proba)), np.clip(proba.astype(int), 0, 2)] = 1.0
            proba = tmp

        p_hold = proba[:, 0]
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]

        feat_names = self.feature_names if hasattr(self, 'feature_names') else [f"f{i}" for i in range(X_val.shape[1])]

        # === 1) Feature Importance ===
        try:
            gain = model.feature_importance(importance_type='gain')
            df_imp = (pd.DataFrame({'feature': feat_names, 'gain': gain})
                      .sort_values('gain', ascending=False)
                      .head(30))
            plt.figure(figsize=(10, max(8, 0.3 * len(df_imp))))
            sns.barplot(data=df_imp, x='gain', y='feature')
            plt.title('Feature Importance (gain) â€” top 30')
            plt.tight_layout()
            plt.savefig(f"{prefix_path}_feat_importance.png")
            plt.close()

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ CSV Ñ Ğ’Ğ¡Ğ•Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ
            pd.DataFrame({'feature': feat_names, 'gain': gain}).sort_values('gain', ascending=False).to_csv(
                f"{prefix_path}_feat_importance.csv", index=False
            )

            # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾  Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ (Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ»Ğ°Ğ³Ğ°Ğ¼)
            base_feat_importance = {}
            for feature, importance in zip(feat_names, gain):
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° (ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ _t0, _t-1 Ğ¸ Ñ‚.Ğ´.)
                if '_t-' in feature:
                    base_feat = feature.split('_t-')[0]  # cmo_14_t-1 -> cmo_14
                elif '_t0' in feature:
                    base_feat = feature.replace('_t0', '')  # cmo_14_t0 -> cmo_14
                else:
                    base_feat = feature  # fallback

                base_feat_importance[base_feat] = base_feat_importance.get(base_feat, 0) + importance

            df_base_imp = pd.DataFrame({
                'base_feature': list(base_feat_importance.keys()),
                'total_gain': list(base_feat_importance.values())
            }).sort_values('total_gain', ascending=False)

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ CSV Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ
            df_base_imp.to_csv(f"{prefix_path}_feat_importance_base_aggregated.csv", index=False)

            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
            logger.info(f"ğŸ¯ Ğ’ĞĞ–ĞĞĞ¡Ğ¢Ğ¬ {len(self.base_feature_names)} Ğ‘ĞĞ—ĞĞ’Ğ«Ğ¥ ĞŸĞ Ğ˜Ğ—ĞĞĞšĞĞ’ (Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ»Ğ°Ğ³Ğ°Ğ¼):")
            for i, row in df_base_imp.iterrows():
                logger.info(f"   {i + 1:2d}. {row['base_feature']}: {row['total_gain']:.0f}")

        except Exception as e:
            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {e}")

        # === 2) Ğ“Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ ===
        y_pred = proba.argmax(axis=1)

        def hist_one(prob, true_class, name, fname):
            mask_pos = (y_val == true_class)
            mask_pred_pos = (y_pred == true_class)

            tp = prob[mask_pos & mask_pred_pos]
            fp = prob[(~mask_pos) & mask_pred_pos]
            fn = prob[mask_pos & (~mask_pred_pos)]
            tn = prob[(~mask_pos) & (~mask_pred_pos)]

            plt.figure(figsize=(8, 5))
            bins = 30
            if len(tp) > 0:
                sns.histplot(tp, bins=bins, stat='density', label='TP', alpha=0.6)
            if len(fp) > 0:
                sns.histplot(fp, bins=bins, stat='density', label='FP', alpha=0.6)
            if len(fn) > 0:
                sns.histplot(fn, bins=bins, stat='density', label='FN', alpha=0.6)
            if len(tn) > 0:
                sns.histplot(tn, bins=bins, stat='density', label='TN', alpha=0.6)
            plt.legend()
            plt.xlabel(f"p({name})")
            plt.ylabel("density")
            plt.title(f"Distributions for {name}")
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()

        hist_one(p_buy, 1, "BUY", f"{prefix_path}_proba_hist_BUY.png")
        hist_one(p_sell, 2, "SELL", f"{prefix_path}_proba_hist_SELL.png")

        # === 3) Max-proba scatter ===
        maxp = proba.max(axis=1)
        plt.figure(figsize=(8, 5))
        sns.scatterplot(x=np.arange(len(maxp)), y=maxp,
                        hue=[{0: 'HOLD', 1: 'BUY', 2: 'SELL'}.get(c, 'UNK') for c in y_val],
                        s=12, linewidth=0)
        plt.title("Max class probability vs true class (val order)")
        plt.xlabel("index in validation set (chronological)")
        plt.ylabel("max proba")
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_maxproba_scatter.png")
        plt.close()

        # === 4) PR curves ===
        Y_bin = label_binarize(y_val, classes=[0, 1, 2])
        curves = [
            ("BUY", Y_bin[:, 1], p_buy),
            ("SELL", Y_bin[:, 2], p_sell),
            ("HOLD", Y_bin[:, 0], p_hold),
        ]
        plt.figure(figsize=(8, 6))
        for name, y_true_bin, y_score in curves:
            precision, recall, _ = precision_recall_curve(y_true_bin, y_score)
            ap = average_precision_score(y_true_bin, y_score)
            plt.plot(recall, precision, label=f"{name} (AP={ap:.3f})")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precisionâ€“Recall curves (one-vs-rest)")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_pr_curves.png")
        plt.close()

        # === 5) SPD curve ===
        self.plot_precision_spd_curve(
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            bars_per_day=bars_per_day,
            delta=0.08,
            cooldown_bars=2,
            prefix_path=prefix_path
        )

    def save_training_report(self, metrics: dict, model_path: str):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸"""
        os.makedirs("models/training_logs", exist_ok=True)

        report = {
            'training_date': datetime.now().isoformat(),
            'symbol': self.symbol,
            'db_dsn': self.db_dsn,
            'model_path': model_path,
            'lookback_window': self.lookback,
            'metrics': metrics,
            'base_feature_names': self.base_feature_names,
            'total_feature_names_count': len(self.feature_names),
        }

        report_filename = model_path.replace('.joblib', '_report.json')
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)

        # Confusion matrices (val Ğ¸ test)
        try:
            labels = ['BUY', 'SELL', 'HOLD']

            cm_val = np.array(metrics.get('val_confusion_matrix', []))
            if cm_val.size > 0:
                plt.figure(figsize=(8, 6))
                sns.heatmap(cm_val, annot=True, fmt='d',
                            xticklabels=labels, yticklabels=labels)
                plt.title('Validation Confusion Matrix')
                plt.tight_layout()
                plt.savefig(report_filename.replace('.json', '_cm_val.png'))
                plt.close()

            cm_test = np.array(metrics.get('test_confusion_matrix', []))
            if cm_test.size > 0:
                plt.figure(figsize=(8, 6))
                sns.heatmap(cm_test, annot=True, fmt='d',
                            xticklabels=labels, yticklabels=labels)
                plt.title('Test Confusion Matrix')
                plt.tight_layout()
                plt.savefig(report_filename.replace('.json', '_cm_test.png'))
                plt.close()
        except Exception as e:
            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ CM: {e}")

        logger.info(f"âœ… ĞÑ‚Ñ‡ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {report_filename}")

    def close(self):
        """ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ"""
        self.data_loader.close()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    print("ğŸš€ Ğ—ĞĞŸĞ£Ğ¡Ğš ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯ ĞœĞĞ”Ğ•Ğ›Ğ˜ (v2.1 - WINDOWED)")
    print("=" * 50)

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ‘Ğ”
    db_file = DATA_DIR / "market_data.sqlite"
    if not db_file.exists():
        print(f"âŒ Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… {db_file} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°!")
        print("   Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ:")
        print("   1. ml_data_preparation.py")
        print("   2. ml_labeling_tool_v3.py")
        return 1

    trainer = None
    try:
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
        db_dsn = MARKET_DB_DSN
        symbol = "ETHUSDT"
        lookback = LOOKBACK_WINDOW

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ run_id Ğ”Ğ›Ğ¯ ĞšĞĞĞšĞ Ğ•Ğ¢ĞĞĞ“Ğ symbol/timeframe
        engine = create_engine(MARKET_DB_DSN)
        with engine.connect() as conn:
            row = conn.execute(
                text("""
                    SELECT run_id 
                      FROM training_dataset_meta 
                     WHERE status='READY' AND symbol=:symbol AND timeframe='5m'
                     ORDER BY created_at DESC LIMIT 1
                """),
                {"symbol": symbol}
            ).fetchone()
        engine.dispose()

        if not row:
            print("âŒ ĞĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… snapshot Ğ² training_dataset_meta Ğ´Ğ»Ñ ETHUSDT/5m!")
            print("   Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ snapshot Ñ‡ĞµÑ€ĞµĞ· [14] Ğ² ml_labeling_tool_v3.py")
            return 1

        run_id = row[0]

        print(f"ğŸ“Š Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {DATA_DIR / 'market_data.sqlite'}")
        print(f"ğŸ¯ Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»: {symbol}")
        print(f"ğŸ“¦ Run ID: {run_id}")
        print(f"ğŸªŸ Lookback Window: {lookback} Ğ±Ğ°Ñ€Ğ¾Ğ²")
        print("=" * 50)

        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        trainer = ModelTrainer(db_dsn, symbol, lookback=lookback)

        use_scaler = False  # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ True
        metrics = trainer.train_model(run_id, use_scaler=use_scaler)

        # Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        print("\nğŸ¯ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯:")
        print(f"   Val Accuracy:  {metrics['val_accuracy']:.4f}")
        print(f"   Test Accuracy: {metrics['test_accuracy']:.4f}")

        print(f"\n   VAL Precision BUY/SELL/HOLD: "
              f"{metrics['val_precision']['BUY']:.4f}/"
              f"{metrics['val_precision']['SELL']:.4f}/"
              f"{metrics['val_precision']['HOLD']:.4f}")
        print(f"   TEST Precision BUY/SELL/HOLD: "
              f"{metrics['test_precision']['BUY']:.4f}/"
              f"{metrics['test_precision']['SELL']:.4f}/"
              f"{metrics['test_precision']['HOLD']:.4f}")

        print(f"\n   VAL Recall BUY/SELL/HOLD: "
              f"{metrics['val_recall']['BUY']:.4f}/"
              f"{metrics['val_recall']['SELL']:.4f}/"
              f"{metrics['val_recall']['HOLD']:.4f}")
        print(f"   TEST Recall BUY/SELL/HOLD: "
              f"{metrics['test_recall']['BUY']:.4f}/"
              f"{metrics['test_recall']['SELL']:.4f}/"
              f"{metrics['test_recall']['HOLD']:.4f}")
        return 0

    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        return 1
    finally:
        if trainer:
            trainer.close()


if __name__ == '__main__':
    sys.exit(main())