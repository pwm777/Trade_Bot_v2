"""
ml_labeling_tool_v3.py
 –≤–µ—Ä—Å–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏
"""
import numpy as np
from sqlalchemy import text
import pandas as pd
import sys
import hashlib, json
from dataclasses import dataclass
from config import BASE_FEATURE_NAMES
from iqts_standards import Direction
from typing import Tuple, List, Dict,  Any, Optional
from datetime import datetime, UTC
import warnings
import logging
import traceback

# msvcrt is Windows-only, make import conditional
try:
    import msvcrt
except ImportError:
    msvcrt = None  # Not available on Linux/macOS

# === –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç ruptures ===
try:
    import ruptures as rpt
    RUPTURES_AVAILABLE = True
except ImportError:
    rpt = None
    RUPTURES_AVAILABLE = False
    logging.warning("‚ö†Ô∏è ruptures –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî —Ñ—É–Ω–∫—Ü–∏–∏ BinSeg –∏ PELT –æ—Ç–∫–ª—é—á–µ–Ω—ã")

# === –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç scipy ===
try:
    from scipy.stats import linregress as scipy_linregress
    SCIPY_AVAILABLE = True
except ImportError:
    scipy_linregress = None
    SCIPY_AVAILABLE = False
    logging.warning("‚ö†Ô∏è scipy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç numpy fallback")

FEATURE_SQL_TYPE_OVERRIDES: dict[str, str] = {
    "is_trend_pattern_1m": "INTEGER",
    "cusum_price_conflict": "INTEGER",
    "cusum_state_conflict": "INTEGER",
    # –µ—Å–ª–∏ —Ä–µ—à–∏—à—å –¥–æ–±–∞–≤–∏—Ç—å –≤ BASE_FEATURE_NAMES:
    "cusum_1m_recent": "INTEGER",
    "cusum_1m_trend_aligned": "INTEGER",
}

# --- DDL –¥–ª—è —Ç–∞–±–ª–∏—Ü —Å–Ω–∞–ø—à–æ—Ç–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ---
def _build_training_dataset_sql() -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç DDL –¥–ª—è training_dataset –Ω–∞ –æ—Å–Ω–æ–≤–µ BASE_FEATURE_NAMES.
    –í—Å–µ —Ñ–∏—á–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é REAL, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤ FEATURE_SQL_TYPE_OVERRIDES.
    """
    feature_lines: list[str] = []
    for name in BASE_FEATURE_NAMES:
        sql_type = FEATURE_SQL_TYPE_OVERRIDES.get(name, "REAL")
        feature_lines.append(f"    {name: <28} {sql_type},")

    feature_block = "\n".join(feature_lines)

    return f"""
CREATE TABLE IF NOT EXISTS training_dataset (
    run_id           TEXT    NOT NULL,
    symbol           TEXT    NOT NULL,
    timeframe        TEXT    NOT NULL,
    ts               INTEGER NOT NULL,
    datetime         TEXT    NOT NULL,
    reversal_label   INTEGER NOT NULL,
    sample_weight    REAL    NOT NULL,
{feature_block}
    created_at       TEXT    NOT NULL,
    PRIMARY KEY (run_id, symbol, ts)
);
"""

CREATE_TRAINING_DATASET_SQL = _build_training_dataset_sql()
CREATE_TRAINING_DATASET_INDEXES_SQL = [
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_id      ON training_dataset(run_id)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_symbol      ON training_dataset(symbol)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_sym     ON training_dataset(run_id, symbol)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_sym_ts      ON training_dataset(symbol, ts)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_ts      ON training_dataset(run_id, ts)"
]

CREATE_TRAINING_DATASET_META_SQL = """
CREATE TABLE IF NOT EXISTS training_dataset_meta (
    run_id             TEXT PRIMARY KEY,
    status             TEXT NOT NULL,               -- CREATING|READY|FAILED
    error_msg          TEXT,
    symbol             TEXT NOT NULL,
    timeframe          TEXT NOT NULL,
    range_start_ts     INTEGER,
    range_end_ts       INTEGER,
    rows_total         INTEGER,
    pos_count          INTEGER,
    neg_count          INTEGER,
    class_dist_json    TEXT,
    hold_bars          INTEGER,
    buffer_bars        INTEGER,
    seed               INTEGER,
    labeling_method    TEXT,
    feature_names_json TEXT,
    featureset_version TEXT,
    features_hash      TEXT,
    config_json        TEXT,
    nan_drop_rows      INTEGER,
    issues_json        TEXT,
    source_hashes_json TEXT,
    created_at         TEXT NOT NULL
);
"""
CREATE_TRAINING_FEATURE_IMPORTANCE_SQL = """
CREATE TABLE IF NOT EXISTS training_feature_importance (
    run_id      TEXT    NOT NULL,
    model_name  TEXT    NOT NULL,
    feature     TEXT    NOT NULL,
    importance  REAL    NOT NULL,
    rank        INTEGER NOT NULL,
    created_at  TEXT    NOT NULL,
    PRIMARY KEY (run_id, model_name, feature)
);
"""
CREATE_TRAINING_FEATURE_IMPORTANCE_INDEXES_SQL = [
    "CREATE INDEX IF NOT EXISTS idx_tfi_run_id       ON training_feature_importance(run_id)",
    "CREATE INDEX IF NOT EXISTS idx_tfi_model_run    ON training_feature_importance(model_name, run_id)"
]
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from sqlalchemy.engine import Engine, create_engine
from pathlib import Path

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"

@dataclass
class LabelingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å SQLAlchemy"""
    db_engine: Engine = None
    symbol: str = "ETHUSDT"
    timeframe: str = "5m"

    # PELT Online
    pelt_window: int = 1000
    pelt_pen: float = 1
    pelt_min_size: int = 10
    pelt_confirm_bar: int = 3

    # CUSUM
    cusum_z_threshold: float = 3 # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π |cusum_zscore|,
    cusum_conf_threshold: float = 1  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è cusum_conf
    hold_bars: int = 3               # –ë–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–æ–ª–¥
    buffer_bars: int = 5             # –ú–µ–Ω—å—à–µ –±—É—Ñ–µ—Ä–Ω—ã—Ö –±–∞—Ä–æ–≤

    # Extremum (min/max)
    extremum_confirm_bar: int = 2
    extremum_window: int = 8
    min_signal_distance: int = 3
    # –§–∏–ª—å—Ç—Ä—ã
    method: str = "CUSUM_EXTREMUM"
    # PnL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    fee_percent: float = 0.0004
    min_profit_target: float = 0.001
    tool: Any = None

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –ø–æ—Ä–æ–≥–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ATR/price) ===
    atr_low_threshold: float = 0.005      # 0.5% –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ = –Ω–∏–∑–∫–∞—è
    atr_high_threshold: float = 0.015     # 1.5% –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ = –≤—ã—Å–æ–∫–∞—è

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –ø–æ—Ä–æ–≥–∏ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞ (R¬≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏) ===
    trend_weak_threshold: float = 0.3    # —Å–ª–∞–±—ã–π —Ç—Ä–µ–Ω–¥
    trend_strong_threshold: float = 0.7  # —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –ø–æ—Ä–æ–≥–∏ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã ===
    price_range_threshold: float = 0.003  # 0.3% –¥–≤–∏–∂–µ–Ω–∏–µ = –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–π ===
    consolidation_hold_every_n_bars: int = 3

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å –¥–ª—è "—Å–ª–∞–±–æ–≥–æ" –ø—Ä–æ—Ñ–∏—Ç–∞ ===
    weak_profit_threshold: float = 0.002  # 0.2%

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –≤–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ HOLD ===
    consolidation_sample_weight: float = 1.5  # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –≤–µ—Å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–π

    # === HOLD —Ä–∞–∑–º–µ—Ç–∫–∞: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–∫–Ω–∞ –¥–ª—è MID ===
    hold_min_window_bars: int = 6
    hold_min_mid_end_gap: int = 3
    hold_mid_margin_left: int = 1
    hold_mid_margin_right: int = 1

    def __post_init__(self):
        if self.db_engine is None:
            self.db_engine = create_engine(MARKET_DB_DSN)


class DataLoader:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å SQLAlchemy"""

    def __init__(self, db_engine: Engine = None, symbol: str = "ETHUSDT",
                 timeframe: str = "5m", config: LabelingConfig = None):
        self.db_engine = db_engine or create_engine(MARKET_DB_DSN)
        self.symbol = symbol
        self.timeframe = timeframe
        self.config = config  # ‚Üê –°–æ—Ö—Ä–∞–Ω—è–µ–º config
        self.feature_names = BASE_FEATURE_NAMES


    def connect(self) -> Engine:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î —á–µ—Ä–µ–∑ SQLAlchemy"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            with self.db_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy: {self.db_engine.url}")
            return self.db_engine
        except Exception as err:
            raise ConnectionError(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {err}")

    def disconnect(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è - –¥–ª—è SQLAlchemy –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"""
        # SQLAlchemy –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏
        logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –±—É–¥–µ—Ç –∑–∞–∫—Ä—ã—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")

    def load_indicators(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ SQLAlchemy"""
        query = """
            SELECT * FROM candles_5m 
            WHERE symbol = ? 
            ORDER BY ts
        """
        try:
            df = pd.read_sql_query(query, self.db_engine, params=(self.symbol,))

            if df.empty:
                raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            if 'ts' in df.columns and 'datetime' not in df.columns:
                df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

            required_cols = ["cusum", "cusum_state", "cusum_zscore", "cusum_conf"]
            missing = [c for c in required_cols if c not in df.columns]
            if missing:
                logger.info("CUSUM 5m: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ %s (—Ç–∞–±–ª–∏—Ü–∞ candles_5m).", missing)
            else:
                # guard: –µ—Å–ª–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Å—Ç–∞—Ç—É—Å—ã
                df["cusum_state"] = df["cusum_state"].replace({"BUY": 1, "SELL": -1, "HOLD": 0})
                # –∏—Ç–æ–≥: Int64-–∫–∞—Ç–µ–≥–æ—Ä–∏—è 1/2/0, NaN -> 0
                df["cusum_state"] = pd.to_numeric(df["cusum_state"], errors="coerce").fillna(0).astype("Int64")

                # alias –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤/–º–æ–¥–µ–ª–µ–π (—Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏)
                df["cusum_signal"] = df["cusum_state"]

                # —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ –∫ float
                df["cusum"] = pd.to_numeric(df["cusum"], errors="coerce")
                df["cusum_zscore"] = pd.to_numeric(df["cusum_zscore"], errors="coerce")
                df["cusum_conf"] = pd.to_numeric(df["cusum_conf"], errors="coerce")

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
            return df

        except Exception as err:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {err}")

    def validate_data_quality(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, Any]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        if df.empty:
            return False, {"error": "DataFrame –ø—É—Å—Ç"}

        required_columns = ['open', 'high', 'low', 'close', 'volume', 'ts']
        checks = {
            'min_rows': len(df) > 100,
            'required_columns': all(col in df.columns for col in required_columns),
            'no_nan_close': df['close'].isna().sum() == 0,
            'high_low_valid': (df['high'] >= df['low']).all(),
            'high_open_valid': (df['high'] >= df['open']).all(),
            'high_close_valid': (df['high'] >= df['close']).all(),
            'low_open_valid': (df['low'] <= df['open']).all(),
            'low_close_valid': (df['low'] <= df['close']).all(),
            'positive_volume': (df['volume'] >= 0).all(),
            'timestamp_monotonic': df['ts'].is_monotonic_increasing
        }

        quality_ok = all(checks.values())
        diagnostics = {
            'passed_checks': sum(checks.values()),
            'total_checks': len(checks),
            'failed_checks': [k for k, v in checks.items() if not v],
            'basic_stats': {
                'period': f"{df['datetime'].min()} to {df['datetime'].max()}" if 'datetime' in df.columns else 'N/A',
                'total_rows': len(df),
                'nan_count': df.isna().sum().sum()
            }
        }
        # DataLoader.validate_data_quality(df) ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∏ CUSUM 5m
        for col in ["cusum", "cusum_zscore", "cusum_conf", "cusum_state"]:
            if col not in df.columns:
                logger.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ %s (CUSUM 5m).", col)

        if "cusum_state" in df.columns:
            bad_mask = ~df["cusum_state"].isin([0, 1, -1]) & df["cusum_state"].notna()
            if bad_mask.any():
                logger.warning("–ù–µ–≤–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è cusum_state –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–∞ %d —Å—Ç—Ä–æ–∫–∞—Ö.", int(bad_mask.sum()))

        if not quality_ok:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {diagnostics['failed_checks']}")

        return quality_ok, diagnostics

    def load_labeled_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ü–†–û–í–ï–†–ö–ê–ú–ò"""
        query = """
            SELECT 
                lr.symbol,
                lr.timestamp as extreme_timestamp,
                lr.reversal_label,
                lr.reversal_confidence as confidence,
                lr.labeling_method as method,
                lr.price_change_after as pnl,
                lr.features_json,
                lr.created_at,
                c.* 
            FROM labeling_results lr
            LEFT JOIN candles_5m c ON lr.timestamp = c.ts AND lr.symbol = c.symbol
            WHERE lr.symbol = ?
            ORDER BY lr.timestamp
        """
        try:
            positives = pd.read_sql_query(
                query,
                self.db_engine,
                params=(self.symbol,)
            )

            if positives.empty:
                logger.warning(f"‚ùå –ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")
                return pd.DataFrame()

            # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            if 'extreme_timestamp' in positives.columns:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç–∞–Ω—É—Ç NaN
                positives['extreme_timestamp'] = pd.to_numeric(positives['extreme_timestamp'], errors='coerce')
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp
                initial_count = len(positives)
                positives = positives.dropna(subset=['extreme_timestamp'])
                removed_count = initial_count - len(positives)
                if removed_count > 0:
                    logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç—Ä–æ–∫ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp")
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'extreme_timestamp' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(positives)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è {self.symbol}")
            if 'reversal_label' in positives.columns:
                logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {positives['reversal_label'].value_counts().to_dict()}")
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'reversal_label' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            return positives

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {err}")
            return pd.DataFrame()

    def safe_correlation_calculation(self, df, columns):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏"""
        try:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ columns —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ df
            available_columns = [col for col in columns if col in df.columns]

            if len(available_columns) < 2:
                return pd.DataFrame()

            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            numeric_cols = df[available_columns].select_dtypes(include=[np.number])

            if len(numeric_cols.columns) < 2:
                return pd.DataFrame()

            # –£–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
            numeric_cols = numeric_cols.loc[:, numeric_cols.std() > 0]

            if len(numeric_cols.columns) < 2:
                return pd.DataFrame()

            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            numeric_cols = numeric_cols.dropna()

            if len(numeric_cols) < 2:
                return pd.DataFrame()

            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", RuntimeWarning)
                corr_matrix = numeric_cols.corr().abs()

            return corr_matrix

        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {err}")
            return pd.DataFrame()

    def get_data_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ SQLAlchemy"""
        stats = {
            'symbol': self.symbol,
            'total_candles': 0,
            'period': 'N/A',
            'total_labels': 0,
            'buy_labels': 0,
            'sell_labels': 0,
            'avg_confidence': 0.0
        }

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.db_engine.connect() –≤–º–µ—Å—Ç–æ self.conn
            with self.db_engine.connect() as conn:
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–≤–µ—á–µ–π
                candles_result = conn.execute(
                    text("SELECT COUNT(*), MIN(ts), MAX(ts) FROM candles_5m WHERE symbol = :symbol"),
                    {'symbol': self.symbol}
                ).fetchone()

                if candles_result:
                    total_candles, min_ts, max_ts = candles_result
                    stats['total_candles'] = total_candles or 0
                    if min_ts and max_ts:
                        stats['period'] = f"{pd.to_datetime(min_ts, unit='ms')} to {pd.to_datetime(max_ts, unit='ms')}"

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫
                labels_result = conn.execute(
                    text("""
                        SELECT COUNT(*), AVG(reversal_confidence),
                               SUM(CASE WHEN reversal_label = 1 THEN 1 ELSE 0 END),
                               SUM(CASE WHEN reversal_label = 2 THEN 1 ELSE 0 END)
                        FROM labeling_results WHERE symbol = :symbol
                    """),
                    {'symbol': self.symbol}
                ).fetchone()

                if labels_result:
                    total_labels, avg_conf, buy_labels, sell_labels = labels_result
                    stats['total_labels'] = total_labels or 0
                    stats['buy_labels'] = buy_labels or 0
                    stats['sell_labels'] = sell_labels or 0
                    stats['avg_confidence'] = float(avg_conf) if avg_conf else 0.0

        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {err}")

        return stats

class AdvancedLabelingTool:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ —Å SQLAlchemy
    """

    def _ensure_training_snapshot_tables(self) -> None:
        """
        –°–æ–∑–¥–∞—ë—Ç (–µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç) —Ç–∞–±–ª–∏—Ü—ã training_dataset –∏ training_dataset_meta + –∏–Ω–¥–µ–∫—Å—ã.
        –£–º–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è: –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —É—Å—Ç–∞—Ä–µ–ª–∞.
        """

        with self.engine.begin() as conn:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ç–∞–±–ª–∏—Ü—ã
            try:
                result = conn.execute(text("PRAGMA table_info(training_dataset)")).fetchall()
                existing_columns = [row[1] for row in result]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–∞—Ä—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                has_old_structure = (
                        'features_json' in existing_columns or
                        'is_negative' in existing_columns or
                        'anti_trade_mask' in existing_columns
                )

                if has_old_structure:
                    logger.info("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ training_dataset - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É")
                    conn.execute(text("DROP TABLE IF EXISTS training_dataset"))
                elif existing_columns:
                    logger.info("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ training_dataset –∞–∫—Ç—É–∞–ª—å–Ω–∞ (29 –∫–æ–ª–æ–Ω–æ–∫)")
            except Exception:
                # –¢–∞–±–ª–∏—Ü—ã –Ω–µ—Ç - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —Å–æ–∑–¥–∞—Å—Ç—Å—è –Ω–∏–∂–µ
                logger.info("üìã –¢–∞–±–ª–∏—Ü–∞ training_dataset –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞")

            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã (IF NOT EXISTS –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è)
            conn.execute(text(CREATE_TRAINING_DATASET_SQL))
            for sql in CREATE_TRAINING_DATASET_INDEXES_SQL:
                conn.execute(text(sql))
            conn.execute(text(CREATE_TRAINING_DATASET_META_SQL))
            conn.execute(text(CREATE_TRAINING_FEATURE_IMPORTANCE_SQL))
            for sql in CREATE_TRAINING_FEATURE_IMPORTANCE_INDEXES_SQL:
                conn.execute(text(sql))

        logger.info("‚úÖ –¢–∞–±–ª–∏—Ü—ã training_dataset, training_dataset_meta, training_feature_importance –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã/—Å–æ–∑–¥–∞–Ω—ã")

    def __init__(self, config: LabelingConfig):
        self.config = config

        logger = logging.getLogger(__name__)
        _VALID_METHODS = {"CUSUM", "EXTREMUM", "PELT_ONLINE", "CUSUM_EXTREMUM"}

        m = (getattr(self.config, "method", None) or "CUSUM_EXTREMUM").upper()
        if m not in _VALID_METHODS:
            logger.warning(
                "Unknown labeling method '%s'. Falling back to 'CUSUM_EXTREMUM'. "
                "Allowed: %s",
                m, sorted(_VALID_METHODS),
            )
            m = "CUSUM_EXTREMUM"
        self.logger = logger
        self.config.method = m
        self.config.tool = self
        self.data_loader = DataLoader(
            db_engine=create_engine(MARKET_DB_DSN),
            symbol=config.symbol,
            timeframe=config.timeframe,
            config=config
        )

        self.labels = []
        self.engine = self.data_loader.connect()
        self.feature_names = self.data_loader.feature_names
        self._ensure_table_exists()
        self.config.pnl_threshold = 0.001
        logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω AdvancedLabelingTool –¥–ª—è {config.symbol}")

    def _check_table_exists(self, table_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —á–µ—Ä–µ–∑ SQLAlchemy"""
        from sqlalchemy import inspect
        try:
            inspector = inspect(self.engine)
            return table_name in inspector.get_table_names()
        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name}: {err}")
            return False

    def _validate_snapshot_frame(self, df: pd.DataFrame):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –≤ –ë–î.
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è 3-–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏.
        """
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        required = ["ts", "datetime", "reversal_label", "sample_weight"]
        missing = [col for col in required if col not in df.columns]
        if missing:
            raise ValueError(f"Snapshot validation failed: missing required columns: {missing}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π reversal_label (0=NO_SIGNAL/HOLD, 1=BUY, 2=SELL)
        allowed_labels = [0, 1, 2]
        if not df["reversal_label"].isin(allowed_labels).all():
            invalid_labels = list(df.loc[~df["reversal_label"].isin(allowed_labels), "reversal_label"].unique())
            raise ValueError(
                f"Invalid reversal_label values: {invalid_labels}. Expected: {allowed_labels}")

        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ts
        duplicates_count = df.duplicated(subset=["ts"]).sum()
        if duplicates_count > 0:
            logger.warning(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ts - —É–¥–∞–ª—è–µ–º")
            df = df.drop_duplicates(subset=["ts"], keep="first")

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        critical_cols = ["ts", "reversal_label", "sample_weight"]
        nan_mask = df[critical_cols].isna().any(axis=1)
        nan_drop_rows = nan_mask.sum()
        if nan_drop_rows > 0:
            logger.warning("‚ö†Ô∏è  –£–¥–∞–ª—è–µ–º %d —Å—Ç—Ä–æ–∫ —Å NaN –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö", nan_drop_rows)
            df = df[~nan_mask]

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        issues = {
            "duplicates_removed": int(duplicates_count),
            "nan_drop_rows": int(nan_drop_rows),
            "class_balance": df["reversal_label"].value_counts().to_dict(),
        }

        # ‚îÄ‚îÄ TS-–≤–∞–ª–∏–¥–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # 1) –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ts
        if not df["ts"].is_monotonic_increasing:
            logger.warning("‚ö†Ô∏è  ts –Ω–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—Ç ‚Äî —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ts")
            df = df.sort_values("ts").reset_index(drop=True)
            issues["ts_sorted"] = True
        else:
            issues["ts_sorted"] = False

        # 2) –ü–æ–∏—Å–∫ —Ä–∞–∑—Ä—ã–≤–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –µ—Å–ª–∏ –∑–Ω–∞–µ–º —à–∞–≥ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        timeframe_ms_map = {"1m": 60_000, "5m": 300_000, "15m": 900_000, "1h": 3_600_000}
        tf = str(getattr(self.config, "timeframe", "5m")).lower()
        expected_step = timeframe_ms_map.get(tf)

        ts_gaps_count = 0
        ts_gap_max = 0

        if expected_step is not None:
            ts_series = df["ts"].astype("int64").sort_values()
            diffs = ts_series.diff().dropna()
            bad_diffs = diffs[diffs != expected_step]

            if not bad_diffs.empty:
                ts_gaps_count = int(bad_diffs.shape[0])
                ts_gap_max = int(bad_diffs.max())
                logger.warning(
                    "‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ %d —Ä–∞–∑—Ä—ã–≤–æ–≤ –ø–æ ts (–æ–∂–∏–¥–∞–ª–∏ —à–∞–≥ %d –º—Å, max_gap=%d –º—Å)",
                    ts_gaps_count,
                    expected_step,
                    ts_gap_max,
                )

        issues["ts_gaps_count"] = ts_gaps_count
        issues["ts_gap_max"] = ts_gap_max

        return df, issues, int(nan_drop_rows), int(duplicates_count)


    def _ensure_table_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü —á–µ—Ä–µ–∑ SQLAlchemy (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)"""
        from sqlalchemy import text

        # labeling_results: –µ–¥–∏–Ω–∞—è —Å—Ö–µ–º–∞
        if not self._check_table_exists('labeling_results'):
            logger.info("üìã –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã labeling_results...")

            create_table_sql = text("""
                CREATE TABLE IF NOT EXISTS labeling_results (
                    symbol TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    timeframe TEXT NOT NULL,
                    reversal_label INTEGER NOT NULL,
                    reversal_confidence REAL DEFAULT 1.0,
                    labeling_method TEXT NOT NULL,
                    labeling_params TEXT,
                    extreme_index INTEGER,
                    extreme_price REAL,
                    extreme_timestamp INTEGER NOT NULL,
                    confirmation_index INTEGER,
                    confirmation_timestamp INTEGER,
                    price_change_after REAL,
                    features_json TEXT,
                    is_high_quality INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (symbol, extreme_timestamp, reversal_label)
                )
            """)

            # –ò–Ω–¥–µ–∫—Å—ã: –∑–∞–ø—Ä–æ—Å—ã —á–∞—Å—Ç–æ –∏–¥—É—Ç –±–µ–∑ reversal_label, –¥–æ–±–∞–≤–∏–º –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–π –∏–Ω–¥–µ–∫—Å.
            create_index_1 = text("""
                CREATE INDEX IF NOT EXISTS idx_labeling_results_symbol_ts
                ON labeling_results(symbol, extreme_timestamp)
            """)
            # –ï—Å–ª–∏ —É —Ç–µ–±—è —á–∞—Å—Ç–æ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä –ø–æ timeframe ‚Äî –¥–æ–±–∞–≤—å –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å:
            create_index_2 = text("""
                CREATE INDEX IF NOT EXISTS idx_labeling_results_symbol_tf_ts
                ON labeling_results(symbol, timeframe, extreme_timestamp)
            """)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º begin() –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏
            with self.engine.begin() as conn:
                conn.execute(create_table_sql)
                conn.execute(create_index_1)
                conn.execute(create_index_2)

            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —Å–æ–∑–¥–∞–Ω–∞")
        else:
            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    def load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            df = self.data_loader.load_indicators()
            quality_ok, diagnostics = self.data_loader.validate_data_quality(df)

            if not quality_ok:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {diagnostics}")

            return df
        except Exception as err:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {err}")

    def _get_all_existing_signals(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫ —Å–∏–º–≤–æ–ª–∞ –∏–∑ –ë–î"""
        if not self._check_table_exists('labeling_results'):
            return []

        try:
            query = """
                SELECT extreme_timestamp, extreme_index, reversal_label, labeling_method 
                FROM labeling_results 
                WHERE symbol = :symbol 
                ORDER BY extreme_index
            """

            with self.engine.connect() as conn:
                result = conn.execute(
                    text(query),
                    {'symbol': self.config.symbol}
                ).fetchall()

            signals = []
            for extreme_ts, extreme_idx, reversal_label, method in result:
                signals.append({
                    'extreme_timestamp': extreme_ts,
                    'extreme_index': extreme_idx,
                    'reversal_label': reversal_label,
                    'labeling_method': method
                })

            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(signals)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫ –∏–∑ –ë–î")
            return signals

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫: {err}")
            return []

    def _calculate_pnl_to_index(self, df: pd.DataFrame, entry_idx: int, signal_type: str, end_idx: int) -> Tuple[
        float, bool]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç PnL –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ñ–æ—Ä–º—É–ª–æ–π
        """
        # ‚úì –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–µ–∏—Ö –≥—Ä–∞–Ω–∏—Ü
        if entry_idx >= len(df) or end_idx >= len(df):
            return 0.0, False

        try:
            entry_price = df['close'].iloc[entry_idx]
            exit_price = df['close'].iloc[end_idx]

            if entry_price <= 0 or exit_price <= 0:
                return 0.0, False

            # ‚úì –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
            if signal_type == 'BUY':
                net_pnl = (exit_price * (1 - self.config.fee_percent) /
                           (entry_price * (1 + self.config.fee_percent))) - 1
            else:  # SELL
                net_pnl = (entry_price * (1 - self.config.fee_percent) /
                           (exit_price * (1 + self.config.fee_percent))) - 1

            is_profitable_enough = net_pnl >= self.config.min_profit_target
            return net_pnl, is_profitable_enough

        except (IndexError, ZeroDivisionError, KeyError) as err:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ PnL –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {entry_idx}: {err}")
            return 0.0, False

    # =========================================================================
    # –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –†–ê–ó–ú–ï–¢–ö–ò (–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞)
    # =========================================================================

    def _calculate_pnl(self, df: pd.DataFrame, entry_idx: int, signal_type: str) -> Tuple[float, bool]:
        """
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á—ë—Ç PnL —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≥—Ä–∞–Ω–∏—Ü –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º—É–ª–æ–π –∫–æ–º–∏—Å—Å–∏–π
        """
        exit_idx = entry_idx + self.config.hold_bars

        # ‚úì –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü –î–û –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ iloc
        if entry_idx >= len(df) or exit_idx >= len(df):
            return 0.0, False

        try:
            entry_price = df['close'].iloc[entry_idx]
            exit_price = df['close'].iloc[exit_idx]

            if entry_price <= 0 or exit_price <= 0:
                return 0.0, False

            # ‚úì –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å —É—á—ë—Ç–æ–º –∫–æ–º–∏—Å—Å–∏–π
            if signal_type == 'BUY':
                # –ü–æ–∫—É–ø–∫–∞: –ø–ª–∞—Ç–∏–º –∫–æ–º–∏—Å—Å–∏—é –ø—Ä–∏ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ
                net_pnl = (exit_price * (1 - self.config.fee_percent) /
                           (entry_price * (1 + self.config.fee_percent))) - 1
            else:  # SELL
                # –®–æ—Ä—Ç: –ø–ª–∞—Ç–∏–º –∫–æ–º–∏—Å—Å–∏—é –ø—Ä–∏ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ
                net_pnl = (entry_price * (1 - self.config.fee_percent) /
                           (exit_price * (1 + self.config.fee_percent))) - 1

            is_profitable_enough = net_pnl >= self.config.min_profit_target
            return net_pnl, is_profitable_enough

        except (IndexError, ZeroDivisionError, KeyError) as err:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ PnL –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {entry_idx}: {err}")
            return 0.0, False

    def _interpret_pnl_results(self, total_metrics: Dict[str, float]):
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ PnL —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏"""
        total_pnl = total_metrics.get('total_pnl', 0)
        success_rate = total_metrics.get('success_rate', 0)
        pnl_ratio = total_metrics.get('pnl_ratio', 0)

        print(f"\nüîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")

        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        if success_rate > 0.6:
            print(f"   üéØ –í—ã—Å–æ–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (>60%)")
        elif success_rate > 0.4:
            print(f"   ‚úÖ –°—Ä–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (40-60%)")
        else:
            print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (<40%)")

        # –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è PNL
        if pnl_ratio > 3:
            print(f"   üíé –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–æ–≤: {pnl_ratio:.1f}:1")
        elif pnl_ratio > 2:
            print(f"   ‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–æ–≤: {pnl_ratio:.1f}:1")
        elif pnl_ratio > 1:
            print(f"   ‚ö†Ô∏è  –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1")
        else:
            print(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1")

        # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–π –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∫–∞–∫ –µ—Å—Ç—å)
        if total_pnl > 0.1:  # >10%
            print(f"   üöÄ –í—ã—Å–æ–∫–∞—è –æ–±—â–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        elif total_pnl > 0.02:  # >2%
            print(f"   ‚úÖ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        elif total_pnl > 0:  # >0%
            print(f"   ‚ö†Ô∏è  –°–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        else:
            print(f"   ‚ùå –£–±—ã—Ç–æ—á–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {total_pnl:+.1%}")

    def _smart_confirmation_system(self, df: pd.DataFrame, signal_idx: int, signal_type: str) -> dict:
        """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"""
        confirm_bar = self._get_confirmation_bars(signal_type)

        confirmation_data = {
            'confirmed': False,
            'confidence_boost': 0.0,
            'early_rejection': False,
            'confirmation_index': signal_idx + confirm_bar,
            'price_change': 0.0
        }

        if signal_idx + 3 >= len(df):
            confirmation_data['early_rejection'] = True
            return confirmation_data

        price_at_signal = df['close'].iloc[signal_idx]
        price_after_3bars = df['close'].iloc[signal_idx + 3]
        expected_move = 0.005

        if signal_type == 'BUY':
            move_percent = (price_after_3bars - price_at_signal) / price_at_signal
            if move_percent < -expected_move:
                confirmation_data['early_rejection'] = True
        else:
            move_percent = (price_at_signal - price_after_3bars) / price_at_signal
            if move_percent < -expected_move:
                confirmation_data['early_rejection'] = True

        if not confirmation_data['early_rejection'] and signal_idx + confirm_bar < len(df):
            confirmation_data['confirmed'] = True
            confirmation_data['price_change'] = move_percent
            if abs(move_percent) > expected_move:
                confirmation_data['confidence_boost'] = 0.15

        return confirmation_data

    def merge_conflicting_labels(self):
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ–π —Ä–∞–∑–º–µ—Ç–∫–∏"""
        try:
            df = self.load_data()
            with self.engine.begin() as conn:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ BUY/SELL –º–µ—Ç–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                signals_query = """
                    SELECT rowid, extreme_timestamp, reversal_label, labeling_method, price_change_after
                    FROM labeling_results 
                    WHERE symbol = :symbol AND reversal_label IN (1,2)
                    ORDER BY extreme_timestamp
                """
                signals = conn.execute(text(signals_query), {'symbol': self.config.symbol}).fetchall()

                if not signals:
                    print("‚úÖ –ù–µ—Ç BUY/SELL –º–µ—Ç–æ–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                    return 0

                fixed_count = 0

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–∞—Ä—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
                for i in range(len(signals) - 1):
                    current_signal = signals[i]
                    next_signal = signals[i + 1]

                    current_ts = current_signal.extreme_timestamp
                    next_ts = next_signal.extreme_timestamp

                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ PNL
                    current_idx_match = df.index[df['ts'] == current_ts].tolist()
                    next_idx_match = df.index[df['ts'] == next_ts].tolist()

                    if not current_idx_match or not next_idx_match:
                        continue

                    current_idx = int(current_idx_match[0])
                    next_idx = int(next_idx_match[0])

                    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º PNL –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
                    signal_type = 'BUY' if current_signal.reversal_label == 1 else 'SELL'
                    pnl, _ = self._calculate_pnl_to_index(df, current_idx, signal_type, next_idx)

                    # –ï—Å–ª–∏ PNL < 0.1% - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É
                    if abs(pnl) < 0.001:
                        print(f"üîÑ –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–∞ ts={current_ts}, PNL={pnl:.4f}")

                        # –£–¥–∞–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–∏–≥–Ω–∞–ª
                        conn.execute(text("DELETE FROM labeling_results WHERE rowid = :rowid"),
                                     {'rowid': current_signal.rowid})

                        # –°—Ç–∞–≤–∏–º HOLD –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º –±–∞—Ä–µ
                        if current_idx + 1 < len(df):
                            next_ts_hold = int(df.iloc[current_idx + 1]['ts'])
                            conn.execute(text("""
                                INSERT OR IGNORE INTO labeling_results 
                                (symbol, timestamp, timeframe, reversal_label, reversal_confidence, 
                                 labeling_method, extreme_timestamp, price_change_after)
                                VALUES (:symbol, :ts, :tf, 0, 1.0, 'VALIDATED_HOLD', :ts, 0.0)
                            """), {'symbol': self.config.symbol, 'ts': next_ts_hold, 'tf': self.config.timeframe})

                        fixed_count += 1

                print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –º–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {fixed_count}")
                return fixed_count

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏: {err}")
            raise

    def _get_confirmation_bars(self, signal_type: str) -> int:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞ —Ä–∞–∑–º–µ—Ç–∫–∏.
        –î–µ–ª–∞–µ—Ç –º–µ—Ç–æ–¥ —É—Å—Ç–æ–π—á–∏–≤—ã–º –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É/–≤–≤–æ–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        import logging
        logger = logging.getLogger(__name__)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤–≤–æ–¥—É)
        method = (getattr(self.config, "method", "") or "").upper()

        # –ë–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤)
        if method == "PELT_ONLINE":
            base_confirmation = 3
        elif method in ("CUSUM", "EXTREMUM", "CUSUM_EXTREMUM"):
            base_confirmation = 2
        else:
            logger.warning(
                "Unknown labeling method '%s' in _get_confirmation_bars; using base_confirmation=2",
                method
            )
            base_confirmation = 2

        confirmation_bars = base_confirmation
        if self.config.method == 'EXTREMUM':
            confirmation_bars += 1  # —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è EXTREMUM

        logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è {signal_type} —Å–∏–≥–Ω–∞–ª–∞")
        return max(1, min(5, confirmation_bars))  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏

    def _binseg_reversals(self, df: pd.DataFrame) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π BinSeg —Å –ª—É—á—à–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        """
        if not RUPTURES_AVAILABLE:
            logger.warning("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ruptures –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return []

        # ‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        optimal_samples = min(5000, len(df))
        if len(df) > optimal_samples:
            logger.info(f"‚ö° –°–æ–∫—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å {len(df)} –¥–æ {optimal_samples} samples")
            df = df.iloc[-optimal_samples:].copy()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        close_vals = df['close'].astype(float).values

        # üéØ –ò–°–ü–û–õ–¨–ó–£–ï–ú –†–ê–ó–ù–´–ï –¢–ò–ü–´ –°–ò–ì–ù–ê–õ–û–í –î–õ–Ø –õ–£–ß–®–ï–ì–û –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø
        signals = {}

        # 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ (–æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏–≥–Ω–∞–ª)
        log_prices = np.log(np.clip(close_vals, 1e-12, None))
        returns = np.diff(log_prices)
        returns = np.insert(returns, 0, 0)
        signals['returns'] = returns

        # 2. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª)
        price_mean = np.mean(close_vals)
        price_std = np.std(close_vals)
        if price_std > 0:
            normalized_prices = (close_vals - price_mean) / price_std
            signals['normalized'] = normalized_prices

        # 3. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏)
        volatility = np.abs(returns) * 100  # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        signals['volatility'] = volatility

        print("üîç BinSeg: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")

        # üéØ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ü–û–î–ë–û–† –ü–ê–†–ê–ú–ï–¢–†–û–í
        best_result = None
        best_score = -np.inf

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        for signal_name, signal_data in signals.items():
            for n_bkps in [8, 12, 15, 18, 20, 25]:  # –ë–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
                for model in ["l2", "rbf"]:  # –¢–æ–ª—å–∫–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏
                    try:
                        if n_bkps >= len(signal_data) // 3:
                            continue

                        # –ó–∞–ø—É—Å–∫–∞–µ–º BinSeg
                        algo = rpt.Binseg(model=model, min_size=15, jump=8).fit(signal_data)
                        changepoints = algo.predict(n_bkps=n_bkps)

                        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ç–æ—á–∫–∏
                        changepoints = [cp for cp in changepoints if 15 < cp < len(df) - 15]

                        if len(changepoints) < 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 —Ç–æ—á–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                            continue

                        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
                        score = self._evaluate_segmentation_improved(df, changepoints, signal_name)

                        # –ë–æ–Ω—É—Å –∑–∞ –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
                        potential_signals = self._count_potential_signals(df, changepoints)
                        score += min(potential_signals * 0.01, 0.1)  # –ë–æ–Ω—É—Å –¥–æ 0.1

                        if score > best_score:
                            best_score = score
                            best_result = {
                                'signal': signal_name,
                                'model': model,
                                'n_bkps': n_bkps,
                                'changepoints': changepoints,
                                'score': score,
                                'potential_signals': potential_signals
                            }

                        print(
                            f"  {signal_name:12} model={model}, n_bkps={n_bkps:2} ‚Üí {len(changepoints):2} —Ç–æ—á–µ–∫, score={score:.3f}, signals={potential_signals}")

                    except Exception as err:
                        # print(f"  {signal_name:12} model={model}, n_bkps={n_bkps:2} ‚Üí –æ—à–∏–±–∫–∞: {err}")
                        continue

        if not best_result:
            logger.warning("‚ùå BinSeg –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞")
            return []

        changepoints = best_result['changepoints']
        print(
            f"‚úÖ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_result['signal']}, model={best_result['model']}, n_bkps={best_result['n_bkps']}")
        print(
            f"üìä –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–µ–∫ —Ä–∞–∑—Ä—ã–≤–∞: {len(changepoints)}, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {best_result['potential_signals']}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ —Ä–∞–∑—Ä—ã–≤–∞ –≤ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        results = self._convert_changepoints_to_signals_improved(df, changepoints)

        logger.info(f"üìä BinSeg –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Å–∏–≥–Ω–∞–ª–æ–≤")

        # üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–ì–ù–ê–õ–û–í
        if results:
            buy_count = sum(1 for r in results if r['type'] == 'BUY')
            sell_count = sum(1 for r in results if r['type'] == 'SELL')
            avg_confidence = np.mean([r['confidence'] for r in results])
            print(f"üìà –ò—Ç–æ–≥–∏: {buy_count} BUY, {sell_count} SELL, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2f}")

        return results

    def _convert_changepoints_to_signals_improved(self, df: pd.DataFrame, changepoints: List[int]) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å –ª—É—á—à–µ–π –ª–æ–≥–∏–∫–æ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è BUY/SELL
        """
        results = []

        for i in range(1, len(changepoints) - 1):
            current_cp = changepoints[i]  # –¢–æ—á–∫–∞ —Ä–∞–∑—Ä—ã–≤–∞
            prev_cp = changepoints[i - 1]  # –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
            next_cp = changepoints[i + 1]  # –ö–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞

            if current_cp >= len(df) or next_cp >= len(df) or prev_cp >= len(df):
                continue

            # üîç –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–†–ï–ù–î–û–í
            # –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ç—Ä–µ–Ω–¥ (2/3 —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
            prev_segment_len = current_cp - prev_cp
            analysis_start = prev_cp + prev_segment_len // 3  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            price_prev_start = df['close'].iloc[analysis_start]
            price_prev_end = df['close'].iloc[current_cp - 1]
            prev_trend = (price_prev_end - price_prev_start) / price_prev_start

            # –¢–µ–∫—É—â–∏–π —Ç—Ä–µ–Ω–¥ (2/3 —Å–µ–≥–º–µ–Ω—Ç–∞)
            current_segment_len = next_cp - current_cp
            analysis_end = current_cp + (2 * current_segment_len) // 3
            if analysis_end >= len(df):
                analysis_end = len(df) - 1

            price_current_start = df['close'].iloc[current_cp]
            price_current_end = df['close'].iloc[analysis_end]
            current_trend = (price_current_end - price_current_start) / price_current_start

            # üéØ –£–õ–£–ß–®–ï–ù–ù–´–ï –ö–†–ò–¢–ï–†–ò–ò:
            min_trend_strength = 0.003  # 0.3% –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ

            # –°–ò–ì–ù–ê–õ BUY: —Å–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ ‚Üí —Å–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç
            if (prev_trend < -min_trend_strength and
                    current_trend > min_trend_strength and
                    abs(current_trend) > abs(prev_trend) * 0.3):  # –ú–µ–Ω—å—à–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Å–∏–ª–µ

                rev_type = "BUY"
                confidence = min(abs(current_trend) * 15 + abs(prev_trend) * 10, 0.95)

            # –°–ò–ì–ù–ê–õ SELL: —Å–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç ‚Üí —Å–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
            elif (prev_trend > min_trend_strength and
                  current_trend < -min_trend_strength and
                  abs(current_trend) > abs(prev_trend) * 0.3):

                rev_type = "SELL"
                confidence = min(abs(current_trend) * 15 + abs(prev_trend) * 10, 0.95)

            else:
                continue

            # üöÄ –í–•–û–î –ù–ê –°–õ–ï–î–£–Æ–©–ï–ô –°–í–ï–ß–ï
            entry_index = current_cp + 1
            if entry_index >= len(df):
                continue

            # ‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–ï –°–ò–ì–ù–ê–õ–ê
            confirmation = self._smart_confirmation_system(df, entry_index, rev_type)

            if confirmation['early_rejection']:
                continue

            conf_idx = confirmation['confirmation_index']
            if conf_idx >= len(df):
                conf_idx = len(df) - 1

            results.append({
                'index': entry_index,
                'type': rev_type,
                'confidence': confidence,
                'extreme_index': current_cp,
                'extreme_timestamp': int(df['ts'].iloc[current_cp]),
                'confirmation_index': conf_idx,
                'confirmation_timestamp': int(df['ts'].iloc[conf_idx]),
                'method': 'BINSEG',
                'reversal_label': 1 if rev_type == 'BUY' else 2,
            })

        return results

    def _evaluate_segmentation_improved(self, df: pd.DataFrame, changepoints: List[int], signal_type: str) -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        """
        if len(changepoints) < 3:
            return -np.inf

        close_vals = df['close'].values
        total_variance = np.var(close_vals)

        if total_variance == 0:
            return 0.0

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é
        segments = []
        start_idx = 0
        for cp in changepoints:
            if cp > start_idx:
                segments.append((start_idx, cp))
                start_idx = cp
        segments.append((start_idx, len(close_vals)))

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–Ω—É—Ç—Ä–∏—Å–µ–≥–º–µ–Ω—Ç–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é
        within_segment_variance = 0.0
        segment_quality = 0.0

        for start, end in segments:
            if end - start > 2:  # –ú–∏–Ω–∏–º—É–º 3 —Ç–æ—á–∫–∏ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
                segment_data = close_vals[start:end]
                seg_variance = np.var(segment_data)
                within_segment_variance += seg_variance * (end - start)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–∞ (–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å)
                if len(segment_data) > 3:
                    x = np.arange(len(segment_data))
                    correlation = np.corrcoef(x, segment_data)[0, 1]
                    if not np.isnan(correlation):
                        segment_quality += abs(correlation) * (end - start)

        within_segment_variance /= len(close_vals)
        explained_variance = 1 - (within_segment_variance / total_variance)

        # –ö–∞—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—á–µ–º –±–æ–ª–µ–µ –ª–∏–Ω–µ–π–Ω—ã —Å–µ–≥–º–µ–Ω—Ç—ã, —Ç–µ–º –ª—É—á—à–µ)
        segment_quality /= len(close_vals)

        # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ/–º–∞–ª–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        optimal_segments = len(close_vals) // 100  # –û–ø—Ç–∏–º—É–º ~1 —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ 100 –±–∞—Ä–æ–≤
        segment_penalty = abs(len(segments) - optimal_segments) * 0.02

        final_score = explained_variance * 0.7 + segment_quality * 0.3 - segment_penalty

        return max(0, final_score)

    def _count_potential_signals(self, df: pd.DataFrame, changepoints: List[int]) -> int:
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        """
        count = 0
        for i in range(1, len(changepoints) - 1):
            cp = changepoints[i]
            if cp + 1 < len(df):
                count += 1
        return count

    def _evaluate_segmentation(self, df: pd.DataFrame, changepoints: List[int]) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        """
        if len(changepoints) < 2:
            return -np.inf

        close_vals = df['close'].values
        total_variance = np.var(close_vals)

        if total_variance == 0:
            return 0.0

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é
        explained_variance = 0.0
        segments = []

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
        start_idx = 0
        for cp in changepoints:
            if cp > start_idx:
                segments.append((start_idx, cp))
                start_idx = cp
        segments.append((start_idx, len(close_vals)))

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–Ω—É—Ç—Ä–∏—Å–µ–≥–º–µ–Ω—Ç–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é
        within_segment_variance = 0.0
        for start, end in segments:
            if end - start > 1:
                segment_data = close_vals[start:end]
                within_segment_variance += np.var(segment_data) * (end - start)

        within_segment_variance /= len(close_vals)
        explained_variance = 1 - (within_segment_variance / total_variance)

        # –®—Ç—Ä–∞—Ñ—É–µ–º –∑–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        penalty = len(changepoints) * 0.01

        return explained_variance - penalty

    def _convert_changepoints_to_signals(self, df: pd.DataFrame, changepoints: List[int]) -> List[Dict]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ —Ä–∞–∑—Ä—ã–≤–∞ –≤ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        """
        results = []

        for i in range(len(changepoints) - 1):
            start = changepoints[i]
            end = changepoints[i + 1]

            if start >= len(df) or end >= len(df):
                continue

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
            segment_prices = df['close'].iloc[start:end].values
            price_start = segment_prices[0]
            price_end = segment_prices[-1]

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
            price_change = (price_end - price_start) / price_start
            trend_up = price_change > 0

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if i > 0:
                prev_start = changepoints[i - 1]
                prev_segment = df['close'].iloc[prev_start:start].values
                prev_trend_up = (prev_segment[-1] - prev_segment[0]) / prev_segment[0] > 0

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–≤–æ—Ä–æ—Ç
                rev_type = None
                if not prev_trend_up and trend_up:
                    rev_type = "BUY"
                elif prev_trend_up and not trend_up:
                    rev_type = "SELL"

                if rev_type:
                    # –í—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å–≤–µ—á–µ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞
                    entry_index = start + 1
                    if entry_index >= len(df):
                        continue

                    # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è
                    confidence = min(abs(price_change) * 20, 0.95)
                    confidence = max(confidence, 0.3)

                    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞
                    confirmation = self._smart_confirmation_system(df, entry_index, rev_type)
                    conf_idx = confirmation['confirmation_index']
                    if conf_idx >= len(df):
                        conf_idx = len(df) - 1

                    results.append({
                        'index': entry_index,
                        'type': rev_type,
                        'confidence': confidence,
                        'extreme_index': start,
                        'extreme_timestamp': int(df['ts'].iloc[start]),
                        'confirmation_index': conf_idx,
                        'confirmation_timestamp': int(df['ts'].iloc[conf_idx]),
                        'method': 'BINSEG',
                        'reversal_label': 1 if rev_type == 'BUY' else 2,
                    })

        return results

    def _pelt_offline_reversals(self, df: pd.DataFrame) -> List[Dict]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è PELT-—Ä–∞–∑–º–µ—Ç–∫–∞ —Å –∑–∞–º–µ–Ω–æ–π –Ω–∞ Binseg –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏–π.
        –í—Ö–æ–¥ ‚Äî —Å–ª–µ–¥—É—é—â–∞—è —Å–≤–µ—á–∞ –ø–æ—Å–ª–µ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞ (–∫–∞–∫ —É EXTREMUM).
        """
        if not RUPTURES_AVAILABLE:
            logger.warning("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ruptures –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return []

        if len(df) < 500:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PELT Offline: {len(df)}")
            return []

        logger.info(f"üìä PELT Offline (Binseg): –∞–Ω–∞–ª–∏–∑ {len(df)} —Å–≤–µ—á–µ–π...")

        # ‚ö° –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        MAX_SAMPLES = 5000
        if len(df) > MAX_SAMPLES:
            logger.info(f"‚ö° –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å {len(df)} ‚Üí {MAX_SAMPLES}")
            df = df.iloc[-MAX_SAMPLES:].copy()

        # === 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏–≥–Ω–∞–ª–∞ ===
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º —Ü–µ–Ω ‚Äî —É—Å—Ç–æ–π—á–∏–≤–æ –∫ –º–∞—Å—à—Ç–∞–±—É –∏ look-ahead
            close_vals = df['close'].astype(float).values
            signal = np.log(np.clip(close_vals, 1e-12, None))
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å–∏–≥–Ω–∞–ª–∞: {e}")
            return []

        # === 2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ ===
        print("\n" + "=" * 60)
        print("üéØ –ù–ê–°–¢–†–û–ô–ö–ê BINSEG: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤")
        print("=" * 60)
        print("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é:")
        print("   üìä [1] –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è:  3‚Äì5 / –¥–µ–Ω—å  (~600‚Äì1000)")
        print("   üìä [2] –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: 10‚Äì15 / –¥–µ–Ω—å (~2000‚Äì3000) ‚Üê —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è")
        print("   üìä [3] –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è:     20‚Äì30 / –¥–µ–Ω—å (~4000‚Äì6000)")
        print("   ‚öôÔ∏è  [4] –°–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ")

        choice = input("\n–í–∞—à –≤—ã–±–æ—Ä [2]: ").strip()
        if choice == '1':
            target_signals_daily = 4.0
        elif choice == '3':
            target_signals_daily = 25.0
        elif choice == '4':
            try:
                val = input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –¥–µ–Ω—å [12]: ").strip()
                target_signals_daily = float(val) if val else 12.0
                if not (1 <= target_signals_daily <= 50):
                    print("‚ö†Ô∏è –î–∏–∞–ø–∞–∑–æ–Ω 1‚Äì50. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ 12.")
                    target_signals_daily = 12.0
            except ValueError:
                print("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ 12.")
                target_signals_daily = 12.0
        else:
            target_signals_daily = 12.0

        print(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ: ~{target_signals_daily:.1f} —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å")

        # === 3. –†–∞—Å—á—ë—Ç —Ü–µ–ª–µ–≤–æ–≥–æ —á–∏—Å–ª–∞ changepoints ===
        bars_per_day = 288  # 5m
        n_samples = len(signal)
        expected_signals = target_signals_daily * (n_samples / bars_per_day)

        # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: changepoints ‚âà 2.55 √ó —Å–∏–≥–Ω–∞–ª—ã (–∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏)
        SIGNAL_TO_CHANGEPOINT_RATIO = 2.55
        target_changepoints = int(expected_signals * SIGNAL_TO_CHANGEPOINT_RATIO)

        print(f"üéØ –¶–µ–ª—å: {target_changepoints} changepoints (‚âà {expected_signals:.0f} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        # === 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ change points —á–µ—Ä–µ–∑ Binseg (–±—ã—Å—Ç—Ä–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ) ===
        try:
            algo = rpt.Binseg(model="l2", min_size=10, jump=5).fit(signal)
            changepoints = algo.predict(n_bkps=target_changepoints)
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É (ruptures –¥–æ–±–∞–≤–ª—è–µ—Ç len(signal) –∫–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω–µ—Ü)
            changepoints = [cp for cp in changepoints if cp < len(df)]
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(changepoints)} changepoints")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Binseg: {e}")
            return []

        # === 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BUY/SELL –ø–æ —Ç—Ä–µ–Ω–¥–∞–º –º–µ–∂–¥—É changepoints ===
        results = []
        for i in range(1, len(changepoints) - 1):
            prev_cp = changepoints[i - 1]
            cur_cp = changepoints[i]
            next_cp = changepoints[i + 1]

            if cur_cp >= len(df):
                continue

            # –¢—Ä–µ–Ω–¥ –¥–æ —Ç–æ—á–∫–∏: [prev_cp ‚Üí cur_cp)
            trend_prev_up = df['close'].iat[cur_cp - 1] > df['close'].iat[prev_cp]
            # –¢—Ä–µ–Ω–¥ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏: [cur_cp ‚Üí next_cp)
            trend_next_up = df['close'].iat[next_cp - 1] > df['close'].iat[cur_cp]

            rev_type = None
            if not trend_prev_up and trend_next_up:
                rev_type = "BUY"
            elif trend_prev_up and not trend_next_up:
                rev_type = "SELL"
            if rev_type is None:
                continue

            # üîÅ –í–•–û–î –ù–ê –°–õ–ï–î–£–Æ–©–ï–ô –°–í–ï–ß–ï –ü–û–°–õ–ï –≠–ö–°–¢–†–ï–ú–£–ú–ê (–ø—Ä–∞–≤–∏–ª—å–Ω–æ!)
            entry_idx = cur_cp + 1
            if entry_idx >= len(df):
                continue

            # –†–∞—Å—á—ë—Ç confidence: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π price move
            move_abs = abs(df['close'].iat[next_cp - 1] - df['close'].iat[cur_cp])
            move_rel = move_abs / df['close'].iat[cur_cp]
            confidence = np.clip(move_rel * 10, 0.3, 0.95)  # 0.3‚Äì0.95

            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞
            confirmation = self._smart_confirmation_system(df, entry_idx, rev_type)
            conf_idx = confirmation['confirmation_index']
            if conf_idx >= len(df):
                conf_idx = len(df) - 1

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            results.append({
                'index': entry_idx,
                'type': rev_type,
                'confidence': float(confidence),
                'extreme_index': int(cur_cp),
                'extreme_timestamp': int(df['ts'].iloc[cur_cp]),
                'confirmation_index': int(conf_idx),
                'confirmation_timestamp': int(df['ts'].iloc[conf_idx]),
                'method': 'PELT_OFFLINE_BINSEG',
                'reversal_label': 1 if rev_type == 'BUY' else 2,
            })

        # === 6. –û—Ç—á—ë—Ç ===
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤")
        if results:
            buy_cnt = sum(1 for r in results if r['type'] == 'BUY')
            sell_cnt = sum(1 for r in results if r['type'] == 'SELL')
            avg_conf = np.mean([r['confidence'] for r in results])
            print(f"üìà –°–∏–≥–Ω–∞–ª—ã: {buy_cnt} BUY, {sell_cnt} SELL, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.2f}")

        return results

    def _cusum_reversals(self, df):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–≤–µ—Ä—Å–∏–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ –≥–æ—Ç–æ–≤—ã–º –ø–æ–ª—è–º CUSUM 5m –∏–∑ –ë–î.
        –ö–æ–¥–∏—Ä–æ–≤–∫–∞: BUY=1, SELL=2, HOLD=0 (HOLD –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º).
        –û—Ç–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ |cusum_zscore| –∏/–∏–ª–∏ cusum_conf —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º.
        """
        cfg = self.config  # LabelingConfig
        out = []

        # –¢—Ä–µ–±—É–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        need = ["ts", "cusum_state", "cusum_zscore", "cusum_conf"]
        if any(c not in df.columns for c in need):
            logger.warning("CUSUM reversals: –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ %s", need)
            return out

        # –∏–Ω–¥–µ–∫—Å—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã: —Å–∏–ª—å–Ω—ã–π z –∏–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        z = df["cusum_zscore"].astype(float)
        conf = df["cusum_conf"].astype(float)
        state = df["cusum_state"].astype("Int64")  # 1/2/0

        cand_mask = (z.abs() >= float(getattr(cfg, "cusum_z_threshold", 1.0))) | \
                    (conf >= float(getattr(cfg, "cusum_conf_threshold", 0.6)))

        idxs = df.index[cand_mask & state.isin([1, 2])]
        if len(idxs) == 0:
            return out

        for i in idxs:
            s = int(state.loc[i])
            signal_type = "BUY" if s == Direction.BUY else "SELL"
            label = Direction.BUY if s == Direction.BUY else Direction.SELL
            base_conf = float(conf.loc[i]) if pd.notna(conf.loc[i]) else 0.0
            base_conf = max(0.2, min(0.95, base_conf))  # –º—è–≥–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã

            # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            confirm = self._smart_confirmation_system(df, i, signal_type)

            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∏–≥–Ω–∞–ª–∞ —Å –í–°–ï–ú–ò –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–æ–ª—è–º–∏
            signal_data = {
                "index": int(i),
                "type": signal_type,  # "BUY"/"SELL"
                "confidence": max(0.0, min(0.99, base_conf + float(confirm.get("confidence_boost", 0.0)))),
                "extreme_timestamp": int(df["ts"].loc[i]) if "ts" in df.columns and pd.notna(df["ts"].loc[i]) else None,
                "confirmation_index": confirm['confirmation_index'],
                "confirmation_timestamp": int(df["ts"].iloc[confirm['confirmation_index']]) if confirm[
                                                                                                   'confirmation_index'] < len(
                    df) else None,
                "method": 'CUSUM'
            }

            out.append(signal_data)

        return out

    def _extremum_reversals(self, df: pd.DataFrame) -> List[Dict]:
        """–≠–∫—Å—Ç—Ä–µ–º—É–º —Ä–µ–≤–µ—Ä—Å–∏–∏ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ–º –∏ –ø–µ—Ä–µ—Å–∫–æ–∫–æ–º –æ–∫–Ω–∞"""
        window = self.config.extremum_window
        confirm_bar = max(1, min(5, self.config.extremum_confirm_bar))
        min_distance = getattr(self.config, 'min_signal_distance', 10)
        low = df['low'].values
        high = df['high'].values
        results = []

        # –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å –Ω–∞—á–∞–ª–∞ –æ–∫–Ω–∞
        i = window
        last_extremum_type = None  # 'BUY' –∏–ª–∏ 'SELL'

        while i < len(df) - window:
            current_low = low[i]
            current_high = high[i]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∏–Ω–∏–º—É–º –≤ –æ–∫–Ω–µ
            is_low_extreme = current_low == np.min(low[i - window:i + window + 1])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∞–∫—Å–∏–º—É–º –≤ –æ–∫–Ω–µ
            is_high_extreme = current_high == np.max(high[i - window:i + window + 1])

            signal_type = None

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ —Å —É—á–µ—Ç–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if is_low_extreme and last_extremum_type != 'BUY':
                signal_type = 'BUY'
            elif is_high_extreme and last_extremum_type != 'SELL':
                signal_type = 'SELL'

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —ç–∫—Å—Ç—Ä–µ–º—É–º
            if signal_type and i + confirm_bar < len(df):
                confirmation = self._smart_confirmation_system(df, i, signal_type)
                if not confirmation['early_rejection']:
                    results.append({
                        'index': i,
                        'type': signal_type,
                        'confidence': 0.7 + confirmation['confidence_boost'],
                        'extreme_timestamp': df['ts'].iloc[i],
                        'confirmation_index': confirmation['confirmation_index'],
                        'confirmation_timestamp': df['ts'].iloc[confirmation['confirmation_index']],
                        'method': 'EXTREMUM'
                    })
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ–º –æ–∫–Ω–æ
                    last_extremum_type = signal_type
                    i += min_distance  # –ü–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ–º –Ω–∞ min_distance –≤–ø–µ—Ä–µ–¥
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—ã—á–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ i

            # –ï—Å–ª–∏ —ç–∫—Å—Ç—Ä–µ–º—É–º –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ –ø—Ä–æ—à–µ–ª –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ - –¥–≤–∏–≥–∞–µ–º—Å—è –Ω–∞ 1 –±–∞—Ä
            i += 1

        return results

    def _cusum_extremum_hybrid(self, df):
        """
        –ì–∏–±—Ä–∏–¥ CUSUM (–∏–∑ –ë–î) + —ç–∫—Å—Ç—Ä–µ–º—É–º—ã. –¢–∏–ø –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º ‚â§ 2.
        –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –¥–≤—É—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ (–∏–ª–∏ –ø–æ –≤–∞—à–µ–π —Ç–µ–∫—É—â–µ–π —Ñ–æ—Ä–º—É–ª–µ).
        """
        cusum_signals = self._cusum_reversals(df)
        extremum_signals = self._extremum_reversals(df)

        if not cusum_signals:
            return extremum_signals
        if not extremum_signals:
            return cusum_signals

        # –±—ã—Å—Ç—Ä—ã–π –º–∞–ø –ø–æ —Ç–∏–ø—É
        by_type = {"BUY": [], "SELL": []}
        for s in cusum_signals:
            by_type[s["type"]].append(s)

        out = []
        for e in extremum_signals:
            group = by_type.get(e["type"], [])
            # –Ω–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏–∑ CUSUM (–ø–æ—Ä–æ–≥ 2 –±–∞—Ä–∞)
            best = None
            best_d = None
            for c in group:
                d = abs(int(c["index"]) - int(e["index"]))
                if d <= 2 and (best is None or d < best_d):
                    best, best_d = c, d
            if best is not None:
                # –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                conf = (float(best["confidence"]) + float(e.get("confidence", 0.0))) / 2.0
                merged = e.copy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ø–∏—é extremum —Å–∏–≥–Ω–∞–ª–∞ –∫–∞–∫ –æ—Å–Ω–æ–≤—É
                merged["confidence"] = max(0.0, min(0.99, conf))
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è –∏–∑ extremum —Å–∏–≥–Ω–∞–ª–∞
                out.append(merged)
            else:
                out.append(e)

        return out

    # =========================================================================
    # –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ú–ï–¢–û–î–´
    # =========================================================================

    def advanced_quality_analysis(self) -> Dict[str, Any]:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ —Å –æ–±—â–∏–º PNL+ –∏ PNL-
        """
        logger.info("üîç –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞...")

        # ‚¨áÔ∏è –ì–ê–†–ê–ù–¢–ò–†–£–ï–ú —á—Ç–æ –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è dict
        default_result = {
            'methods_performance': [],
            'total_metrics': {},
            'best_method': {'method': 'N/A', 'success_rate': 0.0},
            'data_quality_issues': [],
            'timestamp': datetime.now().isoformat()
        }

        try:
            # ‚¨áÔ∏è –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ó–ê–ü–†–û–°: –¥–æ–±–∞–≤–ª—è–µ–º —Å—É–º–º—ã PNL+ –∏ PNL-
            query = """
                SELECT 
                    labeling_method,
                    reversal_label,
                    COUNT(*) as total_signals,
                    AVG(reversal_confidence) as avg_confidence,
                    AVG(price_change_after) as avg_profit,
                    SUM(price_change_after) as total_pnl,
                    -- ‚¨áÔ∏è –î–û–ë–ê–í–õ–Ø–ï–ú –°–£–ú–ú–´ –î–õ–Ø PNL+ –ò PNL-
                    SUM(CASE WHEN price_change_after > 0 THEN price_change_after ELSE 0 END) as total_positive_pnl,
                    SUM(CASE WHEN price_change_after < 0 THEN price_change_after ELSE 0 END) as total_negative_pnl,
                    SUM(CASE WHEN price_change_after >= :min_profit THEN 1 ELSE 0 END) as profitable_signals,
                    SUM(CASE WHEN price_change_after < -:min_profit THEN 1 ELSE 0 END) as loss_signals,
                    MIN(price_change_after) as min_profit,
                    MAX(price_change_after) as max_profit
                FROM labeling_results 
                WHERE symbol = :symbol
                GROUP BY labeling_method, reversal_label
            """

            df_quality = pd.read_sql_query(
                query,
                self.engine,
                params={
                    'min_profit': self.config.min_profit_target,
                    'symbol': self.config.symbol
                }
            )

            if df_quality.empty:
                logger.warning("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
                return default_result

            # === –í–ê–õ–ò–î–ê–¶–ò–Ø –ú–ï–¢–û–ö –° PNL=0 ===
            validation_warnings = []

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            all_labels_query = """
                SELECT 
                    extreme_timestamp,
                    reversal_label,
                    price_change_after,
                    labeling_method
                FROM labeling_results 
                WHERE symbol = :symbol
                ORDER BY extreme_timestamp
            """

            df_all_labels = pd.read_sql_query(
                all_labels_query,
                self.engine,
                params={'symbol': self.config.symbol}
            )

            validated_zero_pnl = 0
            invalid_zero_pnl = 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ BUY/SELL –º–µ—Ç–∫–∏
            for idx, row in df_all_labels[df_all_labels['reversal_label'].isin([1, 2])].iterrows():
                if row['price_change_after'] == 0.0:
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–µ—Ç–∫—É
                    next_labels = df_all_labels[df_all_labels['extreme_timestamp'] > row['extreme_timestamp']]

                    if next_labels.empty:
                        validation_warnings.append({
                            'timestamp': row['extreme_timestamp'],
                            'label': row['reversal_label'],
                            'method': row['labeling_method'],
                            'issue': '–ù–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏'
                        })
                        invalid_zero_pnl += 1
                    else:
                        next_label = next_labels.iloc[0]
                        if next_label['reversal_label'] != 0:
                            validation_warnings.append({
                                'timestamp': row['extreme_timestamp'],
                                'label': row['reversal_label'],
                                'method': row['labeling_method'],
                                'issue': f'–°–ª–µ–¥—É—é—â–∞—è –º–µ—Ç–∫–∞ –Ω–µ HOLD (reversal_label={next_label["reversal_label"]})'
                            })
                            invalid_zero_pnl += 1
                        else:
                            validated_zero_pnl += 1

            analysis = {
                'methods_performance': df_quality.to_dict('records'),
                'total_metrics': self._calculate_total_metrics(df_quality),
                'best_method': self._find_best_method(df_quality),
                'data_quality_issues': self._detect_data_quality_issues(),
                'validation': {
                    'validated_zero_pnl': validated_zero_pnl,
                    'invalid_zero_pnl': invalid_zero_pnl,
                    'warnings': validation_warnings
                },
                'timestamp': datetime.now().isoformat()
            }

            self._log_quality_analysis(analysis)

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if validated_zero_pnl > 0:
                print(f"\n‚úÖ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–æ –º–µ—Ç–æ–∫ —Å PnL=0: {validated_zero_pnl}")

            if validation_warnings:
                print(f"\n‚ö†Ô∏è  WARNINGS: –ù–∞–π–¥–µ–Ω–æ {len(validation_warnings)} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫ —Å PnL=0:")
                for w in validation_warnings[:10]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    label_str = 'BUY' if w['label'] == 1 else 'SELL'
                    print(f"   ‚Ä¢ ts={w['timestamp']} | {label_str} | {w['method']} | {w['issue']}")
                if len(validation_warnings) > 10:
                    print(f"   ... –∏ –µ—â–µ {len(validation_warnings) - 10} warnings")

            return analysis

        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {err}")
            return default_result

    def _calculate_total_metrics(self, df_quality: pd.DataFrame) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å PNL+ –∏ PNL-"""
        if df_quality.empty:
            return {}

        total_profitable = df_quality['profitable_signals'].sum()
        total_signals = df_quality['total_signals'].sum()
        overall_success = total_profitable / total_signals if total_signals > 0 else 0

        # ‚¨áÔ∏è –î–û–ë–ê–í–õ–Ø–ï–ú –†–ê–°–ß–ï–¢ PNL+ –ò PNL-
        total_positive_pnl = df_quality['total_positive_pnl'].sum()
        total_negative_pnl = df_quality['total_negative_pnl'].sum()
        total_pnl = df_quality['total_pnl'].sum()

        return {
            'total_signals': int(total_signals),
            'profitable_signals': int(total_profitable),
            'success_rate': float(overall_success),
            'avg_confidence': float(df_quality['avg_confidence'].mean()),
            'avg_profit': float(df_quality['avg_profit'].mean()),
            'total_pnl': float(total_pnl),
            # ‚¨áÔ∏è –ù–û–í–´–ï –ú–ï–¢–†–ò–ö–ò
            'total_positive_pnl': float(total_positive_pnl),
            'total_negative_pnl': float(total_negative_pnl),
            'pnl_ratio': abs(total_positive_pnl / total_negative_pnl) if total_negative_pnl != 0 else float('inf')
        }

    def _find_best_method(self, df_quality: pd.DataFrame) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –º–µ—Ç–æ–¥–∞ - –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ô –ú–ï–¢–û–î"""
        if df_quality.empty:
            return {'method': 'N/A', 'success_rate': 0.0}

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ SQL-–∑–∞–ø—Ä–æ—Å–∞
        if 'labeling_method' not in df_quality.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏–Ω–∞—á–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
            method_col = df_quality.columns[0] if len(df_quality.columns) > 0 else 'method'
        else:
            method_col = 'labeling_method'

        profitable_methods = df_quality[df_quality['profitable_signals'] > 0].copy()

        if profitable_methods.empty:
            return {'method': 'N/A', 'success_rate': 0.0}

        profitable_methods['success_rate'] = (
                profitable_methods['profitable_signals'] / profitable_methods['total_signals']
        )

        best_idx = profitable_methods['success_rate'].idxmax()
        best_method = profitable_methods.loc[best_idx]

        return {
            'method': best_method[method_col],
            'success_rate': float(best_method['success_rate']),
            'total_signals': int(best_method['total_signals']),
            'avg_profit': float(best_method['avg_profit'])
        }

    def configure_settings(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"""
        print("\n‚öôÔ∏è  –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í:")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"1. –ú–µ—Ç–æ–¥ —Ä–∞–∑–º–µ—Ç–∫–∏: {self.config.method}")
        print(f"2. Min profit target: {self.config.min_profit_target}")
        print(f"3. Hold bars: {self.config.hold_bars}")
        print("–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏")

    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É - –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"""
        stats = self.data_loader.get_data_stats()
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

    def _detect_data_quality_issues(self) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö - SQLAlchemy –≤–µ—Ä—Å–∏—è"""
        issues = []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.engine –≤–º–µ—Å—Ç–æ self.conn
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ç–æ–∫
            query_duplicates = """
                SELECT timestamp, COUNT(*) as cnt 
                FROM labeling_results 
                WHERE symbol = :symbol
                GROUP BY timestamp 
                HAVING COUNT(*) > 1
            """
            duplicates = pd.read_sql_query(
                query_duplicates,
                self.engine,
                params={'symbol': self.config.symbol}
            )
            if not duplicates.empty:
                issues.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ç–æ–∫: {len(duplicates)} —Å–ª—É—á–∞–µ–≤")

            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º...
            # ...

        except Exception as err:
            issues.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞: {err}")

        return issues

    def _log_quality_analysis(self, analysis: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤"""
        logger.info("\n" + "=" * 60)
        logger.info("üìä –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê")
        logger.info("=" * 60)

        total_metrics = analysis.get('total_metrics', {})
        best_method = analysis.get('best_method', {})
        issues = analysis.get('data_quality_issues', [])

        total_pnl_value = total_metrics.get('total_pnl', 0)
        total_positive_pnl = total_metrics.get('total_positive_pnl', 0)
        total_negative_pnl = total_metrics.get('total_negative_pnl', 0)
        pnl_ratio = total_metrics.get('pnl_ratio', 0)
        avg_profit = total_metrics.get('avg_profit', 0)

        # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        logger.info(f"üìà –û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {total_metrics.get('success_rate', 0):.1%}")
        logger.info(f"üí∞ –û–±—â–∏–π PNL+: {total_positive_pnl:+.3f} ({total_positive_pnl * 100:+.1f}%)")
        logger.info(f"üí∏ –û–±—â–∏–π PNL-: {total_negative_pnl:+.3f} ({total_negative_pnl * 100:+.1f}%)")
        logger.info(f"üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ PNL: {pnl_ratio:.1f}:1" if pnl_ratio != float(
            'inf') else "üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ PNL: ‚àû (–Ω–µ—Ç —É–±—ã—Ç–∫–æ–≤)")
        logger.info(f"üèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method.get('method', 'N/A')} ({best_method.get('success_rate', 0):.1%})")
        logger.info(f"üìä –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {total_metrics.get('total_signals', 0)}")
        logger.info(f"üíµ –°—Ä–µ–¥–Ω–∏–π PnL: {avg_profit:.4f} ({avg_profit:.1%})")
        logger.info(f"üéØ –°–æ–≤–æ–∫—É–ø–Ω—ã–π PnL: {total_pnl_value:+.3f} ({total_pnl_value * 100:+.1f}%)")

        if issues:
            logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue in issues:
                logger.warning(f"   ‚Ä¢ {issue}")
        else:
            logger.info("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

        # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ö–û–ù–°–û–õ–¨–ù–´–ô –í–´–í–û–î
        print(f"\nüéØ –ò–¢–û–ì–ò –ê–ù–ê–õ–ò–ó–ê:")
        print(f"   üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {total_metrics.get('success_rate', 0):.1%} —Å–¥–µ–ª–æ–∫")
        print(f"   üí∞ –ü—Ä–∏–±—ã–ª—å: {total_positive_pnl:+.3f} ({total_positive_pnl * 100:+.1f}%)")
        print(f"   üí∏ –£–±—ã—Ç–∫–∏: {total_negative_pnl:+.3f} ({total_negative_pnl * 100:+.1f}%)")
        print(f"   üìä –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å: {total_pnl_value:+.3f} ({total_pnl_value * 100:+.1f}%)")

        if pnl_ratio > 2:
            print(f"   ‚úÖ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (–æ—Ç–ª–∏—á–Ω–æ–µ)")
        elif pnl_ratio > 1:
            print(f"   ‚ö†Ô∏è  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ)")
        else:
            print(f"   ‚ùå –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (—Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è)")

    def export_feature_importance(self, *args, run_id: str | None = None, model_name: str = "unknown",
                                  top_n: int | None = None, **kwargs) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ SQLite (—Ç–∞–±–ª–∏—Ü–∞ training_feature_importance).
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫–∏–µ –≤—Ö–æ–¥—ã:
          ‚Ä¢ export_feature_importance(df, ...)            # df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['feature','importance'] –∏–ª–∏ –∏–Ω–¥–µ–∫—Å=feature
          ‚Ä¢ export_feature_importance(series, ...)        # pandas.Series: index=feature, values=importance
          ‚Ä¢ export_feature_importance(dict, ...)          # dict: feature -> importance
          ‚Ä¢ export_feature_importance(list_of_tuples, ...)# [(feature, importance), ...]
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
          run_id    ‚Äî –∫ –∫–∞–∫–æ–º—É —Å–Ω–∞–ø—à–æ—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏; –µ—Å–ª–∏ None, –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π READY –ø–æ symbol)
          model_name‚Äî –∏–º—è/–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'lgbm_v1')
          top_n     ‚Äî –ø—Ä–∏ –∑–∞–¥–∞–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ top-N –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: —á–∏—Å–ª–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫.
        """


        # 0) ensure DDL
        self._ensure_training_snapshot_tables()

        # 1) –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å run_id, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if run_id is None:
            with self.engine.begin() as conn:
                row = conn.execute(text("""
                    SELECT run_id
                      FROM training_dataset_meta
                     WHERE status='READY' AND symbol=:symbol
                  ORDER BY created_at DESC
                     LIMIT 1
                """), {"symbol": self.config.symbol}).mappings().first()
            if not row:
                raise RuntimeError("–ù–µ—Ç –≥–æ—Ç–æ–≤–æ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞ (status=READY). –£–∫–∞–∂–∏—Ç–µ run_id –≤—Ä—É—á–Ω—É—é.")
            run_id = row["run_id"]

        # 2) –∏–∑–≤–ª–µ—á—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ importance
        importance_obj = None
        if args:
            importance_obj = args[0]
        elif "importance" in kwargs:
            importance_obj = kwargs["importance"]
        elif "df" in kwargs:
            importance_obj = kwargs["df"]

        if importance_obj is None:
            raise ValueError("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á (df/series/dict/list).")

        # 3) –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–∞ -> DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['feature','importance']
        if isinstance(importance_obj, pd.DataFrame):
            df_imp = importance_obj.copy()
            # –¥–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω
            if "feature" not in df_imp.columns or "importance" not in df_imp.columns:
                if df_imp.shape[1] == 1:  # –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–µ–π, index = feature
                    df_imp = df_imp.reset_index()
                    df_imp.columns = ["feature", "importance"]
                elif df_imp.shape[1] >= 2:
                    # –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–∞–∫ feature/importance
                    cols = list(df_imp.columns)
                    df_imp = df_imp[[cols[0], cols[1]]].copy()
                    df_imp.columns = ["feature", "importance"]
        elif hasattr(importance_obj, "to_frame"):  # Series
            s = importance_obj
            df_imp = s.to_frame(name="importance").reset_index()
            if df_imp.columns[0] != "feature":
                df_imp.columns = ["feature", "importance"]
        elif isinstance(importance_obj, dict):
            df_imp = pd.DataFrame(list(importance_obj.items()), columns=["feature", "importance"])
        elif isinstance(importance_obj, (list, tuple)):
            df_imp = pd.DataFrame(importance_obj, columns=["feature", "importance"])
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≤—Ö–æ–¥–∞ –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á: {type(importance_obj)}")

        # —É–±—Ä–∞—Ç—å NaN/inf –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ —Ç–∏–ø—ã
        df_imp = df_imp.dropna(subset=["feature"]).copy()
        df_imp["importance"] = pd.to_numeric(df_imp["importance"], errors="coerce")
        df_imp = df_imp.replace([np.inf, -np.inf], np.nan).dropna(subset=["importance"])

        if df_imp.empty:
            raise ValueError("–¢–∞–±–ª–∏—Ü–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á –ø—É—Å—Ç–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")

        # –Ω–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Å—É–º–º—É=1 (–æ–±—â–µ–ø—Ä–∏–Ω—è—Ç–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏)
        ssum = float(df_imp["importance"].sum())
        if ssum > 0:
            df_imp["importance"] = df_imp["importance"] / ssum

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ç–æ–ø-N
        df_imp = df_imp.sort_values("importance", ascending=False).reset_index(drop=True)
        if top_n is not None and top_n > 0:
            df_imp = df_imp.head(int(top_n))

        df_imp["rank"] = np.arange(1, len(df_imp) + 1, dtype=int)
        created_at = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")
        df_imp["run_id"] = run_id
        df_imp["model_name"] = str(model_name)
        df_imp["created_at"] = created_at

        # –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
        out_cols = ["run_id", "model_name", "feature", "importance", "rank", "created_at"]
        df_out = df_imp[out_cols].copy()

        # 4) —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î (upsert –ø–æ PRIMARY KEY)
        inserted = 0
        with self.engine.begin() as conn:
            # —É–¥–∞–ª–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è (run_id, model_name), —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ç—å —Ä—è–¥—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º —ç–∫—Å–ø–æ—Ä—Ç–µ
            conn.execute(text("""
                DELETE FROM training_feature_importance
                 WHERE run_id=:rid AND model_name=:mname
            """), {"rid": run_id, "mname": model_name})

            df_out.to_sql("training_feature_importance", self.engine, if_exists="append", index=False)
            inserted = len(df_out)

        logger.info("‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: run_id=%s, model=%s, rows=%d", run_id, model_name, inserted)
        # –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ ‚Äî –ø–µ—á–∞—Ç—å top-10
        preview = df_out.sort_values("rank").head(10)
        print("\nüè∑Ô∏è TOP –≤–∞–∂–Ω–æ—Å—Ç–µ–π (–¥–æ 10 —Å—Ç—Ä–æ–∫):")
        for _, r in preview.iterrows():
            print(f"  {r['rank']:>2}. {r['feature']:<30} {r['importance']:.4f}")

        return inserted

    def cross_validation_split(self, n_splits: int = 5, test_size: float = 0.2) -> Dict[str, Any]:
        """
        –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤.
        –î–∞—Ç–∞—Å–µ—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏ —Å–Ω–∞–ø—à–æ—Ç–µ:
          - –º–µ—Ç–∫–∏ –∏–∑ labeling_results (reversal_label ‚àà {0,1,2})
          - —Ñ–∏—á–∏ –∏–∑ candles_* (load_indicators)
          - –∂—ë—Å—Ç–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (_validate_snapshot_frame)
        –†–∞–∑–±–∏–µ–Ω–∏–µ: time-based, –±–µ–∑ —É—Ç–µ—á–∫–∏ –±—É–¥—É—â–µ–≥–æ.
        """
        import pandas as pd
        from collections import Counter
        import traceback
        logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ {n_splits}-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏...")

        try:
            # 1) –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫–∞–∫ –≤ —Å–Ω–∞–ø—à–æ—Ç–µ
            raw_df, _meta = self._build_training_snapshot_dataframe()

            # 2) –ñ—ë—Å—Ç–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è/–æ—á–∏—Å—Ç–∫–∞
            df_clean, issues, nan_drop_rows, duplicates_count = self._validate_snapshot_frame(raw_df)
            if df_clean.empty:
                raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

            dataset = df_clean.reset_index(drop=True)

            # 3) –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            if "ts" in dataset.columns:
                try:
                    # –ü—Ä–æ–±—É–µ–º –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Binance)
                    dataset["datetime"] = pd.to_datetime(dataset["ts"], unit="ms", utc=True)
                except (pd.errors.OutOfBoundsDatetime, OverflowError):
                    try:
                        # –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ - –ø—Ä–æ–±—É–µ–º —Å–µ–∫—É–Ω–¥—ã
                        dataset["datetime"] = pd.to_datetime(dataset["ts"], unit="s", utc=True)
                    except (pd.errors.OutOfBoundsDatetime, OverflowError):
                        # –ö—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞
                        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ts, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é —à–∫–∞–ª—É")
                        dataset["datetime"] = pd.date_range(start="2020-01-01", periods=len(dataset), freq="5min",
                                                            tz="UTC")
            elif "datetime" not in dataset.columns:
                dataset["datetime"] = pd.date_range(start="2020-01-01", periods=len(dataset), freq="5min", tz="UTC")
            # 4) –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            dataset = dataset.sort_values("datetime").reset_index(drop=True)

            total_samples = len(dataset)
            if not (0 < test_size < 1):
                raise ValueError("test_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ (0,1)")
            test_samples = max(1, int(total_samples * test_size))

            # sanity-check –Ω–∞ –æ–±—ä—ë–º
            if total_samples < max(n_splits * 10, n_splits + test_samples):
                raise ValueError(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {total_samples} samples –¥–ª—è {n_splits} —Ñ–æ–ª–¥–æ–≤ (test_size={test_size})")

            logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è CV: {total_samples} —Å—Ç—Ä–æ–∫")

            # 5) –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã (blocked, –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è)
            splits = []
            # —à–∞–≥ —Å–º–µ—â–µ–Ω–∏—è –æ–∫–Ω–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –º—ã –ø–æ–ª—É—á–∏–ª–∏ n_splits —Ñ–æ–ª–¥–æ–≤
            step = (total_samples - test_samples) // n_splits
            step = max(step, 1)

            for i in range(n_splits):
                start_idx = i * step
                end_idx = start_idx + test_samples
                if end_idx > total_samples:
                    end_idx = total_samples
                    start_idx = max(0, end_idx - test_samples)

                test_indices = list(range(start_idx, end_idx))
                train_indices = list(range(0, start_idx)) + list(range(end_idx, total_samples))

                # –Ω–∞ –≤—Å—è–∫–∏–π: –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –∏ –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                if len(set(train_indices).intersection(test_indices)) != 0:
                    logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ñ–æ–ª–¥–µ {i + 1}")
                if len(set(train_indices + test_indices)) != total_samples:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ñ–æ–ª–¥–µ {i + 1}")

                split_info = {
                    "fold": i + 1,
                    "train_indices": train_indices,
                    "test_indices": test_indices,
                    "train_size": len(train_indices),
                    "test_size": len(test_indices),
                    "train_period": (
                        f"{dataset.iloc[train_indices[0]]['datetime']} ‚Üí {dataset.iloc[train_indices[-1]]['datetime']}"
                        if train_indices else "N/A"
                    ),
                    "test_period": (
                        f"{dataset.iloc[test_indices[0]]['datetime']} ‚Üí {dataset.iloc[test_indices[-1]]['datetime']}"
                        if test_indices else "N/A"
                    ),
                }
                splits.append(split_info)

            result = {
                "n_splits": n_splits,
                "test_size": test_size,
                "total_samples": total_samples,
                "splits": splits,
                "class_distribution": dict(Counter(dataset["reversal_label"])),
            }

            logger.info(f"‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {n_splits} —Ñ–æ–ª–¥–æ–≤, {total_samples} samples")
            print("\n‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –í—Å–µ–≥–æ samples: {total_samples}")
            print(f"üéØ –§–æ–ª–¥–æ–≤: {n_splits}")
            print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {result['class_distribution']}")
            return result

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {err}")
            logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            return {}

    def detect_label_leakage(self) -> Dict[str, Any]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ç–µ—á–∫–∏ –º–µ—Ç–æ–∫ ‚Äî —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        """
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –º–µ—Ç–æ–∫...")

        duplicate_pairs = []
        high_corr_features = []
        issues = []

        try:
            df_positives = self.data_loader.load_labeled_data()

            if df_positives.empty:
                return {
                    'leakage_detected': False,
                    'issues': ['–ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'],
                    'high_corr_features': [],
                    'duplicate_feature_pairs': [],
                    'total_positives': 0
                }

            logger.info(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(df_positives)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

            # –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
            print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–¢–û–ö:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ –º–µ—Ç–æ–∫: {len(df_positives)}")
            print(f"   ‚Ä¢ BUY (1): {len(df_positives[df_positives['reversal_label'] == 1])}")
            print(f"   ‚Ä¢ SELL (2): {len(df_positives[df_positives['reversal_label'] == 2])}")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º —Ä–∞–∑–º–µ—Ç–∫–∏
            if 'method' in df_positives.columns:
                method_stats = df_positives['method'].value_counts()
                print(f"   ‚Ä¢ –ü–æ –º–µ—Ç–æ–¥–∞–º: {method_stats.to_dict()}")

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫
            if 'pnl' in df_positives.columns:
                avg_pnl = df_positives['pnl'].mean()
                profitable = len(df_positives[df_positives['pnl'] > 0])
                success_rate = profitable / len(df_positives)
                print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} ({profitable}/{len(df_positives)})")
                print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {avg_pnl:.4f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ñ–∏—á —Å –º–µ—Ç–∫–æ–π
            available_features = [col for col in self.data_loader.feature_names if col in df_positives.columns]

            print(f"\nüîç –ê–ù–ê–õ–ò–ó –§–ò–ß ({len(available_features)} –¥–æ—Å—Ç—É–ø–Ω–æ):")

            feature_correlations = []
            for feature in available_features:
                if df_positives[feature].dtype not in [np.float64, np.int64]:
                    continue

                clean_data = df_positives[[feature, 'reversal_label']].dropna()
                if len(clean_data) < 10:  # –ú–∏–Ω–∏–º—É–º 10 samples
                    continue

                try:
                    correlation = clean_data[feature].corr(clean_data['reversal_label'])
                    if not np.isnan(correlation):
                        feature_correlations.append((feature, abs(correlation)))

                        if abs(correlation) > 0.8:
                            high_corr_features.append((feature, correlation))
                except:
                    continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏—á–∏ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            feature_correlations.sort(key=lambda x: x[1], reverse=True)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á
            if feature_correlations:
                print("   üèÜ –¢–æ–ø-5 —Ñ–∏—á –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –º–µ—Ç–∫–æ–π:")
                for feature, corr in feature_correlations[:5]:
                    leak_warning = " ‚ö†Ô∏è –£–¢–ï–ß–ö–ê!" if abs(corr) > 0.8 else ""
                    print(f"      ‚Ä¢ {feature}: {corr:.4f}{leak_warning}")

            if high_corr_features:
                issues.append(f"–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –º–µ—Ç–∫–æ–π: {len(high_corr_features)} —Ñ–∏—á")
                for feature, corr in high_corr_features:
                    logger.warning(f"   ‚ö†Ô∏è {feature}: {corr:.4f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ–∏—á–∏
            if len(available_features) > 1:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏
                    numeric_features = [f for f in available_features
                                        if df_positives[f].dtype in [np.float64, np.int64]]

                    if len(numeric_features) > 1:
                        corr_matrix = self.data_loader.safe_correlation_calculation(df_positives, numeric_features)

                        for i in range(len(corr_matrix.columns)):
                            for j in range(i + 1, len(corr_matrix.columns)):
                                if corr_matrix.iloc[i, j] > 0.95:
                                    duplicate_pairs.append((
                                        corr_matrix.columns[i],
                                        corr_matrix.columns[j],
                                        corr_matrix.iloc[i, j]
                                    ))

                        if duplicate_pairs:
                            issues.append(f"–î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ–∏—á–∏: {len(duplicate_pairs)} –ø–∞—Ä")
                            print(f"\nüîÅ –î–£–ë–õ–ò–†–£–Æ–©–ò–ï–°–Ø –§–ò–ß–ò:")
                            for pair in duplicate_pairs[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                                print(f"   ‚Ä¢ {pair[0]} ‚âà {pair[1]} (corr={pair[2]:.3f})")
                except Exception as corr_err:
                    issues.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {corr_err}")

            leakage_detected = len(high_corr_features) > 0

            result = {
                'leakage_detected': leakage_detected,
                'issues': issues,
                'high_corr_features': high_corr_features,
                'duplicate_feature_pairs': duplicate_pairs,
                'total_positives': len(df_positives),
                'available_features': len(available_features),
                'feature_correlations': feature_correlations[:10]  # –¢–æ–ø-10 —Ñ–∏—á
            }

            if leakage_detected:
                logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫!")
                print("\n‚ùå –í–´–í–û–î: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —É—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫!")
            else:
                logger.info("‚úÖ –£—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
                print("\n‚úÖ –í–´–í–û–î: –£—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")

            return result

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Ç–µ—á–∫–∏: {err}")
            return {
                'leakage_detected': False,
                'issues': [f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {err}'],
                'high_corr_features': [],
                'duplicate_feature_pairs': [],
                'total_positives': 0
            }

    # =========================================================================
    # –°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ú–ï–¢–û–î–´ –ò–ó –ò–°–•–û–î–ù–û–ì–û –ö–û–î–ê (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    # =========================================================================

    def save_to_db(self, results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –û–®–ò–ë–û–ö"""
        from sqlalchemy import text

        insert_sql = text("""
            INSERT OR REPLACE INTO labeling_results 
            (symbol, timestamp, timeframe, reversal_label, reversal_confidence,
             labeling_method, labeling_params, extreme_index, extreme_price, extreme_timestamp,
             confirmation_index, confirmation_timestamp, price_change_after, features_json,
             is_high_quality, created_at)
            VALUES (:symbol, :timestamp, :timeframe, :reversal_label, :reversal_confidence,
                    :labeling_method, :labeling_params, :extreme_index, :extreme_price, :extreme_timestamp,
                    :confirmation_index, :confirmation_timestamp, :price_change_after, :features_json,
                    :is_high_quality, CURRENT_TIMESTAMP)
        """)

        successful_saves = 0
        with self.engine.connect() as conn:
            for res in results:
                try:
                    #   –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —á—Ç–æ timestamp –±—É–¥—É—Ç INTEGER
                    timestamp = int(res.get('timestamp', 0))
                    extreme_timestamp = int(res.get('extreme_timestamp', 0))
                    confirmation_timestamp = int(res.get('confirmation_timestamp', 0))

                    #   –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                    if timestamp == 0 or extreme_timestamp == 0:
                        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp: {res.get('symbol')}")
                        continue

                    conn.execute(insert_sql, {
                        'symbol': res['symbol'],
                        'timestamp': timestamp,
                        'timeframe': res['timeframe'],
                        'reversal_label': res['reversal_label'],
                        'reversal_confidence': res.get('reversal_confidence', 1.0),
                        'labeling_method': res['labeling_method'],
                        'labeling_params': json.dumps(res.get('labeling_params', {})),
                        'extreme_index': res.get('extreme_index'),
                        'extreme_price': res.get('extreme_price'),
                        'extreme_timestamp': extreme_timestamp,
                        'confirmation_index': res.get('confirmation_index'),
                        'confirmation_timestamp': confirmation_timestamp,
                        'price_change_after': res.get('price_change_after'),
                        'features_json': res.get('features_json'),
                        'is_high_quality': res.get('is_high_quality', 1)
                    })
                    successful_saves += 1

                except Exception as row_err:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ {res.get('symbol', 'N/A')}: {row_err}")
                    continue

            conn.commit()

        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {successful_saves}/{len(results)} –º–µ—Ç–æ–∫ —á–µ—Ä–µ–∑ SQLAlchemy")
        if successful_saves < len(results):
            logger.warning(f"‚ö†Ô∏è –ù–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(results) - successful_saves} –º–µ—Ç–æ–∫ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")

    def manual_mode(self):
        """–†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        df = self.load_data()
        print("\n=== –†–£–ß–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê ===")
        print("–§–æ—Ä–º–∞—Ç: <–∏–Ω–¥–µ–∫—Å>,<BUY/SELL>. 'done' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")

        while True:
            user_input = input(">> ").strip()
            if user_input.lower() == 'done':
                break
            if ',' not in user_input:
                print("–§–æ—Ä–º–∞—Ç: –∏–Ω–¥–µ–∫—Å,—Ç–∏–ø")
                continue
            try:
                idx_str, typ = user_input.split(',')
                idx = int(idx_str.strip())
                typ = typ.strip().upper()
                if typ not in ['BUY', 'SELL']:
                    print("–¢–æ–ª—å–∫–æ BUY –∏–ª–∏ SELL")
                    continue
                if idx < 0 or idx >= len(df):
                    print(f"0 ‚â§ –∏–Ω–¥–µ–∫—Å < {len(df)}")
                    continue

                exit_idx = min(idx + self.config.hold_bars, len(df) - 1)
                pnl, is_profitable = self._calculate_pnl_to_index(df, idx, typ, exit_idx)
                print(f"PnL: {pnl:.4f} (—Ç—Ä–µ–±—É–µ—Ç—Å—è: ‚â•{self.config.min_profit_target:.4f})")

                if not is_profitable:
                    print(f"‚ö†Ô∏è  –ú–µ—Ç–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ profit target! –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å? (y/n)")
                    confirm = input(">> ").strip().lower()
                    if confirm != 'y':
                        continue

                label = 1 if typ == 'BUY' else 2

                row_dict = df.iloc[idx].to_dict()
                for k, v in row_dict.items():
                    if pd.isna(v):
                        row_dict[k] = None
                    elif isinstance(v, pd.Timestamp):
                        row_dict[k] = v.isoformat()
                    elif isinstance(v, (np.integer, np.int64)):
                        row_dict[k] = int(v)
                    elif isinstance(v, (np.floating, np.float64)):
                        row_dict[k] = float(v)
                    elif isinstance(v, str):
                        row_dict[k] = v

                timestamp = int(df['ts'].iloc[idx])

                result = {
                    'symbol': self.config.symbol,
                    'timestamp': timestamp,  # ‚Üê INTEGER
                    'timeframe': self.config.timeframe,
                    'reversal_label': label,
                    'reversal_confidence': 1.0,
                    'labeling_method': 'MANUAL',
                    'extreme_index': idx,
                    'extreme_price': df['close'].iloc[idx],
                    'extreme_timestamp': timestamp,  # ‚Üê INTEGER
                    'confirmation_index': idx,
                    'confirmation_timestamp': timestamp,  # ‚Üê INTEGER
                    'price_change_after': pnl,
                    'features_json': json.dumps(row_dict),
                    'is_high_quality': 1 if is_profitable else 0
                }
                self.save_to_db([result])
                self.labels.append({'index': idx, 'type': typ, 'pnl': pnl})
                print(f"‚úÖ –ú–µ—Ç–∫–∞ {typ} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (PnL: {pnl:.4f})")
            except Exception as err:
                print(f"–û—à–∏–±–∫–∞: {err}")

    def analyze_pnl_distribution(self, method: str = None):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL —á–µ—Ä–µ–∑ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
        """
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
        except ImportError:
            print("‚ùå –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ PnL —Ç—Ä–µ–±—É–µ—Ç—Å—è matplotlib –∏ seaborn")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install matplotlib seaborn")
            return

        logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL...")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            query = """
                SELECT labeling_method, price_change_after as pnl, reversal_label
                FROM labeling_results 
                WHERE symbol = :symbol
            """

            if method:
                query += " AND labeling_method = :method"
                params = {'symbol': self.config.symbol, 'method': method}
            else:
                params = {'symbol': self.config.symbol}

            df_pnl = pd.read_sql_query(query, self.engine, params=params)

            if df_pnl.empty:
                logger.warning("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PnL")
                return

            print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê PnL:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df_pnl)}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {df_pnl['pnl'].mean():.4f}")
            print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ PnL: {df_pnl['pnl'].median():.4f}")
            print(f"   ‚Ä¢ Std PnL: {df_pnl['pnl'].std():.4f}")
            print(f"   ‚Ä¢ Min PnL: {df_pnl['pnl'].min():.4f}")
            print(f"   ‚Ä¢ Max PnL: {df_pnl['pnl'].max():.4f}")
            print(f"   ‚Ä¢ PnL > 0: {(df_pnl['pnl'] > 0).sum()} ({(df_pnl['pnl'] > 0).mean():.1%})")
            print(f"   ‚Ä¢ PnL < 0: {(df_pnl['pnl'] < 0).sum()} ({(df_pnl['pnl'] < 0).mean():.1%})")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º
            if 'labeling_method' in df_pnl.columns:
                print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ú–ï–¢–û–î–ê–ú:")
                method_stats = df_pnl.groupby('labeling_method').agg({
                    'pnl': ['count', 'mean', 'std', 'min', 'max'],
                }).round(4)
                print(method_stats)

            # –°–æ–∑–¥–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
            plt.figure(figsize=(12, 8))

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ 1: –û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            plt.subplot(2, 2, 1)
            plt.hist(df_pnl['pnl'], bins=50, alpha=0.7, edgecolor='black')
            plt.axvline(df_pnl['pnl'].mean(), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df_pnl["pnl"].mean():.4f}')
            plt.axvline(0, color='green', linestyle='-', label='–ù—É–ª–µ–≤–∞—è –æ—Ç–º–µ—Ç–∫–∞')
            plt.xlabel('PnL')
            plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ PnL (–≤—Å–µ –º–µ—Ç–æ–¥—ã)')
            plt.legend()
            plt.grid(True, alpha=0.3)

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ 2: –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ PnL
            plt.subplot(2, 2, 2)
            positive_pnl = df_pnl[df_pnl['pnl'] > 0]['pnl']
            if len(positive_pnl) > 0:
                plt.hist(positive_pnl, bins=30, alpha=0.7, color='green', edgecolor='black')
                plt.axvline(positive_pnl.mean(), color='red', linestyle='--',
                            label=f'–°—Ä–µ–¥–Ω–µ–µ: {positive_pnl.mean():.4f}')
                plt.xlabel('PnL (> 0)')
                plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö PnL')
                plt.legend()
                plt.grid(True, alpha=0.3)

            # Boxplot –ø–æ –º–µ—Ç–æ–¥–∞–º
            plt.subplot(2, 2, 3)
            if 'labeling_method' in df_pnl.columns and df_pnl['labeling_method'].nunique() > 1:
                df_pnl.boxplot(column='pnl', by='labeling_method', ax=plt.gca())
                plt.title('PnL –ø–æ –º–µ—Ç–æ–¥–∞–º —Ä–∞–∑–º–µ—Ç–∫–∏')
                plt.suptitle('')  # –£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                plt.xticks(rotation=45)

            # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            plt.subplot(2, 2, 4)
            sorted_pnl = np.sort(df_pnl['pnl'])
            cumulative = np.arange(1, len(sorted_pnl) + 1) / len(sorted_pnl)
            plt.plot(sorted_pnl, cumulative, linewidth=2)
            plt.axvline(0, color='green', linestyle='--', alpha=0.7, label='–ù—É–ª–µ–≤–∞—è –æ—Ç–º–µ—Ç–∫–∞')
            plt.xlabel('PnL')
            plt.ylabel('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
            plt.title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ PnL')
            plt.grid(True, alpha=0.3)
            plt.legend()

            plt.tight_layout()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pnl_analysis_{self.config.symbol}_{timestamp}.png"
            plt.savefig(filename, dpi=150, bbox_inches='tight')
            plt.show()

            print(f"\nüíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")

            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
            self._analyze_pnl_outliers(df_pnl)

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ PnL: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    def _analyze_pnl_outliers(self, df_pnl: pd.DataFrame):
        """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ PnL"""
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í PnL:")

        Q1 = df_pnl['pnl'].quantile(0.25)
        Q3 = df_pnl['pnl'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df_pnl[(df_pnl['pnl'] < lower_bound) | (df_pnl['pnl'] > upper_bound)]

        print(f"   ‚Ä¢ –í—ã–±—Ä–æ—Å—ã (IQR –º–µ—Ç–æ–¥): {len(outliers)} —Å–∏–≥–Ω–∞–ª–æ–≤")
        print(f"   ‚Ä¢ –ì—Ä–∞–Ω–∏—Ü—ã –≤—ã–±—Ä–æ—Å–æ–≤: [{lower_bound:.4f}, {upper_bound:.4f}]")

        if not outliers.empty and 'labeling_method' in outliers.columns:
            print(f"   ‚Ä¢ –í—ã–±—Ä–æ—Å—ã –ø–æ –º–µ—Ç–æ–¥–∞–º:")
            outlier_methods = outliers['labeling_method'].value_counts()
            for method, count in outlier_methods.items():
                print(f"      - {method}: {count} –≤—ã–±—Ä–æ—Å–æ–≤")

        # –ê–Ω–∞–ª–∏–∑ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if len(df_pnl) > 0:
            extreme_positive = df_pnl.nlargest(5, 'pnl')
            extreme_negative = df_pnl.nsmallest(5, 'pnl')

            print(f"\nüìà –¢–û–ü-5 —Å–∞–º—ã—Ö –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:")
            for _, row in extreme_positive.iterrows():
                method = row.get('labeling_method', 'N/A')
                print(f"   ‚Ä¢ {method}: {row['pnl']:.4f}")

            print(f"\nüìâ –¢–û–ü-5 —Å–∞–º—ã—Ö —É–±—ã—Ç–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:")
            for _, row in extreme_negative.iterrows():
                method = row.get('labeling_method', 'N/A')
                print(f"   ‚Ä¢ {method}: {row['pnl']:.4f}")

    def quick_pnl_analysis(self):
        """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ PnL –±–µ–∑ –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        query = """
            SELECT labeling_method, price_change_after as pnl 
            FROM labeling_results 
            WHERE symbol = :symbol
        """

        try:
            df_pnl = pd.read_sql_query(query, self.engine, params={'symbol': self.config.symbol})

            if df_pnl.empty:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return

            print(f"\nüìä –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó PnL:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df_pnl)}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {df_pnl['pnl'].mean():.4f}")
            print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ PnL: {df_pnl['pnl'].median():.4f}")
            print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {(df_pnl['pnl'] > 0).mean():.1%}")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º
            if 'labeling_method' in df_pnl.columns:
                print(f"\nüìà –ü–û –ú–ï–¢–û–î–ê–ú:")
                for method in df_pnl['labeling_method'].unique():
                    method_data = df_pnl[df_pnl['labeling_method'] == method]
                    success_rate = (method_data['pnl'] > 0).mean()
                    avg_pnl = method_data['pnl'].mean()
                    print(
                        f"   ‚Ä¢ {method}: {len(method_data)} —Å–∏–≥–Ω–∞–ª–æ–≤, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%}, —Å—Ä–µ–¥–Ω–∏–π PnL: {avg_pnl:.4f}")

        except Exception as err:
            print(f"‚ùå –û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ PnL: {err}")

    def clear_labeling_table(self, confirmation_required: bool = True):
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–∞–∑–º–µ—Ç–∫–∞–º–∏
        """
        try:
            if confirmation_required:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π
                stats_query = """
                    SELECT labeling_method, COUNT(*) as count 
                    FROM labeling_results 
                    WHERE symbol = :symbol 
                    GROUP BY labeling_method
                """
                stats = pd.read_sql_query(stats_query, self.engine, params={'symbol': self.config.symbol})

                if stats.empty:
                    print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —É–∂–µ –ø—É—Å—Ç–∞")
                    return

                print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë–£–î–ï–¢ –£–î–ê–õ–ï–ù–û –í–°–ï–• –ú–ï–¢–û–ö!")
                print(f"üìä –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {self.config.symbol}:")
                total_count = 0
                for _, row in stats.iterrows():
                    print(f"   ‚Ä¢ {row['labeling_method']}: {row['count']} –º–µ—Ç–æ–∫")
                    total_count += row['count']
                print(f"   ‚Ä¢ –í–°–ï–ì–û: {total_count} –º–µ—Ç–æ–∫")

                confirm = input(f"\n‚ùì –í—ã —É–≤–µ—Ä–µ–Ω—ã? –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–æ–±—Ä–∞—Ç–∏–º–æ! (y/N): ").strip().lower()
                if confirm != 'y':
                    print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
                    return

            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É
            delete_query = "DELETE FROM labeling_results WHERE symbol = :symbol"
            with self.engine.connect() as conn:
                result = conn.execute(text(delete_query), {'symbol': self.config.symbol})
                conn.commit()

            deleted_count = result.rowcount
            logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {deleted_count} –º–µ—Ç–æ–∫ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.config.symbol}")
            print(f"‚úÖ –û—á–∏—â–µ–Ω–æ {deleted_count} –º–µ—Ç–æ–∫")

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    def clear_all_labeling_tables(self):
        """
        –û—á–∏—Å—Ç–∫–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ç–∫–∏ (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã)
        """
        try:
            print(f"\n‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –í–ù–ò–ú–ê–ù–ò–ï!")
            print(f"   –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–∞ –í–°–Ø —Ç–∞–±–ª–∏—Ü–∞ labeling_results!")
            print(f"   –í—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –±—É–¥—É—Ç –ø–æ—Ç–µ—Ä—è–Ω—ã!")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats_query = "SELECT COUNT(*) as total_count FROM labeling_results"
            total_count = pd.read_sql_query(stats_query, self.engine).iloc[0]['total_count']
            print(f"   –í—Å–µ–≥–æ –º–µ—Ç–æ–∫ –≤ –±–∞–∑–µ: {total_count}")

            confirm1 = input(f"\n‚ùì –í—ã –∞–±—Å–æ–ª—é—Ç–Ω–æ —É–≤–µ—Ä–µ–Ω—ã? (yes/NO): ").strip().lower()
            if confirm1 != 'yes':
                print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
                return

            # –û—á–∏—â–∞–µ–º –≤—Å—é —Ç–∞–±–ª–∏—Ü—É
            delete_query = "DELETE FROM labeling_results"
            with self.engine.connect() as conn:
                result = conn.execute(text(delete_query))
                conn.commit()

            deleted_count = result.rowcount
            logger.info(f"üßπ –û—á–∏—â–µ–Ω–∞ –≤—Å—è —Ç–∞–±–ª–∏—Ü–∞ labeling_results: {deleted_count} –º–µ—Ç–æ–∫")
            print(f"‚úÖ –û—á–∏—â–µ–Ω–∞ –≤—Å—è —Ç–∞–±–ª–∏—Ü–∞: {deleted_count} –º–µ—Ç–æ–∫")

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    # =========================================================================
    # –£–õ–£–ß–®–ï–ù–ù–´–ï –ú–ï–¢–û–î–´ –†–ê–ó–ú–ï–¢–ö–ò HOLD –° –ê–ù–ê–õ–ò–ó–û–ú –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–ò –ò –¢–†–ï–ù–î–ê
    # =========================================================================

    def _calculate_range_metrics(self, range_bars: pd.DataFrame) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –±–∞—Ä–æ–≤.

        Args:
            range_bars: DataFrame —Å –±–∞—Ä–∞–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'close', –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ 'atr')

        Returns:
            dict: {
                'atr_normalized': float,  # ATR –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫ —Ü–µ–Ω–µ
                'price_range': float,     # –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                'trend_strength': float,  # R¬≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (0-1)
                'volatility_level': str,  # 'LOW', 'MEDIUM', 'HIGH'
                'trend_level': str        # 'WEAK', 'MODERATE', 'STRONG'
            }
        """
        if range_bars.empty or len(range_bars) < 2:
            return {
                'atr_normalized': 0.0,
                'price_range': 0.0,
                'trend_strength': 0.0,
                'volatility_level': 'MEDIUM',
                'trend_level': 'WEAK'
            }

        close_prices = range_bars['close'].values

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫ ATR (–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ!)
        atr_normalized = 0.0
        atr_candidates = ['atr_14_normalized', 'atr', 'atr_14']  # ‚Üê –í–∞—à–∞ –ë–î –∏—Å–ø–æ–ª—å–∑—É–µ—Ç atr_14_normalized

        atr_col_found = None
        for col in atr_candidates:
            if col in range_bars.columns:
                atr_col_found = col
                break

        if atr_col_found:
            atr_values = range_bars[atr_col_found].dropna()
            if len(atr_values) > 0:
                atr_mean = float(atr_values.mean())

                if 'normalized' in atr_col_found.lower():
                    atr_normalized = atr_mean  # –£–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ
                else:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ —Ü–µ–Ω–µ
                    price_mean = float(range_bars['close'].mean())
                    if price_mean > 0:
                        atr_normalized = atr_mean / price_mean

        # Fallback: –µ—Å–ª–∏ ATR –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ—Ü–µ–Ω–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ (high - low)
        elif 'high' in range_bars.columns and 'low' in range_bars.columns:
            hl_range = (range_bars['high'] - range_bars['low']).mean()
            price_mean = float(range_bars['close'].mean())
            if price_mean > 0:
                atr_normalized = float(hl_range) / price_mean

        # –î–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã (–æ—Ç –ø–µ—Ä–≤–æ–≥–æ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±–∞—Ä–∞)
        price_start = float(close_prices[0])
        price_end = float(close_prices[-1])
        price_range = abs(price_end - price_start) / price_start if price_start > 0 else 0.0

        # –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é (R¬≤)
        trend_strength = 0.0
        if len(close_prices) >= 3:
            x = np.arange(len(close_prices))

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ scipy
            try:
                from scipy.stats import linregress
                slope, intercept, r_value, p_value, std_err = linregress(x, close_prices)
                trend_strength = float(r_value ** 2)  # R¬≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å
            except ImportError:
                # Fallback: –ø—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ numpy
                try:
                    correlation = np.corrcoef(x, close_prices)[0, 1]
                    if not np.isnan(correlation):
                        trend_strength = float(correlation ** 2)
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å trend_strength: {e}")
                    trend_strength = 0.0
            except Exception as e:
                # –õ—é–±—ã–µ –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
                logger.debug(f"–û—à–∏–±–∫–∞ linregress: {e}")
                try:
                    correlation = np.corrcoef(x, close_prices)[0, 1]
                    if not np.isnan(correlation):
                        trend_strength = float(correlation ** 2)
                except Exception:
                    trend_strength = 0.0

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        volatility_level = self._classify_volatility(atr_normalized)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç—Ä–µ–Ω–¥–∞
        trend_level = self._classify_trend(trend_strength)

        return {
            'atr_normalized': atr_normalized,
            'price_range': price_range,
            'trend_strength': trend_strength,
            'volatility_level': volatility_level,
            'trend_level': trend_level
        }

    def _classify_volatility(self, atr_normalized: float) -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ ATR.

        Args:
            atr_normalized: ATR –¥–µ–ª–µ–Ω–Ω—ã–π –Ω–∞ —Ü–µ–Ω—É (–∏–ª–∏ —É–∂–µ normalized –∏–∑ –ë–î)

        Returns:
            'LOW', 'MEDIUM', –∏–ª–∏ 'HIGH'
        """
        cfg = self.config

        # –ü–æ—Ä–æ–≥–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        low_threshold = getattr(cfg, 'atr_low_threshold', 0.005)  # 0.5%
        high_threshold = getattr(cfg, 'atr_high_threshold', 0.015)  # 1.5%

        if atr_normalized < low_threshold:
            return 'LOW'
        elif atr_normalized > high_threshold:
            return 'HIGH'
        else:
            return 'MEDIUM'

    def _classify_trend(self, trend_strength: float) -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ R¬≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏).

        Args:
            trend_strength: R¬≤ –æ—Ç –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (0. 0 - 1.0)

        Returns:
            'WEAK', 'MODERATE', –∏–ª–∏ 'STRONG'
        """
        cfg = self.config

        # –ü–æ—Ä–æ–≥–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        weak_threshold = getattr(cfg, 'trend_weak_threshold', 0.3)  # R¬≤ < 0.3
        strong_threshold = getattr(cfg, 'trend_strong_threshold', 0.7)  # R¬≤ > 0.7

        if trend_strength < weak_threshold:
            return 'WEAK'
        elif trend_strength > strong_threshold:
            return 'STRONG'
        else:
            return 'MODERATE'

    def _classify_range(self, metrics: dict, pnl: float) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ HOLD.

        Args:
            metrics: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∏–∑ _calculate_range_metrics()
            pnl: PnL –¥–∏–∞–ø–∞–∑–æ–Ω–∞

        Returns:
            str or None: —Ç–∏–ø –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∏–ª–∏ None (–Ω–µ —Ä–∞–∑–º–µ—á–∞–µ–º –∫–∞–∫ HOLD)
                - 'HOLD_AFTER_LOSS': —É–±—ã—Ç–æ—á–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
                - 'HOLD_CONSOLIDATION': —è–≤–Ω–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è
                - 'HOLD_WEAK_PROFIT': —Å–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å –±–µ–∑ —Ç—Ä–µ–Ω–¥–∞
                - 'HOLD_CHOPPY': —Ä–≤–∞–Ω—ã–π —Ä—ã–Ω–æ–∫ (–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å, —Å–ª–∞–±—ã–π —Ç—Ä–µ–Ω–¥)
        """
        cfg = self.config
        min_profit = getattr(cfg, 'min_profit_target', 0.001)
        weak_profit = getattr(cfg, 'weak_profit_threshold', 0.002)
        price_range_thr = getattr(cfg, 'price_range_threshold', 0.003)

        # –£–±—ã—Ç–æ—á–Ω—ã–π ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1
        if pnl < min_profit:
            return "HOLD_AFTER_LOSS"

        # –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2 (–Ω–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å + —Å–ª–∞–±—ã–π —Ç—Ä–µ–Ω–¥ + –º–∞–ª–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ)
        if (metrics['volatility_level'] == 'LOW' and
            metrics['trend_level'] == 'WEAK' and
            metrics['price_range'] < price_range_thr):
            return "HOLD_CONSOLIDATION"

        # –°–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å –±–µ–∑ —Ç—Ä–µ–Ω–¥–∞ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3
        if (pnl < weak_profit and
            metrics['trend_level'] == 'WEAK'):
            return "HOLD_WEAK_PROFIT"

        # –†–≤–∞–Ω—ã–π —Ä—ã–Ω–æ–∫ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4 (–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
        if (metrics['volatility_level'] == 'HIGH' and
            metrics['trend_level'] == 'WEAK'):
            return "HOLD_CHOPPY"

        # –ü—Ä–∏–±—ã–ª—å–Ω—ã–π —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥ ‚Äî –ù–ï —Ä–∞–∑–º–µ—á–∞–µ–º –∫–∞–∫ HOLD
        return None

    def _generate_holds_for_range(self, ts_list: List[int], range_type: str) -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HOLD-–º–µ—Ç–∫–∏ –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞.

        Args:
            ts_list: –°–ø–∏—Å–æ–∫ timestamp –∏–∑ candles_5m (int64!)
            range_type: –¢–∏–ø –¥–∏–∞–ø–∞–∑–æ–Ω–∞

        Returns:
            List[Dict]: HOLD-–º–µ—Ç–∫–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        """
        holds = []

        if not ts_list or len(ts_list) < 2:
            return holds

        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ HOLD-–º–µ—Ç–∫–∏
        def _create_hold(ts: int, method: str) -> Dict:
            # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: —è–≤–Ω–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ int (Python int64)
            ts_int64 = int(ts)

            return {
                "symbol": self.config.symbol,
                "timestamp": ts_int64,
                "timeframe": self.config.timeframe,
                "reversal_label": 0,
                "reversal_confidence": 1.0,
                "labeling_method": method,
                "labeling_params": None,
                "extreme_index": None,
                "extreme_price": None,
                "extreme_timestamp": ts_int64,  # ‚úÖ –î—É–±–ª–∏—Ä—É–µ–º –∫–∞–∫ int64
                "confirmation_index": None,
                "confirmation_timestamp": None,
                "price_change_after": 0.0,
                "features_json": None,
                "is_high_quality": 1,
            }

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        if range_type == "HOLD_CONSOLIDATION":
            step = getattr(self.config, 'consolidation_hold_every_n_bars', 3)
            for i in range(1, len(ts_list) - 1, step):
                holds.append(_create_hold(ts_list[i], "HOLD_CONSOLIDATION"))

        elif range_type == "HOLD_AFTER_LOSS":
            min_window = getattr(self.config, 'min_window_bars', 6)
            if len(ts_list) >= min_window:
                mid_idx = len(ts_list) // 2
                holds.append(_create_hold(ts_list[mid_idx], "HOLD_AFTER_LOSS_MID"))
            holds.append(_create_hold(ts_list[-1], "HOLD_AFTER_LOSS_END"))

        elif range_type == "HOLD_WEAK_PROFIT":
            if len(ts_list) >= 6:
                mid_idx = len(ts_list) // 2
                holds.append(_create_hold(ts_list[mid_idx], "HOLD_WEAK_PROFIT_MID"))
            holds.append(_create_hold(ts_list[-1], "HOLD_WEAK_PROFIT_END"))

        elif range_type == "HOLD_CHOPPY":
            holds.append(_create_hold(ts_list[-1], "HOLD_CHOPPY"))

        return holds

    def mark_unprofitable_ranges_as_negatives(self) -> dict:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ HOLD —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥–∞.

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –í–°–ï –¥–∏–∞–ø–∞–∑–æ–Ω—ã –º–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ —É–±—ã—Ç–æ—á–Ω—ã–µ):
        - –£–±—ã—Ç–æ—á–Ω—ã–µ (pnl < min_profit_target) ‚Üí HOLD –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
        - –ü—Ä–∏–±—ã–ª—å–Ω—ã–µ –Ω–æ —Å–ª–∞–±—ã–µ ‚Üí HOLD —Å —É—á–µ—Ç–æ–º —É—Å–ª–æ–≤–∏–π
        - –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ (–Ω–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å + —Å–ª–∞–±—ã–π —Ç—Ä–µ–Ω–¥) ‚Üí HOLD –ø–ª–æ—Ç–Ω–æ
        - –ü–µ—Ä–∏–æ–¥—ã –ø–æ—Å–ª–µ —Å–∏–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π ‚Üí HOLD (–æ–∂–∏–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)

        Returns:
            dict: {
                'updated_losers': int,
                'hold_after_loss': int,
                'hold_consolidation': int,
                'hold_weak_profit': int,
                'hold_choppy': int,
                'total_holds': int
            }
        """
        symbol = self.config.symbol
        tf = self.config.timeframe
        candles_table = f"candles_{tf}"
        thr = float(getattr(self.config, "min_profit_target", 0.001))

        logger.info(f"üîß –£–ª—É—á—à–µ–Ω–Ω–∞—è HOLD-—Ä–∞–∑–º–µ—Ç–∫–∞ —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥–∞ | {symbol} {tf}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'updated_losers': 0,
            'hold_after_loss': 0,
            'hold_consolidation': 0,
            'hold_weak_profit': 0,
            'hold_choppy': 0,
            'total_holds': 0,
            'ranges_analyzed': 0,
            'ranges_skipped': 0
        }

        inserted_rows = []

        with self.engine.begin() as conn:
            # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï BUY/SELL –º–µ—Ç–∫–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
            all_signals = pd.read_sql(
                text("""
                    SELECT reversal_label, extreme_timestamp, price_change_after
                      FROM labeling_results
                     WHERE symbol=:symbol AND timeframe=:tf
                       AND reversal_label IN (1,2)
                     ORDER BY extreme_timestamp
                """),
                conn,
                params={"symbol": symbol, "tf": tf},
            )

            if all_signals.empty or len(all_signals) < 2:
                logger.info(f"‚ÑπÔ∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤")
                return stats

            # 2) –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ HOLD (–¥–ª—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
            existing_holds = set()
            res = conn.execute(
                text("""
                    SELECT extreme_timestamp
                      FROM labeling_results
                     WHERE symbol=:symbol AND timeframe=:tf
                       AND reversal_label=0
                """),
                {"symbol": symbol, "tf": tf},
            )
            for r in res:
                existing_holds.add(int(r[0]))

            new_holds_in_batch = set()

            # 3) –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–∞—Ä—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
            for i in range(len(all_signals) - 1):
                current_sig = all_signals.iloc[i]
                next_sig = all_signals.iloc[i + 1]

                ts_cur = int(current_sig.extreme_timestamp)
                ts_next = int(next_sig.extreme_timestamp)
                pnl = float(current_sig.price_change_after) if pd.notna(current_sig.price_change_after) else 0.0

                # 3.1) –û–±–Ω–æ–≤–ª—è–µ–º —É–±—ã—Ç–æ—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã (pnl < threshold)
                if pnl < thr:
                    conn.execute(
                        text("""
                            UPDATE labeling_results
                               SET price_change_after = 0.0
                             WHERE symbol=:symbol AND timeframe=:tf
                               AND extreme_timestamp=:ts
                        """),
                        {"symbol": symbol, "tf": tf, "ts": ts_cur},
                    )
                    stats['updated_losers'] += 1

                # 3.2) ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ä—ã –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º –∫–æ–ª–æ–Ω–∫–∏ ATR
                range_bars_result = conn.execute(
                    text(f"""
                        SELECT ts, open, high, low, close, volume,
                               COALESCE(atr_14_normalized, NULL) as atr
                          FROM {candles_table}
                         WHERE symbol=:symbol
                           AND ts > :ts_cur
                           AND ts < :ts_next
                         ORDER BY ts ASC
                    """),
                    {"symbol": symbol, "ts_cur": ts_cur, "ts_next": ts_next},
                ).fetchall()

                if not range_bars_result:
                    stats['ranges_skipped'] += 1
                    continue

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
                range_bars = pd.DataFrame(
                    range_bars_result,
                    columns=['ts', 'open', 'high', 'low', 'close', 'volume', 'atr']
                )

                stats['ranges_analyzed'] += 1

                # 3.3) –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                try:
                    metrics = self._calculate_range_metrics(range_bars)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ {ts_cur}-{ts_next}: {e}")
                    continue

                # 3.4) –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
                try:
                    range_type = self._classify_range(metrics, pnl)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ {ts_cur}-{ts_next}: {e}")
                    continue

                if range_type is None:
                    # –ü—Ä–∏–±—ã–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥ ‚Äî –Ω–µ —Ä–∞–∑–º–µ—á–∞–µ–º
                    continue

                # 3.5) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º HOLD-–º–µ—Ç–∫–∏
                ts_list = range_bars['ts'].astype('int64').tolist()
                try:
                    holds = self._generate_holds_for_range(ts_list, range_type)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ HOLD –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ {ts_cur}-{ts_next}: {e}")
                    continue

                # 3. 6) –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ batch
                for hold in holds:
                    ts_hold = hold['extreme_timestamp']
                    if ts_hold not in existing_holds and ts_hold not in new_holds_in_batch:
                        inserted_rows.append(hold)
                        new_holds_in_batch.add(ts_hold)

                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º
                        method = hold['labeling_method']
                        if 'LOSS' in method:
                            stats['hold_after_loss'] += 1
                        elif 'CONSOLIDATION' in method:
                            stats['hold_consolidation'] += 1
                        elif 'WEAK_PROFIT' in method:
                            stats['hold_weak_profit'] += 1
                        elif 'CHOPPY' in method:
                            stats['hold_choppy'] += 1

            # 4) –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ HOLD
            if inserted_rows:
                pd.DataFrame(inserted_rows).to_sql(
                    "labeling_results", conn, if_exists="append", index=False
                )

        stats['total_holds'] = len(inserted_rows)

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(
            "‚úÖ HOLD —Ä–∞–∑–º–µ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:\n"
            f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['ranges_analyzed']}\n"
            f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –±–∞—Ä–æ–≤): {stats['ranges_skipped']}\n"
            f"   ‚Ä¢ –£–±—ã—Ç–æ—á–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {stats['hold_after_loss']} HOLD\n"
            f"   ‚Ä¢ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: {stats['hold_consolidation']} HOLD (–ø–ª–æ—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞)\n"
            f"   ‚Ä¢ –°–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å: {stats['hold_weak_profit']} HOLD\n"
            f"   ‚Ä¢ –†–≤–∞–Ω—ã–π —Ä—ã–Ω–æ–∫: {stats['hold_choppy']} HOLD\n"
            f"   ‚Ä¢ –ò–¢–û–ì–û: {stats['total_holds']} HOLD –º–µ—Ç–æ–∫\n"
            f"   ‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω–æ —É–±—ã—Ç–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {stats['updated_losers']}"
        )

        return stats

    def _calculate_unprofitable_hold_ranges(self, df: pd.DataFrame, signals: List[Dict]) -> List[Dict]:
        """
        –î–∏–∞–ø–∞–∑–æ–Ω—ã –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ BUY/SELL, –≥–¥–µ PnL < –ø–æ—Ä–æ–≥–∞ (–∏–ª–∏ <0 ‚Äî —Å–º. —É—Å–ª–æ–≤–∏–µ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã: {start_index, end_index, pnl, signal_type, range_length}
        """
        if not signals or len(signals) < 2:
            return []

        # ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã
        clean = []
        ts_to_idx = {int(ts): i for i, ts in enumerate(df["ts"].astype(int))}
        for s in signals:
            rl = int(s.get("reversal_label", -1))
            if rl not in (1, 2):
                continue
            idx = s.get("extreme_index")
            if idx is None or not isinstance(idx, (int, np.integer)):
                ts = s.get("extreme_timestamp")
                if ts is None or int(ts) not in ts_to_idx:
                    continue
                idx = ts_to_idx[int(ts)]
            if 0 <= idx < len(df):
                clean.append({"extreme_index": int(idx), "reversal_label": rl})

        if len(clean) < 2:
            return []

        clean.sort(key=lambda x: x["extreme_index"])
        hold_ranges = []

        for i in range(len(clean) - 1):
            start_idx = clean[i]["extreme_index"]
            raw_end = clean[i + 1]["extreme_index"]
            # —Å—á–∏—Ç–∞–µ–º PnL –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ë–ê–†–ê –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —Å–∏–≥–Ω–∞–ª–æ–º
            end_idx = max(start_idx + 1, raw_end - 1)
            if end_idx <= start_idx or end_idx >= len(df):
                continue

            signal_type = 'BUY' if clean[i]["reversal_label"] == 1 else 'SELL'
            pnl, _ = self._calculate_pnl_to_index(df, start_idx, signal_type, end_idx)

            # –≤—ã–±–µ—Ä–∏ –æ–¥–Ω–æ –∏–∑ —É—Å–ª–æ–≤–∏–π:
            # if pnl < 0:                                  # —Å—Ç—Ä–æ–≥–æ —É–±—ã—Ç–æ—á–Ω—ã–µ
            if pnl < float(self.config.min_profit_target):  # ¬´–Ω–µ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ –ø–æ –ø–æ—Ä–æ–≥—É¬ª
                hold_ranges.append({
                    "start_index": start_idx,
                    "end_index": raw_end,  # –≤–∞–∂–Ω–æ: –ø–æ–ª—É–∏–Ω—Ç–µ—Ä–≤–∞–ª [start, raw_end)
                    "pnl": float(pnl),
                    "signal_type": signal_type,
                    "range_length": raw_end - start_idx
                })

        self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(hold_ranges)} —É–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –º–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏")
        return hold_ranges

    def _run_auto_with_method(self, strategy_func, method_name: str, df: pd.DataFrame = None):
        """–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Å —Ä–∞—Å—á–µ—Ç–æ–º PnL –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏"""
        if df is None:
            df = self.load_data()
        signals = strategy_func(df)
        results = []

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏
        sorted_signals = sorted(signals, key=lambda x: x['index'])

        for i, sig in enumerate(sorted_signals):
            idx = sig['index']

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –≤—ã—Ö–æ–¥–∞: –ª–∏–±–æ —Å–ª–µ–¥—É—é—â–∞—è –º–µ—Ç–∫–∞, –ª–∏–±–æ hold_bars
            if i + 1 < len(sorted_signals):
                # –ï—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª - –≤—ã—Ö–æ–¥–∏–º –Ω–∞ –±–∞—Ä–µ –ø–µ—Ä–µ–¥ –Ω–∏–º
                next_idx = sorted_signals[i + 1]['index']
                exit_idx = max(idx + 1, next_idx - 1)  # –º–∏–Ω–∏–º—É–º +1 –±–∞—Ä –æ—Ç –≤—Ö–æ–¥–∞
            else:
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª - –∏—Å–ø–æ–ª—å–∑—É–µ–º hold_bars
                exit_idx = idx + self.config.hold_bars

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü
            if exit_idx >= len(df):
                exit_idx = len(df) - 1

            if exit_idx <= idx:  # –∑–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                continue

            # –†–∞—Å—á–µ—Ç PnL –¥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤—ã—Ö–æ–¥–∞
            pnl, is_profitable = self._calculate_pnl_to_index(df, idx, sig['type'], exit_idx)

            row_dict = df.iloc[idx].to_dict()
            for k, v in row_dict.items():
                if pd.isna(v):
                    row_dict[k] = None
                elif isinstance(v, pd.Timestamp):
                    row_dict[k] = v.isoformat()
                elif isinstance(v, (np.integer, np.int64)):
                    row_dict[k] = int(v)
                elif isinstance(v, (np.floating, np.float64)):
                    row_dict[k] = float(v)
                elif isinstance(v, str):
                    row_dict[k] = v

            timestamp = int(df['ts'].iloc[idx])
            extreme_timestamp = int(sig.get('extreme_timestamp', timestamp))
            confirmation_timestamp = int(sig.get('confirmation_timestamp', timestamp))

            # —Å–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
            labeling_params = {}
            for k, v in self.config.__dict__.items():
                if k in ['db_engine', 'tool']:  # –∏—Å–∫–ª—é—á–∞–µ–º –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã
                    continue
                try:
                    # –ü—Ä–æ–±—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                    json.dumps(v)
                    labeling_params[k] = v
                except (TypeError, ValueError):
                    # –ï—Å–ª–∏ –Ω–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    labeling_params[k] = str(v)

            results.append({
                'symbol': self.config.symbol,
                'timestamp': timestamp,
                'timeframe': self.config.timeframe,
                'reversal_label': 1 if sig['type'] == 'BUY' else 2,
                'reversal_confidence': sig['confidence'],
                'labeling_method': method_name,
                'labeling_params': json.dumps(labeling_params),
                'extreme_index': idx,
                'extreme_price': float(df['close'].iloc[idx]),
                'extreme_timestamp': extreme_timestamp,
                'confirmation_index': sig['confirmation_index'],
                'confirmation_timestamp': confirmation_timestamp,
                'price_change_after': pnl,
                'features_json': json.dumps(row_dict),
                'is_high_quality': 1 if is_profitable else 0
            })

        if results:
            self.save_to_db(results)
            profitable_count = sum(1 for r in results if r['is_high_quality'] == 1)
            total_count = len(results)
            print(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_count} —Å–∏–≥–Ω–∞–ª–æ–≤ ({profitable_count} –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö, {total_count - profitable_count} —É–±—ã—Ç–æ—á–Ω—ã—Ö)")
            return len(results)
        else:
            print(f"‚ùå –°–∏–≥–Ω–∞–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return 0

    def _run_auto(self, strategy_func, method_name: str):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return self._run_auto_with_method(strategy_func, method_name)

    def enhanced_main_menu(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        while True:
            print("\n" + "=" * 60)
            print("           ML LABELING TOOL v3 ‚Äî –†–ê–°–®–ò–†–ï–ù–ù–û–ï –ú–ï–ù–Æ")
            print("=" * 60)

            stats = self.data_loader.get_data_stats()
            print(
                f"üìä –°–∏–º–≤–æ–ª: {self.config.symbol} | –°–≤–µ—á–µ–π: {stats.get('total_candles', 'N/A')} | –ú–µ—Ç–æ–∫: {stats.get('total_labels', 'N/A')}")

            print("\nüéØ –†–µ–∂–∏–º—ã —Ä–∞–∑–º–µ—Ç–∫–∏:")
            print("[0]  BinSeg - –±—ã—Å—Ç—Ä–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–∞–≤—Ç–æ)")
            print("[1] PELT Offline - –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä penalty (–∞–≤—Ç–æ)")
            print("[2] CUSUM (–∞–≤—Ç–æ)")
            print("[3] Min/Max —ç–∫—Å—Ç—Ä–µ–º—É–º—ã (–∞–≤—Ç–æ)")
            print("[4] CUSUM + Min/Max –≥–∏–±—Ä–∏–¥ (–∞–≤—Ç–æ)")
            print("[5] –†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞")

            print("\nüìà –ê–Ω–∞–ª–∏–∑ –∏ –∫–∞—á–µ—Å—Ç–≤–æ:")
            print("[6] –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞")
            print("[7] –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –º–µ—Ç–æ–∫")
            print("[8] –≠–∫—Å–ø–æ—Ä—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á")
            print("[9] –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è")
            print("[16] –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL")
            print("[17] –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ PnL")

            print("\n‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏:")
            print("[11] –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            print("[13] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫")
            print("[14] –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            print("[18] –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç–æ–∫ (—Ç–µ–∫—É—â–∏–π —Å–∏–º–≤–æ–ª)")
            print("[19] –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç–æ–∫ –ø–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º")
            print("[20] –ü–æ–º–µ—Ç–∫–∞ —É–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∫–∞–∫ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤")
            print("[21] –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫")


            print("\n[22] –í—ã—Ö–æ–¥")

            choice = input("\n–í–∞—à –≤—ã–±–æ—Ä: ").strip()
            if choice == '0':
                self._run_auto(self._binseg_reversals, "BINSEG")
            elif choice == '1':
                self._run_auto(self._pelt_offline_reversals, "PELT_OFFLINE")
            elif choice == '2':
                self._run_auto(self._cusum_reversals, "CUSUM")
            elif choice == '3':
                self._run_auto(self._extremum_reversals, "EXTREMUM")
            elif choice == '4':
                self._run_auto(self._cusum_extremum_hybrid, "CUSUM_EXTREMUM")
            elif choice == '5':
                self.manual_mode()
            elif choice == '6':
                self.advanced_quality_analysis()
            elif choice == '7':
                self.detect_label_leakage()
            elif choice == '8':
                try:
                    print("\nüè∑Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á –≤ –ë–î (training_feature_importance)")
                    rid = input("run_id (–ø—É—Å—Ç–æ = –ø–æ—Å–ª–µ–¥–Ω–∏–π READY): ").strip() or None
                    model_name = input("–ò–º—è –º–æ–¥–µ–ª–∏ [unknown]: ").strip() or "unknown"
                    top_n_inp = input("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å top-N (–ø—É—Å—Ç–æ = –≤—Å–µ): ").strip()
                    top_n = int(top_n_inp) if top_n_inp else None

                    saved = self.export_feature_importance(run_id=rid, model_name=model_name, top_n=top_n)
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏: {saved}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '9':
                try:
                    n_splits = int(input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ [5]: ") or "5")
                    result = self.cross_validation_split(n_splits=n_splits)
                    if result:
                        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {n_splits} —Ñ–æ–ª–¥–æ–≤ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '11':
                self.configure_settings()
            elif choice == '13':
                self.show_stats()
            elif choice == '14':
                try:
                    print("\nüì¶ –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å–Ω–∞–ø—à–æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ë–î")
                    print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –≥–æ—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ –∏–∑ labeling_results")
                    print(f"   –ú–µ—Ç–æ–¥ —Ä–∞–∑–º–µ—Ç–∫–∏: {self.config.method}")
                    print(f"   –°–∏–º–≤–æ–ª: {self.config.symbol}, —Ç–∞–π–º—Ñ—Ä–µ–π–º: {self.config.timeframe}")

                    confirm = input("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ —Å–Ω–∞–ø—à–æ—Ç–∞? (y/N): ").strip().lower()
                    if confirm != 'y':
                        print("‚ùå –°–æ–∑–¥–∞–Ω–∏–µ —Å–Ω–∞–ø—à–æ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–æ")
                        continue

                    run_id = self.create_training_snapshot()
                    print(f"‚úÖ –°–Ω–∞–ø—à–æ—Ç —Å–æ–∑–¥–∞–Ω: run_id={run_id}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {err}")
            elif choice == '16':
                try:
                    method = input("–ú–µ—Ç–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ [–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ]: ").strip()
                    if method:
                        self.analyze_pnl_distribution(method=method)
                    else:
                        self.analyze_pnl_distribution()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '17':
                try:
                    self.quick_pnl_analysis()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '18':
                try:
                    self.clear_labeling_table()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '19':
                try:
                    self.clear_all_labeling_tables()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '20':
                    try:
                        stats = self.mark_unprofitable_ranges_as_negatives()
                        print(f"‚úÖ HOLD —Ä–∞–∑–º–µ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
                        print(f"   ‚Ä¢ –£–±—ã—Ç–æ—á–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {stats.get('hold_after_loss', 0)} HOLD")
                        print(f"   ‚Ä¢ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: {stats.get('hold_consolidation', 0)} HOLD")
                        print(f"   ‚Ä¢ –°–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å: {stats.get('hold_weak_profit', 0)} HOLD")
                        print(f"   ‚Ä¢ –†–≤–∞–Ω—ã–π —Ä—ã–Ω–æ–∫: {stats.get('hold_choppy', 0)} HOLD")
                        print(f"   ‚Ä¢ –ò–¢–û–ì–û: {stats.get('total_holds', 0)} HOLD –º–µ—Ç–æ–∫")
                    except Exception as err:
                        print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '21':
                try:
                    count = self.merge_conflicting_labels()
                    print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫: {count}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '22':
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

    def _expand_hold_ranges(self, labels_df: pd.DataFrame, all_timestamps: set) -> dict:
        """
        –í–ï–†–°–ò–Ø –ë–ï–ó –†–ê–°–®–ò–†–ï–ù–ò–Ø HOLD.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º ts, –±–µ–∑ –∑–∞–ª–∏–≤–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–∫–æ–≤.
        –ù–∞–¥—ë–∂–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö –Ω–∞ –æ–¥–∏–Ω ts.
        """

        if labels_df is None or labels_df.empty:
            return {}

        # 1) —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º
        labels_df = labels_df.loc[:, ~labels_df.columns.duplicated()].copy()

        # 2) –Ω–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ 'ts' –∏ 'reversal_label' (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
        cols_lower = {c.lower(): c for c in labels_df.columns}
        ts_col = cols_lower.get('ts')
        lab_col = cols_lower.get('reversal_label')

        if ts_col is None or lab_col is None:
            self.logger.warning("–ù–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ 'ts' –∏/–∏–ª–∏ 'reversal_label' –≤ labels_df")
            return {}

        df = labels_df[[ts_col, lab_col]].copy()

        # 3) –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤—ã–º —Ç–∏–ø–∞–º; –≤—ã–±—Ä–æ—Å–∏—Ç—å NaN/–Ω–µ—á–∏—Å–ª–æ–≤—ã–µ
        df[ts_col] = pd.to_numeric(df[ts_col], errors='coerce')
        df[lab_col] = pd.to_numeric(df[lab_col], errors='coerce')
        df = df.dropna(subset=[ts_col, lab_col])

        # 4) –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ int (–ø–æ—Å–ª–µ dropna –±–µ–∑–æ–ø–∞—Å–Ω–æ)
        df[ts_col] = df[ts_col].astype(np.int64)
        df[lab_col] = df[lab_col].astype(np.int64)

        # 5) –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ ts, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ all_timestamps (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã)
        if all_timestamps:
            # all_timestamps –º–æ–∂–µ—Ç –±—ã—Ç—å set(int) ‚Äî –æ–∫
            df = df[df[ts_col].isin(all_timestamps)]

        if df.empty:
            return {}

        # 6) –µ—Å–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É ts –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–∫ ‚Äî –±–µ—Ä—ë–º –ü–û–°–õ–ï–î–ù–Æ–Æ (–ø–æ –ø–æ—Ä—è–¥–∫—É —Å—Ç—Ä–æ–∫)
        # (–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–≤—É—é ‚Äî –∑–∞–º–µ–Ω–∏—Ç–µ .last() –Ω–∞ .first())
        df = df.groupby(ts_col, as_index=False)[lab_col].last()

        # 7) –≤–µ—Ä–Ω—É—Ç—å –∫–∞–∫ dict{ts:int -> label:int}
        return dict(zip(df[ts_col].tolist(), df[lab_col].tolist()))

    def _build_training_snapshot_dataframe(self):
        import pandas as pd
        from sqlalchemy import text
        market_df = self.data_loader.load_indicators()
        if market_df is None or market_df.empty:
            raise RuntimeError("–ü—É—Å—Ç—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        if "ts" not in market_df.columns:
            raise RuntimeError("–í market_df –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'ts'")

        # ‚úÖ –î–û–ë–ê–í–¨–¢–ï DEBUG:
        logger.info(f"üîç market_df –∫–æ–ª–æ–Ω–æ–∫: {len(market_df.columns)}")
        logger.info(f"üîç –ü–µ—Ä–≤—ã–µ 10 –∫–æ–ª–æ–Ω–æ–∫: {list(market_df.columns[:10])}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ BASE_FEATURE_NAMES
        missing_features = [f for f in BASE_FEATURE_NAMES if f not in market_df.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        else:
            logger.info("‚úÖ –í—Å–µ BASE_FEATURE_NAMES –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ market_df")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å
        sample_row = market_df.iloc[0][BASE_FEATURE_NAMES]
        null_count = sample_row.isna().sum()
        logger.info(f"üîç –í –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ NULL –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {null_count}/{len(BASE_FEATURE_NAMES)}")
        if null_count > 0:
            null_features = sample_row[sample_row.isna()].index.tolist()
            logger.warning(f"‚ö†Ô∏è NULL –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ: {null_features}")

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(market_df)} —Å–≤–µ—á–µ–π —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
        with self.engine.begin() as conn:
            rows = conn.execute(text("""
                SELECT extreme_timestamp AS ts, reversal_label
                FROM labeling_results
                WHERE symbol=:symbol AND timeframe=:timeframe AND reversal_label IN (0,1,2)
                ORDER BY extreme_timestamp
            """), {"symbol": self.config.symbol, "timeframe": self.config.timeframe}).fetchall()
        if not rows:
            raise RuntimeError("–ù–µ—Ç –º–µ—Ç–æ–∫ –≤ labeling_results")
        labels_df = pd.DataFrame(rows, columns=["ts", "reversal_label"])
        valid_mask = labels_df['reversal_label'].isin([0, 1, 2])
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {invalid_count} –º–µ—Ç–æ–∫ —Å –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ - —É–¥–∞–ª—è–µ–º")
            labels_df = labels_df[valid_mask]

        logger.info(
            f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(labels_df)} –º–µ—Ç–æ–∫ "
            f"(NO_SIGNAL/HOLD: {(labels_df['reversal_label'] == 0).sum()}, "
            f"BUY: {(labels_df['reversal_label'] == 1).sum()}, "
            f"SELL: {(labels_df['reversal_label'] == 2).sum()})")

        all_timestamps = set(market_df['ts'].values)
        expanded_labels = self._expand_hold_ranges(labels_df, all_timestamps)
        hold_count = sum(1 for l in expanded_labels.values() if l == 0)
        logger.info(f"‚úÖ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è HOLD (label=0): {hold_count}")

        # üîπ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞—Ä—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –º–µ—Ç–∫–∞ –≤ labeling_results
        mapped = market_df["ts"].map(expanded_labels)
        labeled_mask = mapped.notna()
        labeled_df = market_df.loc[labeled_mask].copy()

        # ‚úÖ –î–û–ë–ê–í–¨–¢–ï DEBUG:
        logger.info(f"üîç labeled_df: {len(labeled_df)} —Å—Ç—Ä–æ–∫, {len(labeled_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        sample_labeled = labeled_df.iloc[0][BASE_FEATURE_NAMES]
        null_labeled = sample_labeled.isna().sum()
        logger.info(f"üîç –í –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ labeled_df NULL –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {null_labeled}/{len(BASE_FEATURE_NAMES)}")

        if null_labeled > 0:
            null_features_labeled = sample_labeled[sample_labeled.isna()].index.tolist()
            logger.warning(f"‚ö†Ô∏è NULL –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ labeled_df: {null_features_labeled}")

        if labeled_df.empty:
            raise RuntimeError("–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è expanded_labels –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –±–∞—Ä–æ–≤")

        labeled_df["reversal_label"] = mapped[labeled_mask].astype(int)

        # ‚úÖ –î–û–ë–ê–í–¨–¢–ï: –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NULL –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        critical_features = ['cmo_14', 'adx_14', 'bb_position', 'atr_14_normalized', 'trend_acceleration_ema7']
        before_null_drop = len(labeled_df)
        labeled_df = labeled_df.dropna(subset=critical_features)
        after_null_drop = len(labeled_df)

        dropped_count = before_null_drop - after_null_drop
        if dropped_count > 0:
            logger.info(f"üîß –£–¥–∞–ª–µ–Ω–æ {dropped_count} –º–µ—Ç–æ–∫ —Å NULL –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ (–ø—Ä–æ–≥—Ä–µ–≤)")

        if labeled_df.empty:
            raise RuntimeError("‚ùå –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NULL –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")

        # sanity-check: —Ç–æ–ª—å–∫–æ 0/1/2
        invalid_mask = ~labeled_df["reversal_label"].isin([0, 1, 2])
        if invalid_mask.any():
            invalid_count = invalid_mask.sum()
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {invalid_count} –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞ ‚Äî —É–¥–∞–ª—è–µ–º –∏—Ö")
            labeled_df = labeled_df[~invalid_mask]

        if labeled_df.empty:
            raise RuntimeError("–í—Å–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –±–∞—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –∫–∞–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ (–ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ 0/1/2)")

        class_counts_before = labeled_df["reversal_label"].value_counts().to_dict()
        logger.info(
            "   –î–û downsample (–ø–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –±–∞—Ä–∞–º): HOLD=%s, BUY=%s, SELL=%s",
            class_counts_before.get(0, 0),
            class_counts_before.get(1, 0),
            class_counts_before.get(2, 0),
        )

        # üîπ HOLD = —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ HOLD-–º–µ—Ç–∫–∏ –∏–∑ labeling_results
        hold_df = labeled_df[labeled_df["reversal_label"] == 0]
        signals_df = labeled_df[labeled_df["reversal_label"] != 0]

        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ HOLD
        n_hold_max = 10_000
        if len(hold_df) > 0:
            hold_sample = hold_df.sample(
                n=min(n_hold_max, len(hold_df)),
                random_state=42,
            )
        else:
            hold_sample = hold_df

        logger.info(
            "‚úÖ Downsample HOLD: %s ‚Üí %s",
            len(hold_df),
            len(hold_sample),
        )

        # –∏—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏
        dataset_df = pd.concat([hold_sample, signals_df], ignore_index=True).sort_values("ts").reset_index(drop=True)

        # ‚úÖ –î–û–ë–ê–í–¨–¢–ï DEBUG:
        logger.info(f"üîç dataset_df –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫: {len(dataset_df)} —Å—Ç—Ä–æ–∫, {len(dataset_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_present = [f for f in BASE_FEATURE_NAMES if f in dataset_df.columns]
        logger.info(f"üîç –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ dataset_df: {len(features_present)}/{len(BASE_FEATURE_NAMES)}")

        sample_dataset = dataset_df.iloc[0][features_present]
        null_dataset = sample_dataset.isna().sum()
        logger.info(f"üîç NULL –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ dataset_df: {null_dataset}/{len(features_present)}")


        class_counts_after = dataset_df["reversal_label"].value_counts().to_dict()
        total = len(dataset_df)

        #  –Ø–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –º–µ—Ç–∫–∏
        invalid_labels = set(class_counts_after.keys()) - {0, 1, 2}
        if invalid_labels:
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –º–µ—Ç–∫–∏: {invalid_labels}")
            dataset_df = dataset_df[dataset_df["reversal_label"].isin([0, 1, 2])]
            class_counts_after = dataset_df["reversal_label"].value_counts().to_dict()
            total = len(dataset_df)

        logger.info(
            "‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: %s (NO_SIGNAL=%s, BUY=%s, SELL=%s)",
            total,
            class_counts_after.get(0, 0),
            class_counts_after.get(1, 0),
            class_counts_after.get(2, 0),
        )
        # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –≤–µ—Å–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è 0/1/2
        if class_counts_after:
            max_count = max(class_counts_after.values())
            weights_map = {}
            for label in [0, 1, 2]:
                count = class_counts_after.get(label, 0)
                if count > 0:
                    weights_map[label] = max_count / count
                else:
                    weights_map[label] = 0.0  # –ö–ª–∞—Å—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    logger.warning(f"‚ö†Ô∏è –ö–ª–∞—Å—Å {label} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")

            dataset_df["sample_weight"] = dataset_df["reversal_label"].map(weights_map)
        else:
            logger.error("‚ùå class_counts_after –ø—É—Å—Ç!")
            dataset_df["sample_weight"] = 1.0
        # —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è + –≤—Å–µ —Ñ–∏—á–∏ –∏–∑ BASE_FEATURE_NAMES
        feature_columns = list(BASE_FEATURE_NAMES)
        allowed_columns = ["ts","datetime",
            "reversal_label","sample_weight",
            *feature_columns,]
        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–µ –ø–∞–¥–∞–µ–º –µ—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ—Ç
        dataset_df = dataset_df[[c for c in allowed_columns if c in dataset_df.columns]]
        dataset_df['symbol'] = self.config.symbol
        dataset_df['run_id'] = None
        dataset_df['timeframe'] = self.config.timeframe
        dataset_df['created_at'] = None
        meta_info = {
            "class_dist": {
                "no_signal": int(class_counts_after.get(0, 0)),
                "buy": int(class_counts_after.get(1, 0)),
                "sell": int(class_counts_after.get(2, 0)),
                "total": total,
            },
            "buffer_bars": getattr(self.config, "buffer_bars", None),
            "seed": getattr(self.config, "seed", None),
            "config_json": {
                "method": self.config.method,
                "timeframe": self.config.timeframe,
                "symbol": self.config.symbol,
            },
            "issues": {},
        }

        return dataset_df, meta_info

    def create_training_snapshot(self, run_id: str | None = None) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ë–î-—Å–Ω–∞–ø—à–æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
          - –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü snapshot (DDL)
          - –ø–∏—à–µ—Ç meta(status=CREATING)
          - —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ–∑–∏—Ç–∏–≤—ã/–Ω–µ–≥–∞—Ç–∏–≤—ã, anti_trade_mask, sample_weight)
          - –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ
          - –ø–∏—à–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ training_dataset –∏ –∞–≥—Ä–µ–≥–∞—Ç—ã –≤ training_dataset_meta
          - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç meta(status=READY)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç run_id.
        """

        self._ensure_training_snapshot_tables()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ run_id
        created_at = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")
        if not run_id:
            payload = {
                "symbol": self.config.symbol,
                "timeframe": self.config.timeframe,
                "method": self.config.method,
            }
            sh = hashlib.sha1(json.dumps(payload, sort_keys=True).encode("utf-8")).hexdigest()[:8]
            stamp = datetime.now(UTC).strftime("%Y%m%d_%H%M")
            run_id = f"{self.config.symbol.replace('/', '_')}_{self.config.timeframe}_{stamp}_{sh}"
        # META: status=CREATING
        meta_defaults = {
            "run_id": run_id,
            "status": "CREATING",
            "error_msg": None,
            "symbol": self.config.symbol,
            "timeframe": self.config.timeframe,
            "created_at": created_at
        }
        with self.engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO training_dataset_meta(run_id,status,error_msg,symbol,timeframe,created_at)
                VALUES (:run_id,:status,:error_msg,:symbol,:timeframe,:created_at)
                ON CONFLICT(run_id) DO UPDATE SET status=excluded.status, error_msg=NULL
            """), meta_defaults)

        try:
            # 1) –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä)
            dataset_df, meta_info = self._build_training_snapshot_dataframe()

            # 2) –í–∞–ª–∏–¥–∞—Ü–∏—è/–æ—á–∏—Å—Ç–∫–∞
            try:
                df_clean, issues, nan_drop_rows, duplicates_count = self._validate_snapshot_frame(dataset_df)
            except Exception as e:
                logger.info("Snapshot validation failed: %s", e)
                raise

            # 3) –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –∏ –º–µ—Ç—Ä–∏–∫–∏
            range_start_ts = int(df_clean["ts"].min()) if len(df_clean) else None
            range_end_ts = int(df_clean["ts"].max()) if len(df_clean) else None

            if "anti_trade_mask" in df_clean.columns and len(df_clean) > 0:
                try:
                    issues["anti_trade_coverage"] = float(df_clean["anti_trade_mask"].mean())
                except Exception:
                    pass

            rows_total = int(len(df_clean))
            pos_count = int((df_clean["reversal_label"].isin([1, 2])).sum()) if rows_total else 0
            hold_bars = int((df_clean["reversal_label"] == 0).sum()) if rows_total else 0
            neg_count = 0  # –ø–æ –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–µ –Ω–µ–≥–∞—Ç–∏–≤—ã –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º

            # 4) –ó–∞–ø–∏—Å—å —Å—Ç—Ä–æ–∫ snapshot
            df_clean = df_clean.copy()
            df_clean["run_id"] = run_id
            df_clean["symbol"] = self.config.symbol
            df_clean["timeframe"] = self.config.timeframe
            df_clean["created_at"] = created_at


            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ BASE_FEATURE_NAMES
            required_columns = (
                    [
                        'run_id',
                        'symbol',
                        'timeframe',
                        'ts',
                        'datetime',
                        'reversal_label',
                        'sample_weight',
                    ]
                    + BASE_FEATURE_NAMES
                    + ['created_at']
            )

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ DDL
            final_columns = [col for col in required_columns if col in df_clean.columns]
            df_for_db = df_clean[final_columns].copy()

            # batch insert
            df_for_db.to_sql("training_dataset", self.engine, if_exists="append", index=False)

            # 5) –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ META (READY) ‚Äî –±–µ–∑ featureset/source_hash/feature_names
            meta_payload = {
                "run_id": run_id,
                "status": "READY",
                "error_msg": None,
                "rows_total": rows_total,
                "pos_count": pos_count,
                "neg_count": neg_count,
                "hold_bars": hold_bars,
                "class_dist_json": json.dumps(meta_info.get("class_dist", {})),
                "buffer_bars": meta_info.get("buffer_bars"),
                "seed": meta_info.get("seed"),
                "labeling_method": self.config.method,
                "config_json": json.dumps(meta_info.get("config_json", {})),
                "nan_drop_rows": int(nan_drop_rows),
                "issues_json": json.dumps(issues, ensure_ascii=False),
                "range_start_ts": range_start_ts,
                "range_end_ts": range_end_ts,
            }
            with self.engine.begin() as conn:
                conn.execute(text("""
                    UPDATE training_dataset_meta
                       SET status=:status,
                           error_msg=:error_msg,
                           rows_total=:rows_total,
                           pos_count=:pos_count,
                           neg_count=:neg_count,
                           class_dist_json=:class_dist_json,
                           hold_bars=:hold_bars,
                           buffer_bars=:buffer_bars,
                           seed=:seed,
                           labeling_method=:labeling_method,
                           config_json=:config_json,
                           nan_drop_rows=:nan_drop_rows,
                           issues_json=:issues_json,
                           range_start_ts=:range_start_ts,
                           range_end_ts=:range_end_ts
                     WHERE run_id=:run_id
                """), meta_payload)

            logger.info(
                "‚úÖ Snapshot READY | run_id=%s | rows=%s | pos=%s | neg=%s | anti_trade=%s | range=[%s..%s] | nan_drop=%s",
                run_id, rows_total, pos_count, neg_count,
                (f"{issues.get('anti_trade_coverage'):.4f}" if isinstance(issues.get("anti_trade_coverage"),
                                                                          float) else "N/A"),
                range_start_ts, range_end_ts, nan_drop_rows
            )
            return run_id

        except Exception as e:
            err = str(e)
            logger.info("‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–Ω–∞–ø—à–æ—Ç–∞: %s", err)
            with self.engine.begin() as conn:
                conn.execute(text("""
                    UPDATE training_dataset_meta
                       SET status='FAILED', error_msg=:err
                     WHERE run_id=:run_id
                """), {"run_id": run_id, "err": err})
            raise

    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        try:
            if hasattr(self, "data_loader") and hasattr(self.data_loader, "db_engine"):
                self.data_loader.db_engine.dispose()
                logger.info("üîå SQLAlchemy engine –∑–∞–∫—Ä—ã—Ç.")
            if hasattr(self, "engine"):
                self.engine.dispose()
                logger.info("üßπ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ.")
        except Exception as err:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {err}")


# === –ó–ê–ü–£–°–ö ===
if __name__ == '__main__':
    tool = None
    try:
        # –ò–°–ü–û–õ–¨–ó–£–ï–ú –ù–ê–°–¢–†–û–ô–ö–ò –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ –ò–ó LabelingConfig
        # –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        config = LabelingConfig(
            symbol="ETHUSDT",
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ defaults –∫–ª–∞—Å—Å–∞ LabelingConfig
        )

        tool = AdvancedLabelingTool(config)
        tool.enhanced_main_menu()

    except KeyboardInterrupt:
        print("\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(0)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

        logger.error(f"–î–µ—Ç–∞–ª–∏: {traceback.format_exc()}")
        sys.exit(1)
    finally:
        if tool is not None:
            tool.close()