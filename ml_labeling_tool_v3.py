"""
ml_labeling_tool_v3.py
 –≤–µ—Ä—Å–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏
"""
import numpy as np
from sqlalchemy import text
import pandas as pd
import sys
import hashlib, json
from dataclasses import dataclass
from iqts_standards import Direction
from typing import Tuple, List, Dict,  Any
from datetime import datetime, UTC
import warnings
import logging
import traceback

# --- DDL –¥–ª—è —Ç–∞–±–ª–∏—Ü —Å–Ω–∞–ø—à–æ—Ç–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ---
CREATE_TRAINING_DATASET_SQL = """
CREATE TABLE IF NOT EXISTS training_dataset (
    run_id           TEXT    NOT NULL,
    symbol           TEXT    NOT NULL,
    timeframe        TEXT    NOT NULL,
    ts               INTEGER NOT NULL,
    datetime         TEXT    NOT NULL,
    reversal_label   INTEGER NOT NULL,
    sample_weight    REAL    NOT NULL,
    cmo_14           REAL,
    volume           REAL,
    trend_acceleration_ema7     REAL,
    regime_volatility           REAL,
    bb_width                    REAL,
    adx_14                      REAL,
    plus_di_14                  REAL,
    minus_di_14                 REAL,
    atr_14_normalized           REAL,
    volume_ratio_ema3           REAL,
    candle_relative_body        REAL,
    upper_shadow_ratio          REAL,
    lower_shadow_ratio          REAL,
    price_vs_vwap               REAL,
    bb_position                 REAL,
    cusum_1m_recent             INTEGER,
    cusum_1m_quality_score      REAL,
    cusum_1m_trend_aligned      INTEGER,
    cusum_1m_price_move         REAL,
    is_trend_pattern_1m         INTEGER,
    body_to_range_ratio_1m      REAL,
    close_position_in_range_1m  REAL,
    created_at       TEXT    NOT NULL,
    PRIMARY KEY (run_id, symbol, ts)
);
"""
CREATE_TRAINING_DATASET_INDEXES_SQL = [
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_id      ON training_dataset(run_id)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_symbol      ON training_dataset(symbol)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_sym     ON training_dataset(run_id, symbol)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_sym_ts      ON training_dataset(symbol, ts)",
    "CREATE INDEX IF NOT EXISTS idx_training_dataset_run_ts      ON training_dataset(run_id, ts)"
]

CREATE_TRAINING_DATASET_META_SQL = """
CREATE TABLE IF NOT EXISTS training_dataset_meta (
    run_id             TEXT PRIMARY KEY,
    status             TEXT NOT NULL,               -- CREATING|READY|FAILED
    error_msg          TEXT,
    symbol             TEXT NOT NULL,
    timeframe          TEXT NOT NULL,
    range_start_ts     INTEGER,
    range_end_ts       INTEGER,
    rows_total         INTEGER,
    pos_count          INTEGER,
    neg_count          INTEGER,
    class_dist_json    TEXT,
    hold_bars          INTEGER,
    buffer_bars        INTEGER,
    seed               INTEGER,
    labeling_method    TEXT,
    feature_names_json TEXT,
    featureset_version TEXT,
    features_hash      TEXT,
    config_json        TEXT,
    nan_drop_rows      INTEGER,
    issues_json        TEXT,
    source_hashes_json TEXT,
    created_at         TEXT NOT NULL
);
"""
CREATE_TRAINING_FEATURE_IMPORTANCE_SQL = """
CREATE TABLE IF NOT EXISTS training_feature_importance (
    run_id      TEXT    NOT NULL,
    model_name  TEXT    NOT NULL,
    feature     TEXT    NOT NULL,
    importance  REAL    NOT NULL,
    rank        INTEGER NOT NULL,
    created_at  TEXT    NOT NULL,
    PRIMARY KEY (run_id, model_name, feature)
);
"""
CREATE_TRAINING_FEATURE_IMPORTANCE_INDEXES_SQL = [
    "CREATE INDEX IF NOT EXISTS idx_tfi_run_id       ON training_feature_importance(run_id)",
    "CREATE INDEX IF NOT EXISTS idx_tfi_model_run    ON training_feature_importance(model_name, run_id)"
]
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

import ruptures as rpt
RUPTURES_AVAILABLE = True

from sqlalchemy.engine import Engine, create_engine
from pathlib import Path

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"

@dataclass
class LabelingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å SQLAlchemy"""
    db_engine: Engine = None
    symbol: str = "ETHUSDT"
    timeframe: str = "5m"

    # PELT Online
    pelt_window: int = 1000
    pelt_pen: float = 1
    pelt_min_size: int = 10
    pelt_confirm_bar: int = 3

    # CUSUM
    cusum_z_threshold: float = 3 # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π |cusum_zscore|,
    cusum_conf_threshold: float = 1  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è cusum_conf
    hold_bars: int = 3               # –ë–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–æ–ª–¥
    buffer_bars: int = 5             # –ú–µ–Ω—å—à–µ –±—É—Ñ–µ—Ä–Ω—ã—Ö –±–∞—Ä–æ–≤

    # Extremum (min/max)
    extremum_confirm_bar: int = 2
    extremum_window: int = 8
    min_signal_distance: int = 3
    # –§–∏–ª—å—Ç—Ä—ã
    method: str = "CUSUM_EXTREMUM"
    # PnL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    fee_percent: float = 0.0004
    min_profit_target: float = 0.001
    tool: Any = None

    def __post_init__(self):
        if self.db_engine is None:
            self.db_engine = create_engine(MARKET_DB_DSN)


class DataLoader:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å SQLAlchemy"""

    def __init__(self, db_engine: Engine = None, symbol: str = "ETHUSDT",
                 timeframe: str = "5m", config: LabelingConfig = None):
        self.db_engine = db_engine or create_engine(MARKET_DB_DSN)
        self.symbol = symbol
        self.timeframe = timeframe
        self.config = config  # ‚Üê –°–æ—Ö—Ä–∞–Ω—è–µ–º config
        self.feature_names = []
        self._initialize_features()

    def _initialize_features(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∏—á –¥–ª—è ML"""
        self.feature_names = [
            # 'trend_momentum_z',  # ‚Üê –£–î–ê–õ–ò–¢–¨ (–¥—É–±–ª–∏–∫–∞—Ç bb_position)
             'cmo_14',
            # 'macd_histogram',
            'volume',
            'trend_acceleration_ema7', 'regime_volatility', 'bb_width', 'adx_14',
            'plus_di_14',
            'minus_di_14', 'atr_14_normalized', 'volume_ratio_ema3',
            'candle_relative_body', 'upper_shadow_ratio', 'lower_shadow_ratio',
            'price_vs_vwap', 'bb_position', 'cusum_1m_recent', 'cusum_1m_quality_score',
            'cusum_1m_trend_aligned', 'cusum_1m_price_move', 'is_trend_pattern_1m',
            'body_to_range_ratio_1m', 'close_position_in_range_1m',
        ]

    def connect(self) -> Engine:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î —á–µ—Ä–µ–∑ SQLAlchemy"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            with self.db_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy: {self.db_engine.url}")
            return self.db_engine
        except Exception as err:
            raise ConnectionError(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {err}")

    def disconnect(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è - –¥–ª—è SQLAlchemy –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"""
        # SQLAlchemy –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏
        logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –±—É–¥–µ—Ç –∑–∞–∫—Ä—ã—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")

    def load_indicators(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ SQLAlchemy"""
        query = """
            SELECT * FROM candles_5m 
            WHERE symbol = ? 
            ORDER BY ts
        """
        try:
            df = pd.read_sql_query(query, self.db_engine, params=(self.symbol,))

            if df.empty:
                raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            if 'ts' in df.columns and 'datetime' not in df.columns:
                df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

            required_cols = ["cusum", "cusum_state", "cusum_zscore", "cusum_conf"]
            missing = [c for c in required_cols if c not in df.columns]
            if missing:
                logger.info("CUSUM 5m: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ %s (—Ç–∞–±–ª–∏—Ü–∞ candles_5m).", missing)
            else:
                # guard: –µ—Å–ª–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Å—Ç–∞—Ç—É—Å—ã
                df["cusum_state"] = df["cusum_state"].replace({"BUY": 1, "SELL": -1, "HOLD": 0})
                # –∏—Ç–æ–≥: Int64-–∫–∞—Ç–µ–≥–æ—Ä–∏—è 1/2/0, NaN -> 0
                df["cusum_state"] = pd.to_numeric(df["cusum_state"], errors="coerce").fillna(0).astype("Int64")

                # alias –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤/–º–æ–¥–µ–ª–µ–π (—Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏)
                df["cusum_signal"] = df["cusum_state"]

                # —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ –∫ float
                df["cusum"] = pd.to_numeric(df["cusum"], errors="coerce")
                df["cusum_zscore"] = pd.to_numeric(df["cusum_zscore"], errors="coerce")
                df["cusum_conf"] = pd.to_numeric(df["cusum_conf"], errors="coerce")

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
            return df

        except Exception as err:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {err}")

    def validate_data_quality(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, Any]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        if df.empty:
            return False, {"error": "DataFrame –ø—É—Å—Ç"}

        required_columns = ['open', 'high', 'low', 'close', 'volume', 'ts']
        checks = {
            'min_rows': len(df) > 100,
            'required_columns': all(col in df.columns for col in required_columns),
            'no_nan_close': df['close'].isna().sum() == 0,
            'high_low_valid': (df['high'] >= df['low']).all(),
            'high_open_valid': (df['high'] >= df['open']).all(),
            'high_close_valid': (df['high'] >= df['close']).all(),
            'low_open_valid': (df['low'] <= df['open']).all(),
            'low_close_valid': (df['low'] <= df['close']).all(),
            'positive_volume': (df['volume'] >= 0).all(),
            'timestamp_monotonic': df['ts'].is_monotonic_increasing
        }

        quality_ok = all(checks.values())
        diagnostics = {
            'passed_checks': sum(checks.values()),
            'total_checks': len(checks),
            'failed_checks': [k for k, v in checks.items() if not v],
            'basic_stats': {
                'period': f"{df['datetime'].min()} to {df['datetime'].max()}" if 'datetime' in df.columns else 'N/A',
                'total_rows': len(df),
                'nan_count': df.isna().sum().sum()
            }
        }
        # DataLoader.validate_data_quality(df) ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∏ CUSUM 5m
        for col in ["cusum", "cusum_zscore", "cusum_conf", "cusum_state"]:
            if col not in df.columns:
                logger.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ %s (CUSUM 5m).", col)

        if "cusum_state" in df.columns:
            bad_mask = ~df["cusum_state"].isin([0, 1, -1]) & df["cusum_state"].notna()
            if bad_mask.any():
                logger.warning("–ù–µ–≤–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è cusum_state –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–∞ %d —Å—Ç—Ä–æ–∫–∞—Ö.", int(bad_mask.sum()))

        if not quality_ok:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {diagnostics['failed_checks']}")

        return quality_ok, diagnostics

    def load_labeled_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ü–†–û–í–ï–†–ö–ê–ú–ò"""
        query = """
            SELECT 
                lr.symbol,
                lr.timestamp as extreme_timestamp,
                lr.reversal_label,
                lr.reversal_confidence as confidence,
                lr.labeling_method as method,
                lr.price_change_after as pnl,
                lr.features_json,
                lr.created_at,
                c.* 
            FROM labeling_results lr
            LEFT JOIN candles_5m c ON lr.timestamp = c.ts AND lr.symbol = c.symbol
            WHERE lr.symbol = ?
            ORDER BY lr.timestamp
        """
        try:
            positives = pd.read_sql_query(
                query,
                self.db_engine,
                params=(self.symbol,)
            )

            if positives.empty:
                logger.warning(f"‚ùå –ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")
                return pd.DataFrame()

            # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            if 'extreme_timestamp' in positives.columns:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç–∞–Ω—É—Ç NaN
                positives['extreme_timestamp'] = pd.to_numeric(positives['extreme_timestamp'], errors='coerce')
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp
                initial_count = len(positives)
                positives = positives.dropna(subset=['extreme_timestamp'])
                removed_count = initial_count - len(positives)
                if removed_count > 0:
                    logger.warning(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç—Ä–æ–∫ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp")
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'extreme_timestamp' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(positives)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è {self.symbol}")
            if 'reversal_label' in positives.columns:
                logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {positives['reversal_label'].value_counts().to_dict()}")
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'reversal_label' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            return positives

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {err}")
            return pd.DataFrame()

    def safe_correlation_calculation(self, df, columns):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏"""
        try:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ columns —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ df
            available_columns = [col for col in columns if col in df.columns]

            if len(available_columns) < 2:
                return pd.DataFrame()

            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            numeric_cols = df[available_columns].select_dtypes(include=[np.number])

            if len(numeric_cols.columns) < 2:
                return pd.DataFrame()

            # –£–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
            numeric_cols = numeric_cols.loc[:, numeric_cols.std() > 0]

            if len(numeric_cols.columns) < 2:
                return pd.DataFrame()

            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            numeric_cols = numeric_cols.dropna()

            if len(numeric_cols) < 2:
                return pd.DataFrame()

            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", RuntimeWarning)
                corr_matrix = numeric_cols.corr().abs()

            return corr_matrix

        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {err}")
            return pd.DataFrame()

    def get_data_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ SQLAlchemy"""
        stats = {
            'symbol': self.symbol,
            'total_candles': 0,
            'period': 'N/A',
            'total_labels': 0,
            'buy_labels': 0,
            'sell_labels': 0,
            'avg_confidence': 0.0
        }

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.db_engine.connect() –≤–º–µ—Å—Ç–æ self.conn
            with self.db_engine.connect() as conn:
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–≤–µ—á–µ–π
                candles_result = conn.execute(
                    text("SELECT COUNT(*), MIN(ts), MAX(ts) FROM candles_5m WHERE symbol = :symbol"),
                    {'symbol': self.symbol}
                ).fetchone()

                if candles_result:
                    total_candles, min_ts, max_ts = candles_result
                    stats['total_candles'] = total_candles or 0
                    if min_ts and max_ts:
                        stats['period'] = f"{pd.to_datetime(min_ts, unit='ms')} to {pd.to_datetime(max_ts, unit='ms')}"

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫
                labels_result = conn.execute(
                    text("""
                        SELECT COUNT(*), AVG(reversal_confidence),
                               SUM(CASE WHEN reversal_label = 1 THEN 1 ELSE 0 END),
                               SUM(CASE WHEN reversal_label = 2 THEN 1 ELSE 0 END)
                        FROM labeling_results WHERE symbol = :symbol
                    """),
                    {'symbol': self.symbol}
                ).fetchone()

                if labels_result:
                    total_labels, avg_conf, buy_labels, sell_labels = labels_result
                    stats['total_labels'] = total_labels or 0
                    stats['buy_labels'] = buy_labels or 0
                    stats['sell_labels'] = sell_labels or 0
                    stats['avg_confidence'] = float(avg_conf) if avg_conf else 0.0

        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {err}")

        return stats

class AdvancedLabelingTool:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ —Å SQLAlchemy
    """

    def _ensure_training_snapshot_tables(self) -> None:
        """
        –°–æ–∑–¥–∞—ë—Ç (–µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç) —Ç–∞–±–ª–∏—Ü—ã training_dataset –∏ training_dataset_meta + –∏–Ω–¥–µ–∫—Å—ã.
        –£–º–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è: –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —É—Å—Ç–∞—Ä–µ–ª–∞.
        """

        with self.engine.begin() as conn:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ç–∞–±–ª–∏—Ü—ã
            try:
                result = conn.execute(text("PRAGMA table_info(training_dataset)")).fetchall()
                existing_columns = [row[1] for row in result]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–∞—Ä—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                has_old_structure = (
                        'features_json' in existing_columns or
                        'is_negative' in existing_columns or
                        'anti_trade_mask' in existing_columns
                )

                if has_old_structure:
                    logger.info("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ training_dataset - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É")
                    conn.execute(text("DROP TABLE IF EXISTS training_dataset"))
                elif existing_columns:
                    logger.info("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ training_dataset –∞–∫—Ç—É–∞–ª—å–Ω–∞ (29 –∫–æ–ª–æ–Ω–æ–∫)")
            except Exception:
                # –¢–∞–±–ª–∏—Ü—ã –Ω–µ—Ç - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —Å–æ–∑–¥–∞—Å—Ç—Å—è –Ω–∏–∂–µ
                logger.info("üìã –¢–∞–±–ª–∏—Ü–∞ training_dataset –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞")

            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã (IF NOT EXISTS –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è)
            conn.execute(text(CREATE_TRAINING_DATASET_SQL))
            for sql in CREATE_TRAINING_DATASET_INDEXES_SQL:
                conn.execute(text(sql))
            conn.execute(text(CREATE_TRAINING_DATASET_META_SQL))
            conn.execute(text(CREATE_TRAINING_FEATURE_IMPORTANCE_SQL))
            for sql in CREATE_TRAINING_FEATURE_IMPORTANCE_INDEXES_SQL:
                conn.execute(text(sql))

        logger.info("‚úÖ –¢–∞–±–ª–∏—Ü—ã training_dataset, training_dataset_meta, training_feature_importance –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã/—Å–æ–∑–¥–∞–Ω—ã")

    def __init__(self, config: LabelingConfig):
        self.config = config

        logger = logging.getLogger(__name__)
        _VALID_METHODS = {"CUSUM", "EXTREMUM", "PELT_ONLINE", "CUSUM_EXTREMUM"}

        m = (getattr(self.config, "method", None) or "CUSUM_EXTREMUM").upper()
        if m not in _VALID_METHODS:
            logger.warning(
                "Unknown labeling method '%s'. Falling back to 'CUSUM_EXTREMUM'. "
                "Allowed: %s",
                m, sorted(_VALID_METHODS),
            )
            m = "CUSUM_EXTREMUM"
        self.logger = logger
        self.config.method = m
        self.config.tool = self
        self.data_loader = DataLoader(
            db_engine=create_engine(MARKET_DB_DSN),
            symbol=config.symbol,
            timeframe=config.timeframe,
            config=config
        )

        self.labels = []
        self.engine = self.data_loader.connect()
        self.feature_names = self.data_loader.feature_names
        self._ensure_table_exists()
        self.config.pnl_threshold = 0.001
        logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω AdvancedLabelingTool –¥–ª—è {config.symbol}")

    def _check_table_exists(self, table_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —á–µ—Ä–µ–∑ SQLAlchemy"""
        from sqlalchemy import inspect
        try:
            inspector = inspect(self.engine)
            return table_name in inspector.get_table_names()
        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name}: {err}")
            return False

    def _validate_snapshot_frame(self, df: pd.DataFrame):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –≤ –ë–î.
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è 4-–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏.
        """
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        required = ["ts", "datetime", "reversal_label", "sample_weight"]
        missing = [col for col in required if col not in df.columns]
        if missing:
            raise ValueError(f"Snapshot validation failed: missing required columns: {missing}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π reversal_label (0,1,2,3)
        if not df["reversal_label"].isin([0, 1, 2, 3]).all():
            invalid_labels = df[~df["reversal_label"].isin([0, 1, 2, 3])]["reversal_label"].unique()
            raise ValueError(f"Invalid reversal_label values: {invalid_labels}. Expected: 0,1,2,3")

        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ts
        duplicates_count = df.duplicated(subset=["ts"]).sum()
        if duplicates_count > 0:
            logger.warning(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ts - —É–¥–∞–ª—è–µ–º")
            df = df.drop_duplicates(subset=["ts"], keep="first")

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        critical_cols = ["ts", "reversal_label", "sample_weight"]
        nan_mask = df[critical_cols].isna().any(axis=1)
        nan_drop_rows = nan_mask.sum()
        if nan_drop_rows > 0:
            logger.warning(f"‚ö†Ô∏è  –£–¥–∞–ª—è–µ–º {nan_drop_rows} —Å—Ç—Ä–æ–∫ —Å NaN –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö")
            df = df[~nan_mask]

        issues = {
            "duplicates_removed": int(duplicates_count),
            "class_balance": df["reversal_label"].value_counts().to_dict()
        }

        return df, issues, int(nan_drop_rows), int(duplicates_count)

    def _ensure_table_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü —á–µ—Ä–µ–∑ SQLAlchemy (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)"""
        from sqlalchemy import text

        # labeling_results: –µ–¥–∏–Ω–∞—è —Å—Ö–µ–º–∞
        if not self._check_table_exists('labeling_results'):
            logger.info("üìã –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã labeling_results...")

            create_table_sql = text("""
                CREATE TABLE IF NOT EXISTS labeling_results (
                    symbol TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    timeframe TEXT NOT NULL,
                    reversal_label INTEGER NOT NULL,
                    reversal_confidence REAL DEFAULT 1.0,
                    labeling_method TEXT NOT NULL,
                    labeling_params TEXT,
                    extreme_index INTEGER,
                    extreme_price REAL,
                    extreme_timestamp INTEGER NOT NULL,
                    confirmation_index INTEGER,
                    confirmation_timestamp INTEGER,
                    price_change_after REAL,
                    features_json TEXT,
                    is_high_quality INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (symbol, extreme_timestamp, reversal_label)
                )
            """)

            # –ò–Ω–¥–µ–∫—Å—ã: –∑–∞–ø—Ä–æ—Å—ã —á–∞—Å—Ç–æ –∏–¥—É—Ç –±–µ–∑ reversal_label, –¥–æ–±–∞–≤–∏–º –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–π –∏–Ω–¥–µ–∫—Å.
            create_index_1 = text("""
                CREATE INDEX IF NOT EXISTS idx_labeling_results_symbol_ts
                ON labeling_results(symbol, extreme_timestamp)
            """)
            # –ï—Å–ª–∏ —É —Ç–µ–±—è —á–∞—Å—Ç–æ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä –ø–æ timeframe ‚Äî –¥–æ–±–∞–≤—å –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å:
            create_index_2 = text("""
                CREATE INDEX IF NOT EXISTS idx_labeling_results_symbol_tf_ts
                ON labeling_results(symbol, timeframe, extreme_timestamp)
            """)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º begin() –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏
            with self.engine.begin() as conn:
                conn.execute(create_table_sql)
                conn.execute(create_index_1)
                conn.execute(create_index_2)

            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —Å–æ–∑–¥–∞–Ω–∞")
        else:
            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    def load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            df = self.data_loader.load_indicators()
            quality_ok, diagnostics = self.data_loader.validate_data_quality(df)

            if not quality_ok:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {diagnostics}")

            return df
        except Exception as err:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {err}")

    def _get_all_existing_signals(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫ —Å–∏–º–≤–æ–ª–∞ –∏–∑ –ë–î"""
        if not self._check_table_exists('labeling_results'):
            return []

        try:
            query = """
                SELECT extreme_timestamp, extreme_index, reversal_label, labeling_method 
                FROM labeling_results 
                WHERE symbol = :symbol 
                ORDER BY extreme_index
            """

            with self.engine.connect() as conn:
                result = conn.execute(
                    text(query),
                    {'symbol': self.config.symbol}
                ).fetchall()

            signals = []
            for extreme_ts, extreme_idx, reversal_label, method in result:
                signals.append({
                    'extreme_timestamp': extreme_ts,
                    'extreme_index': extreme_idx,
                    'reversal_label': reversal_label,
                    'labeling_method': method
                })

            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(signals)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫ –∏–∑ –ë–î")
            return signals

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–∫: {err}")
            return []

    def _calculate_pnl_to_index(self, df: pd.DataFrame, entry_idx: int, signal_type: str, end_idx: int) -> Tuple[
        float, bool]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç PnL –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        if end_idx >= len(df):
            return 0.0, False

        try:
            entry_price = df['close'].iloc[entry_idx]
            exit_price = df['close'].iloc[end_idx]

            if entry_price <= 0:
                return 0.0, False

            if signal_type == 'BUY':
                effective_entry = entry_price * (1 + self.config.fee_percent)
                effective_exit = exit_price * (1 - self.config.fee_percent)
                net_pnl = (effective_exit - effective_entry) / effective_entry
            else:  # SELL
                effective_entry = entry_price * (1 - self.config.fee_percent)
                effective_exit = exit_price * (1 + self.config.fee_percent)
                net_pnl = (effective_entry - effective_exit) / effective_entry

            is_profitable_enough = net_pnl >= self.config.min_profit_target
            return net_pnl, is_profitable_enough

        except (IndexError, ZeroDivisionError, KeyError) as err:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ PnL –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {entry_idx}: {err}")
            return 0.0, False

    # =========================================================================
    # –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –†–ê–ó–ú–ï–¢–ö–ò (–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞)
    # =========================================================================

    def _calculate_pnl(self, df: pd.DataFrame, entry_idx: int, signal_type: str) -> Tuple[float, bool]:
        if entry_idx + self.config.hold_bars >= len(df):
            return 0.0, False

        try:
            entry_price = df['close'].iloc[entry_idx]
            exit_price = df['close'].iloc[entry_idx + self.config.hold_bars]

            if entry_price <= 0:
                return 0.0, False

            if signal_type == 'BUY':
                effective_entry = entry_price * (1 + self.config.fee_percent)
                effective_exit = exit_price * (1 - self.config.fee_percent)
                net_pnl = (effective_exit - effective_entry) / effective_entry  # ‚Üê —ç—Ç–æ —É–∂–µ –≤ –¥–æ–ª—è—Ö (0.01 = 1%)

            else:  # SELL
                effective_entry = entry_price * (1 - self.config.fee_percent)
                effective_exit = exit_price * (1 + self.config.fee_percent)
                net_pnl = (effective_entry - effective_exit) / effective_entry  # ‚Üê —ç—Ç–æ —É–∂–µ –≤ –¥–æ–ª—è—Ö (0.01 = 1%)

            is_profitable_enough = net_pnl >= self.config.min_profit_target
            return net_pnl, is_profitable_enough

        except (IndexError, ZeroDivisionError, KeyError) as err:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ PnL –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {entry_idx}: {err}")
            return 0.0, False

    def _interpret_pnl_results(self, total_metrics: Dict[str, float]):
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ PnL —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏"""
        total_pnl = total_metrics.get('total_pnl', 0)
        success_rate = total_metrics.get('success_rate', 0)
        pnl_ratio = total_metrics.get('pnl_ratio', 0)

        print(f"\nüîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")

        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        if success_rate > 0.6:
            print(f"   üéØ –í—ã—Å–æ–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (>60%)")
        elif success_rate > 0.4:
            print(f"   ‚úÖ –°—Ä–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (40-60%)")
        else:
            print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} (<40%)")

        # –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è PNL
        if pnl_ratio > 3:
            print(f"   üíé –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–æ–≤: {pnl_ratio:.1f}:1")
        elif pnl_ratio > 2:
            print(f"   ‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–æ–≤: {pnl_ratio:.1f}:1")
        elif pnl_ratio > 1:
            print(f"   ‚ö†Ô∏è  –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1")
        else:
            print(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1")

        # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–π –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∫–∞–∫ –µ—Å—Ç—å)
        if total_pnl > 0.1:  # >10%
            print(f"   üöÄ –í—ã—Å–æ–∫–∞—è –æ–±—â–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        elif total_pnl > 0.02:  # >2%
            print(f"   ‚úÖ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        elif total_pnl > 0:  # >0%
            print(f"   ‚ö†Ô∏è  –°–ª–∞–±–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {total_pnl:+.1%}")
        else:
            print(f"   ‚ùå –£–±—ã—Ç–æ—á–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {total_pnl:+.1%}")

    def _smart_confirmation_system(self, df: pd.DataFrame, signal_idx: int, signal_type: str) -> dict:
        """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"""
        confirm_bar = self._get_confirmation_bars(signal_type)

        confirmation_data = {
            'confirmed': False,
            'confidence_boost': 0.0,
            'early_rejection': False,
            'confirmation_index': signal_idx + confirm_bar,
            'price_change': 0.0
        }

        if signal_idx + 3 >= len(df):
            confirmation_data['early_rejection'] = True
            return confirmation_data

        price_at_signal = df['close'].iloc[signal_idx]
        price_after_3bars = df['close'].iloc[signal_idx + 3]
        expected_move = 0.005

        if signal_type == 'BUY':
            move_percent = (price_after_3bars - price_at_signal) / price_at_signal
            if move_percent < -expected_move:
                confirmation_data['early_rejection'] = True
        else:
            move_percent = (price_at_signal - price_after_3bars) / price_at_signal
            if move_percent < -expected_move:
                confirmation_data['early_rejection'] = True

        if not confirmation_data['early_rejection'] and signal_idx + confirm_bar < len(df):
            confirmation_data['confirmed'] = True
            confirmation_data['price_change'] = move_percent
            if abs(move_percent) > expected_move:
                confirmation_data['confidence_boost'] = 0.15

        return confirmation_data

    def merge_conflicting_labels(self):
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ–π —Ä–∞–∑–º–µ—Ç–∫–∏"""
        try:
            df = self.load_data()
            with self.engine.begin() as conn:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ BUY/SELL –º–µ—Ç–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                signals_query = """
                    SELECT rowid, extreme_timestamp, reversal_label, labeling_method, price_change_after
                    FROM labeling_results 
                    WHERE symbol = :symbol AND reversal_label IN (1,2)
                    ORDER BY extreme_timestamp
                """
                signals = conn.execute(text(signals_query), {'symbol': self.config.symbol}).fetchall()

                if not signals:
                    print("‚úÖ –ù–µ—Ç BUY/SELL –º–µ—Ç–æ–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                    return 0

                fixed_count = 0

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–∞—Ä—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
                for i in range(len(signals) - 1):
                    current_signal = signals[i]
                    next_signal = signals[i + 1]

                    current_ts = current_signal.extreme_timestamp
                    next_ts = next_signal.extreme_timestamp

                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ PNL
                    current_idx_match = df.index[df['ts'] == current_ts].tolist()
                    next_idx_match = df.index[df['ts'] == next_ts].tolist()

                    if not current_idx_match or not next_idx_match:
                        continue

                    current_idx = int(current_idx_match[0])
                    next_idx = int(next_idx_match[0])

                    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º PNL –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
                    signal_type = 'BUY' if current_signal.reversal_label == 1 else 'SELL'
                    pnl, _ = self._calculate_pnl_to_index(df, current_idx, signal_type, next_idx)

                    # –ï—Å–ª–∏ PNL < 0.1% - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É
                    if abs(pnl) < 0.001:
                        print(f"üîÑ –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–∞ ts={current_ts}, PNL={pnl:.4f}")

                        # –£–¥–∞–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–∏–≥–Ω–∞–ª
                        conn.execute(text("DELETE FROM labeling_results WHERE rowid = :rowid"),
                                     {'rowid': current_signal.rowid})

                        # –°—Ç–∞–≤–∏–º HOLD –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º –±–∞—Ä–µ
                        if current_idx + 1 < len(df):
                            next_ts_hold = int(df.iloc[current_idx + 1]['ts'])
                            conn.execute(text("""
                                INSERT OR IGNORE INTO labeling_results 
                                (symbol, timestamp, timeframe, reversal_label, reversal_confidence, 
                                 labeling_method, extreme_timestamp, price_change_after)
                                VALUES (:symbol, :ts, :tf, 0, 1.0, 'VALIDATED_HOLD', :ts, 0.0)
                            """), {'symbol': self.config.symbol, 'ts': next_ts_hold, 'tf': self.config.timeframe})

                        fixed_count += 1

                print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –º–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {fixed_count}")
                return fixed_count

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏: {err}")
            raise

    def _get_confirmation_bars(self, signal_type: str) -> int:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞ —Ä–∞–∑–º–µ—Ç–∫–∏.
        –î–µ–ª–∞–µ—Ç –º–µ—Ç–æ–¥ —É—Å—Ç–æ–π—á–∏–≤—ã–º –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É/–≤–≤–æ–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        import logging
        logger = logging.getLogger(__name__)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤–≤–æ–¥—É)
        method = (getattr(self.config, "method", "") or "").upper()

        # –ë–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤)
        if method == "PELT_ONLINE":
            base_confirmation = 3
        elif method in ("CUSUM", "EXTREMUM", "CUSUM_EXTREMUM"):
            base_confirmation = 2
        else:
            logger.warning(
                "Unknown labeling method '%s' in _get_confirmation_bars; using base_confirmation=2",
                method
            )
            base_confirmation = 2

        confirmation_bars = base_confirmation
        if self.config.method == 'EXTREMUM':
            confirmation_bars += 1  # —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è EXTREMUM

        logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è {signal_type} —Å–∏–≥–Ω–∞–ª–∞")
        return max(1, min(5, confirmation_bars))  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏

    def _pelt_offline_reversals(self, df: pd.DataFrame) -> List[Dict]:
        """
        PELT offline —Å –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–æ–º penalty –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ü–µ—Å—Å–∞.
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±—É–¥—É—â–µ–µ.
        –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ + –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ 'q'.
        –í–•–û–î –ù–ê –°–õ–ï–î–£–Æ–©–ï–ô –°–í–ï–ß–ï –ü–û–°–õ–ï CHANGEPPOINT'–∞ (–∫–∞–∫ –≤ extremum).
        """
        if not RUPTURES_AVAILABLE:
            logger.warning("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ruptures –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return []

        if len(df) < 500:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PELT Offline: {len(df)}")
            return []

        # –ò–º–ø–æ—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–∂–∞—Ç–∏—è –∫–ª–∞–≤–∏—à (Windows)
        try:
            import msvcrt
            has_keyboard_check = True
        except ImportError:
            has_keyboard_check = False
            logger.warning("‚ö†Ô∏è –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (—Ç–æ–ª—å–∫–æ Windows)")

        logger.info(f"üìä PELT Offline: –∞–Ω–∞–ª–∏–∑ {len(df)} —Å–≤–µ—á–µ–π...")

        # üéØ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –í–´–ë–û–† –¶–ï–õ–ï–í–û–ì–û –ö–û–õ–ò–ß–ï–°–¢–í–ê
        print("\n" + "=" * 60)
        print("üéØ –ù–ê–°–¢–†–û–ô–ö–ê PELT OFFLINE: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤")
        print("=" * 60)
        print("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é:")
        print("   üìä [1] –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è:  3-5/–¥–µ–Ω—å   (~600-1000 —Å–∏–≥–Ω–∞–ª–æ–≤)")
        print("   üìä [2] –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: 10-15/–¥–µ–Ω—å (~2000-3000 —Å–∏–≥–Ω–∞–ª–æ–≤)")
        print("   üìä [3] –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è:     20-30/–¥–µ–Ω—å (~4000-6000 —Å–∏–≥–Ω–∞–ª–æ–≤)")
        print("   ‚öôÔ∏è  [4] –°–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ")

        choice = input("\n–í—ã–±–æ—Ä [2]: ").strip()

        if choice == '1':
            target_signals_daily = 4.0
            print("‚úÖ –í—ã–±—Ä–∞–Ω–∞ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: ~4 —Å–∏–≥–Ω–∞–ª–∞/–¥–µ–Ω—å")
        elif choice == '3':
            target_signals_daily = 25.0
            print("‚úÖ –í—ã–±—Ä–∞–Ω–∞ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: ~25 —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å")
        elif choice == '4':
            custom_input = input("–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –¥–µ–Ω—å [12]: ").strip()
            if custom_input:
                try:
                    target_signals_daily = float(custom_input)
                    if target_signals_daily < 1 or target_signals_daily > 50:
                        print("‚ö†Ô∏è –ó–Ω–∞—á–µ–Ω–∏–µ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (1-50), –∏—Å–ø–æ–ª—å–∑—É—é 12")
                        target_signals_daily = 12.0
                    else:
                        print(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {target_signals_daily} —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å")
                except ValueError:
                    print("‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—é 12")
                    target_signals_daily = 12.0
            else:
                target_signals_daily = 12.0
        else:  # default –∏–ª–∏ '2'
            target_signals_daily = 12.0
            print("‚úÖ –í—ã–±—Ä–∞–Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: ~12 —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å")

        print("=" * 60 + "\n")

        min_size = max(4, self.config.pelt_min_size)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏–≥–Ω–∞–ª–∞
        close_vals = df['close'].astype(float).values
        signal = np.log(np.clip(close_vals, 1e-12, None))
        n_samples = len(signal)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º bars_per_day (–¥–ª—è 5m = 288)
        bars_per_day = 288

        # ‚ö†Ô∏è –ö–û–†–†–ï–ö–¶–ò–Ø: changepoints ‚â† —Å–∏–≥–Ω–∞–ª—ã
        SIGNAL_TO_CHANGEPOINT_RATIO = 2.55  # —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏

        target_changepoints = target_signals_daily * (n_samples / bars_per_day) * SIGNAL_TO_CHANGEPOINT_RATIO
        target_total = target_changepoints

        # –î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ penalty
        target_low = int(target_total * 0.8)
        target_high = int(target_total * 1.2)

        # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
        expected_signals = target_changepoints / SIGNAL_TO_CHANGEPOINT_RATIO
        expected_signals_daily = expected_signals * bars_per_day / n_samples

        # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä penalty
        start_pen, end_pen = 1e-7, 1e-2
        n_steps = 30

        best_penalty = None
        best_changepoints = None
        closest_distance = float('inf')

        # üå°Ô∏è –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û–î–ë–û–†–ê
        print(f"üéØ –¶–µ–ª—å: {target_total:.1f} changepoints (–¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ~{target_signals_daily:.1f} —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å)")
        print(f"üîç –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ penalty (–¥–∏–∞–ø–∞–∑–æ–Ω: {target_low}-{target_high} —Ç–æ—á–µ–∫)...")
        if has_keyboard_check:
            print(f"üí° –ù–∞–∂–º–∏—Ç–µ 'q' –∏–ª–∏ Enter –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
        print("–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–¥–±–æ—Ä–∞:")

        pens = np.logspace(np.log10(start_pen), np.log10(end_pen), num=n_steps)

        # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        max_found = 0
        iterations_without_improvement = 0
        early_stop = False
        manual_stop = False

        for i, pen in enumerate(pens):
            # üõë –ü–†–û–í–ï–†–ö–ê –ù–ê–ñ–ê–¢–ò–Ø –ö–õ–ê–í–ò–®–ò
            if has_keyboard_check and msvcrt.kbhit():
                key = msvcrt.getch()
                if key in [b'q', b'Q', b'\r']:  # q, Q –∏–ª–∏ Enter
                    sys.stdout.write(f"\rüõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    manual_stop = True
                    break

            try:
                pen_value = float(pen) if hasattr(pen, 'item') else pen
                algo = rpt.Pelt(model="l2", min_size=min_size, jump=5).fit(signal)
                changepoints = algo.predict(pen=pen_value)
                changepoints = [cp for cp in changepoints if cp < len(df)]
                n_cp = max(len(changepoints) - 1, 0)

                # üõë –í–´–•–û–î –ï–°–õ–ò –¢–û–ß–ö–ò –£–ñ–ï –ù–ò–ñ–ï –¶–ï–õ–ï–í–û–ì–û –î–ò–ê–ü–ê–ó–û–ù–ê
                if n_cp < target_low and best_changepoints is not None:
                    sys.stdout.write(f"\rüõë –í—ã—Ö–æ–¥: —Ç–æ—á–∫–∏ {n_cp} < –º–∏–Ω–∏–º—É–º–∞ {target_low}")
                    break

                # –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–ï –ú–ê–ö–°–ò–ú–£–ú–ê
                if n_cp > max_found:
                    max_found = n_cp
                    iterations_without_improvement = 0
                else:
                    iterations_without_improvement += 1

                dist = abs(n_cp - target_total)
                if dist < closest_distance:
                    closest_distance = dist
                    best_penalty = pen_value
                    best_changepoints = changepoints

                # üå°Ô∏è –¢–ï–†–ú–û–ú–ï–¢–†
                bar_len = 20
                filled = int((i + 1) / n_steps * bar_len)
                bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)
                color = "\033[92m" if target_low <= n_cp <= target_high else "\033[0m"
                kb_hint = " [q-—Å—Ç–æ–ø]" if has_keyboard_check else ""
                sys.stdout.write(
                    f"\r  {bar} {i + 1}/{n_steps} | pen={pen_value:.8f} | {color}{n_cp:5d} —Ç–æ—á–µ–∫{kb_hint}\033[0m"
                )
                sys.stdout.flush()

                # üõë –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê
                if i >= 4:
                    if max_found < target_low * 0.5 and iterations_without_improvement >= 3:
                        sys.stdout.write(f"\r‚ö†Ô∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {i + 1}/{n_steps}")
                        early_stop = True
                        break

            except Exception as err:
                continue

            if manual_stop:
                break

        sys.stdout.write("\n")

        # üõë –ü–û–õ–ù–ê–Ø –û–°–¢–ê–ù–û–í–ö–ê, –ï–°–õ–ò –ë–´–õ–û –ù–ê–ñ–ê–¢–û 'Q'
        if manual_stop:
            print(
                f"‚úÖ –ü–æ–¥–±–æ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–Ω–∞–π–¥–µ–Ω–æ {len(best_changepoints) if best_changepoints else 0} —Ç–æ—á–µ–∫)")
            if best_changepoints is None or len(best_changepoints) == 0:
                logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ change points –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
                return []
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º best_penalty –∫–∞–∫ —Ä–∞–±–æ—á–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º

        # üîÑ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ü–†–û–•–û–î –¢–û–õ–¨–ö–û –ï–°–õ–ò –ù–ï –ë–´–õ–û –†–£–ß–ù–û–ô –û–°–¢–ê–ù–û–í–ö–ò
        if not manual_stop and not early_stop and best_changepoints is not None:
            n_best = len(best_changepoints) - 1
            if n_best < target_low * 0.9:
                print(f"‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ({n_best} < {target_low}). –ü—Ä–æ–±—É–µ–º –º–µ–Ω—å—à–∏–µ penalty...")
                start_pen_new = start_pen / 100
                end_pen_new = best_penalty
                pens_extra = np.logspace(np.log10(start_pen_new), np.log10(end_pen_new), num=20)
                print("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥–±–æ—Ä:")

                for i, pen in enumerate(pens_extra):
                    if has_keyboard_check and msvcrt.kbhit():
                        key = msvcrt.getch()
                        if key in [b'q', b'Q', b'\r']:
                            sys.stdout.write(f"\rüõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                            manual_stop = True
                            break

                    try:
                        pen_value = float(pen) if hasattr(pen, 'item') else pen
                        algo = rpt.Pelt(model="l2", min_size=min_size, jump=5).fit(signal)
                        changepoints = algo.predict(pen=pen_value)
                        changepoints = [cp for cp in changepoints if cp < len(df)]
                        n_cp = max(len(changepoints) - 1, 0)
                        dist = abs(n_cp - target_total)
                        if dist < closest_distance:
                            closest_distance = dist
                            best_penalty = pen_value
                            best_changepoints = changepoints

                        bar_len = 15
                        filled = int((i + 1) / 20 * bar_len)
                        bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)
                        color = "\033[92m" if target_low <= n_cp <= target_high else "\033[0m"
                        kb_hint = " [q-—Å—Ç–æ–ø]" if has_keyboard_check else ""
                        sys.stdout.write(
                            f"\r  {bar} {i + 1}/20 | pen={pen_value:.8f} | {color}{n_cp:5d} —Ç–æ—á–µ–∫{kb_hint}\033[0m"
                        )
                        sys.stdout.flush()
                    except Exception as err:
                        continue

                    if manual_stop:
                        break

                sys.stdout.write("\n")

        if not manual_stop and not early_stop:
            print("‚úÖ –ü–æ–¥–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω")

        if best_changepoints is None or len(best_changepoints) == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ change points.")
            logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ change points")
            return []

        # üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê
        changepoints_daily = (len(best_changepoints) * bars_per_day / n_samples) if n_samples > 0 else 0
        estimated_signals = (len(best_changepoints) - 1) / SIGNAL_TO_CHANGEPOINT_RATIO
        estimated_signals_daily = estimated_signals * bars_per_day / n_samples
        deviation = abs(estimated_signals_daily - target_signals_daily)

        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–î–ë–û–†–ê:")
        print(f"   üéØ –õ—É—á—à–∏–π penalty: {best_penalty:.7f}")
        print(f"   üìà –ù–∞–π–¥–µ–Ω–æ changepoints: {len(best_changepoints)} (~{changepoints_daily:.1f}/–¥–µ–Ω—å)")
        print(f"   üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Å–∏–≥–Ω–∞–ª—ã: ~{estimated_signals:.0f} (~{estimated_signals_daily:.1f}/–¥–µ–Ω—å)")
        print(f"   üéØ –¶–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω changepoints: {target_low}-{target_high}")
        print(
            f"   {'‚úÖ' if deviation <= target_signals_daily * 0.3 else '‚ö†Ô∏è'} –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ü–µ–ª–∏: {deviation:.1f} —Å–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å")

        logger.info(
            f"‚úÖ PELT Offline: penalty={best_penalty:.7f}, {len(best_changepoints)} changepoints, ~{estimated_signals:.0f} —Å–∏–≥–Ω–∞–ª–æ–≤ (~{estimated_signals_daily:.1f}/–¥–µ–Ω—å)")

        changepoints = best_changepoints
        results = []

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ BUY/SELL —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –º–µ–∂–¥—É changepoints
        for i in range(len(changepoints) - 1):
            start = changepoints[i]  # –∏–Ω–¥–µ–∫—Å —ç–∫—Å—Ç—Ä–µ–º—É–º–∞
            end = changepoints[i + 1]

            if start >= len(df) or end >= len(df):
                continue

            # –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if i > 0:
                prev_start = changepoints[i - 1]
                if prev_start >= len(df):
                    continue

                # –¢—Ä–µ–Ω–¥ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ changepoint
                current_trend_up = df['close'].iat[end - 1] > df['close'].iat[start]
                # –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ç—Ä–µ–Ω–¥
                prev_trend_up = df['close'].iat[start - 1] > df['close'].iat[prev_start]

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–≤–µ—Ä—Å
                rev_type = None
                if not prev_trend_up and current_trend_up:
                    rev_type = "BUY"
                elif prev_trend_up and not current_trend_up:
                    rev_type = "SELL"

                if rev_type is None:
                    continue

                # üîÅ –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–•–û–î –ù–ê –°–õ–ï–î–£–Æ–©–ï–ô –°–í–ï–ß–ï –ü–û–°–õ–ï –≠–ö–°–¢–†–ï–ú–£–ú–ê
                entry_index = start + 1
                if entry_index >= len(df):
                    continue  # –ó–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

                extreme_ts = int(df['ts'].iat[start])
                entry_ts = int(df['ts'].iat[entry_index])

                confidence = min(abs((df['close'].iat[end - 1] - df['close'].iat[start]) / df['close'].iat[start]),
                                 0.95)
                confidence = max(confidence, 0.5)

                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å smart system)
                confirmation = self._smart_confirmation_system(df, entry_index, rev_type)
                conf_idx = confirmation['confirmation_index']
                if conf_idx >= len(df):
                    conf_idx = len(df) - 1

                results.append({
                    'index': entry_index,
                    'type': rev_type,
                    'confidence': confidence,
                    'extreme_index': start,
                    'extreme_timestamp': extreme_ts,
                    'confirmation_index': conf_idx,
                    'confirmation_timestamp': int(df['ts'].iat[conf_idx]),
                    'method': 'PELT_OFFLINE',
                    'reversal_label': 1 if rev_type == 'BUY' else 2,
                })

        logger.info(f"üìä PELT Offline –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤ (–≤—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å–≤–µ—á–µ)")
        if results:
            buy_count = sum(1 for r in results if r['type'] == 'BUY')
            sell_count = sum(1 for r in results if r['type'] == 'SELL')
            avg_conf = np.mean([r['confidence'] for r in results])
            print(f"üìà –°–∏–≥–Ω–∞–ª—ã: {buy_count} BUY, {sell_count} SELL, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.2f}\n")
            logger.info(f"üìà –î–µ—Ç–∞–ª–∏: {buy_count} BUY, {sell_count} SELL, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.2f}")

        return results

    def _cusum_reversals(self, df):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–≤–µ—Ä—Å–∏–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ –≥–æ—Ç–æ–≤—ã–º –ø–æ–ª—è–º CUSUM 5m –∏–∑ –ë–î.
        –ö–æ–¥–∏—Ä–æ–≤–∫–∞: BUY=1, SELL=2, HOLD=0 (HOLD –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º).
        –û—Ç–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ |cusum_zscore| –∏/–∏–ª–∏ cusum_conf —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º.
        """
        cfg = self.config  # LabelingConfig
        out = []

        # –¢—Ä–µ–±—É–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        need = ["ts", "cusum_state", "cusum_zscore", "cusum_conf"]
        if any(c not in df.columns for c in need):
            logger.warning("CUSUM reversals: –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ %s", need)
            return out

        # –∏–Ω–¥–µ–∫—Å—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã: —Å–∏–ª—å–Ω—ã–π z –∏–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        z = df["cusum_zscore"].astype(float)
        conf = df["cusum_conf"].astype(float)
        state = df["cusum_state"].astype("Int64")  # 1/2/0

        cand_mask = (z.abs() >= float(getattr(cfg, "cusum_z_threshold", 1.0))) | \
                    (conf >= float(getattr(cfg, "cusum_conf_threshold", 0.6)))

        idxs = df.index[cand_mask & state.isin([1, 2])]
        if len(idxs) == 0:
            return out

        for i in idxs:
            s = int(state.loc[i])
            signal_type = "BUY" if s == Direction.BUY else "SELL"
            label = Direction.BUY if s == Direction.BUY else Direction.SELL
            base_conf = float(conf.loc[i]) if pd.notna(conf.loc[i]) else 0.0
            base_conf = max(0.2, min(0.95, base_conf))  # –º—è–≥–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã

            # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            confirm = self._smart_confirmation_system(df, i, signal_type)

            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∏–≥–Ω–∞–ª–∞ —Å –í–°–ï–ú–ò –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–æ–ª—è–º–∏
            signal_data = {
                "index": int(i),
                "type": signal_type,  # "BUY"/"SELL"
                "confidence": max(0.0, min(0.99, base_conf + float(confirm.get("confidence_boost", 0.0)))),
                "extreme_timestamp": int(df["ts"].loc[i]) if "ts" in df.columns and pd.notna(df["ts"].loc[i]) else None,
                "confirmation_index": confirm['confirmation_index'],
                "confirmation_timestamp": int(df["ts"].iloc[confirm['confirmation_index']]) if confirm[
                                                                                                   'confirmation_index'] < len(
                    df) else None,
                "method": 'CUSUM'
            }

            out.append(signal_data)

        return out

    def _extremum_reversals(self, df: pd.DataFrame) -> List[Dict]:
        """–≠–∫—Å—Ç—Ä–µ–º—É–º —Ä–µ–≤–µ—Ä—Å–∏–∏ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ–º –∏ –ø–µ—Ä–µ—Å–∫–æ–∫–æ–º –æ–∫–Ω–∞"""
        window = self.config.extremum_window
        confirm_bar = max(1, min(5, self.config.extremum_confirm_bar))
        min_distance = getattr(self.config, 'min_signal_distance', 10)
        low = df['low'].values
        high = df['high'].values
        results = []

        # –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å –Ω–∞—á–∞–ª–∞ –æ–∫–Ω–∞
        i = window
        last_extremum_type = None  # 'BUY' –∏–ª–∏ 'SELL'

        while i < len(df) - window:
            current_low = low[i]
            current_high = high[i]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∏–Ω–∏–º—É–º –≤ –æ–∫–Ω–µ
            is_low_extreme = current_low == np.min(low[i - window:i + window + 1])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∞–∫—Å–∏–º—É–º –≤ –æ–∫–Ω–µ
            is_high_extreme = current_high == np.max(high[i - window:i + window + 1])

            signal_type = None

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ —Å —É—á–µ—Ç–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if is_low_extreme and last_extremum_type != 'BUY':
                signal_type = 'BUY'
            elif is_high_extreme and last_extremum_type != 'SELL':
                signal_type = 'SELL'

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —ç–∫—Å—Ç—Ä–µ–º—É–º
            if signal_type and i + confirm_bar < len(df):
                confirmation = self._smart_confirmation_system(df, i, signal_type)
                if not confirmation['early_rejection']:
                    results.append({
                        'index': i,
                        'type': signal_type,
                        'confidence': 0.7 + confirmation['confidence_boost'],
                        'extreme_timestamp': df['ts'].iloc[i],
                        'confirmation_index': confirmation['confirmation_index'],
                        'confirmation_timestamp': df['ts'].iloc[confirmation['confirmation_index']],
                        'method': 'EXTREMUM'
                    })
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ–º –æ–∫–Ω–æ
                    last_extremum_type = signal_type
                    i += min_distance  # –ü–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ–º –Ω–∞ min_distance –≤–ø–µ—Ä–µ–¥
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—ã—á–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ i

            # –ï—Å–ª–∏ —ç–∫—Å—Ç—Ä–µ–º—É–º –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ –ø—Ä–æ—à–µ–ª –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ - –¥–≤–∏–≥–∞–µ–º—Å—è –Ω–∞ 1 –±–∞—Ä
            i += 1

        return results

    def _cusum_extremum_hybrid(self, df):
        """
        –ì–∏–±—Ä–∏–¥ CUSUM (–∏–∑ –ë–î) + —ç–∫—Å—Ç—Ä–µ–º—É–º—ã. –¢–∏–ø –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º ‚â§ 2.
        –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –¥–≤—É—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ (–∏–ª–∏ –ø–æ –≤–∞—à–µ–π —Ç–µ–∫—É—â–µ–π —Ñ–æ—Ä–º—É–ª–µ).
        """
        cusum_signals = self._cusum_reversals(df)
        extremum_signals = self._extremum_reversals(df)

        if not cusum_signals:
            return extremum_signals
        if not extremum_signals:
            return cusum_signals

        # –±—ã—Å—Ç—Ä—ã–π –º–∞–ø –ø–æ —Ç–∏–ø—É
        by_type = {"BUY": [], "SELL": []}
        for s in cusum_signals:
            by_type[s["type"]].append(s)

        out = []
        for e in extremum_signals:
            group = by_type.get(e["type"], [])
            # –Ω–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏–∑ CUSUM (–ø–æ—Ä–æ–≥ 2 –±–∞—Ä–∞)
            best = None
            best_d = None
            for c in group:
                d = abs(int(c["index"]) - int(e["index"]))
                if d <= 2 and (best is None or d < best_d):
                    best, best_d = c, d
            if best is not None:
                # –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                conf = (float(best["confidence"]) + float(e.get("confidence", 0.0))) / 2.0
                merged = e.copy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ø–∏—é extremum —Å–∏–≥–Ω–∞–ª–∞ –∫–∞–∫ –æ—Å–Ω–æ–≤—É
                merged["confidence"] = max(0.0, min(0.99, conf))
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è –∏–∑ extremum —Å–∏–≥–Ω–∞–ª–∞
                out.append(merged)
            else:
                out.append(e)

        return out

    # =========================================================================
    # –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ú–ï–¢–û–î–´
    # =========================================================================

    def advanced_quality_analysis(self) -> Dict[str, Any]:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ —Å –æ–±—â–∏–º PNL+ –∏ PNL-
        """
        logger.info("üîç –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞...")

        # ‚¨áÔ∏è –ì–ê–†–ê–ù–¢–ò–†–£–ï–ú —á—Ç–æ –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è dict
        default_result = {
            'methods_performance': [],
            'total_metrics': {},
            'best_method': {'method': 'N/A', 'success_rate': 0.0},
            'data_quality_issues': [],
            'timestamp': datetime.now().isoformat()
        }

        try:
            # ‚¨áÔ∏è –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ó–ê–ü–†–û–°: –¥–æ–±–∞–≤–ª—è–µ–º —Å—É–º–º—ã PNL+ –∏ PNL-
            query = """
                SELECT 
                    labeling_method,
                    reversal_label,
                    COUNT(*) as total_signals,
                    AVG(reversal_confidence) as avg_confidence,
                    AVG(price_change_after) as avg_profit,
                    SUM(price_change_after) as total_pnl,
                    -- ‚¨áÔ∏è –î–û–ë–ê–í–õ–Ø–ï–ú –°–£–ú–ú–´ –î–õ–Ø PNL+ –ò PNL-
                    SUM(CASE WHEN price_change_after > 0 THEN price_change_after ELSE 0 END) as total_positive_pnl,
                    SUM(CASE WHEN price_change_after < 0 THEN price_change_after ELSE 0 END) as total_negative_pnl,
                    SUM(CASE WHEN price_change_after >= :min_profit THEN 1 ELSE 0 END) as profitable_signals,
                    SUM(CASE WHEN price_change_after < -:min_profit THEN 1 ELSE 0 END) as loss_signals,
                    MIN(price_change_after) as min_profit,
                    MAX(price_change_after) as max_profit
                FROM labeling_results 
                WHERE symbol = :symbol
                GROUP BY labeling_method, reversal_label
            """

            df_quality = pd.read_sql_query(
                query,
                self.engine,
                params={
                    'min_profit': self.config.min_profit_target,
                    'symbol': self.config.symbol
                }
            )

            if df_quality.empty:
                logger.warning("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
                return default_result

            # === –í–ê–õ–ò–î–ê–¶–ò–Ø –ú–ï–¢–û–ö –° PNL=0 ===
            validation_warnings = []

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            all_labels_query = """
                SELECT 
                    extreme_timestamp,
                    reversal_label,
                    price_change_after,
                    labeling_method
                FROM labeling_results 
                WHERE symbol = :symbol
                ORDER BY extreme_timestamp
            """

            df_all_labels = pd.read_sql_query(
                all_labels_query,
                self.engine,
                params={'symbol': self.config.symbol}
            )

            validated_zero_pnl = 0
            invalid_zero_pnl = 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ BUY/SELL –º–µ—Ç–∫–∏
            for idx, row in df_all_labels[df_all_labels['reversal_label'].isin([1, 2])].iterrows():
                if row['price_change_after'] == 0.0:
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–µ—Ç–∫—É
                    next_labels = df_all_labels[df_all_labels['extreme_timestamp'] > row['extreme_timestamp']]

                    if next_labels.empty:
                        validation_warnings.append({
                            'timestamp': row['extreme_timestamp'],
                            'label': row['reversal_label'],
                            'method': row['labeling_method'],
                            'issue': '–ù–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏'
                        })
                        invalid_zero_pnl += 1
                    else:
                        next_label = next_labels.iloc[0]
                        if next_label['reversal_label'] != 0:
                            validation_warnings.append({
                                'timestamp': row['extreme_timestamp'],
                                'label': row['reversal_label'],
                                'method': row['labeling_method'],
                                'issue': f'–°–ª–µ–¥—É—é—â–∞—è –º–µ—Ç–∫–∞ –Ω–µ HOLD (reversal_label={next_label["reversal_label"]})'
                            })
                            invalid_zero_pnl += 1
                        else:
                            validated_zero_pnl += 1

            analysis = {
                'methods_performance': df_quality.to_dict('records'),
                'total_metrics': self._calculate_total_metrics(df_quality),
                'best_method': self._find_best_method(df_quality),
                'data_quality_issues': self._detect_data_quality_issues(),
                'validation': {
                    'validated_zero_pnl': validated_zero_pnl,
                    'invalid_zero_pnl': invalid_zero_pnl,
                    'warnings': validation_warnings
                },
                'timestamp': datetime.now().isoformat()
            }

            self._log_quality_analysis(analysis)

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if validated_zero_pnl > 0:
                print(f"\n‚úÖ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–æ –º–µ—Ç–æ–∫ —Å PnL=0: {validated_zero_pnl}")

            if validation_warnings:
                print(f"\n‚ö†Ô∏è  WARNINGS: –ù–∞–π–¥–µ–Ω–æ {len(validation_warnings)} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫ —Å PnL=0:")
                for w in validation_warnings[:10]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    label_str = 'BUY' if w['label'] == 1 else 'SELL'
                    print(f"   ‚Ä¢ ts={w['timestamp']} | {label_str} | {w['method']} | {w['issue']}")
                if len(validation_warnings) > 10:
                    print(f"   ... –∏ –µ—â–µ {len(validation_warnings) - 10} warnings")

            return analysis

        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {err}")
            return default_result

    def _calculate_total_metrics(self, df_quality: pd.DataFrame) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å PNL+ –∏ PNL-"""
        if df_quality.empty:
            return {}

        total_profitable = df_quality['profitable_signals'].sum()
        total_signals = df_quality['total_signals'].sum()
        overall_success = total_profitable / total_signals if total_signals > 0 else 0

        # ‚¨áÔ∏è –î–û–ë–ê–í–õ–Ø–ï–ú –†–ê–°–ß–ï–¢ PNL+ –ò PNL-
        total_positive_pnl = df_quality['total_positive_pnl'].sum()
        total_negative_pnl = df_quality['total_negative_pnl'].sum()
        total_pnl = df_quality['total_pnl'].sum()

        return {
            'total_signals': int(total_signals),
            'profitable_signals': int(total_profitable),
            'success_rate': float(overall_success),
            'avg_confidence': float(df_quality['avg_confidence'].mean()),
            'avg_profit': float(df_quality['avg_profit'].mean()),
            'total_pnl': float(total_pnl),
            # ‚¨áÔ∏è –ù–û–í–´–ï –ú–ï–¢–†–ò–ö–ò
            'total_positive_pnl': float(total_positive_pnl),
            'total_negative_pnl': float(total_negative_pnl),
            'pnl_ratio': abs(total_positive_pnl / total_negative_pnl) if total_negative_pnl != 0 else float('inf')
        }

    def _find_best_method(self, df_quality: pd.DataFrame) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –º–µ—Ç–æ–¥–∞ - –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ô –ú–ï–¢–û–î"""
        if df_quality.empty:
            return {'method': 'N/A', 'success_rate': 0.0}

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ SQL-–∑–∞–ø—Ä–æ—Å–∞
        if 'labeling_method' not in df_quality.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏–Ω–∞—á–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
            method_col = df_quality.columns[0] if len(df_quality.columns) > 0 else 'method'
        else:
            method_col = 'labeling_method'

        profitable_methods = df_quality[df_quality['profitable_signals'] > 0].copy()

        if profitable_methods.empty:
            return {'method': 'N/A', 'success_rate': 0.0}

        profitable_methods['success_rate'] = (
                profitable_methods['profitable_signals'] / profitable_methods['total_signals']
        )

        best_idx = profitable_methods['success_rate'].idxmax()
        best_method = profitable_methods.loc[best_idx]

        return {
            'method': best_method[method_col],
            'success_rate': float(best_method['success_rate']),
            'total_signals': int(best_method['total_signals']),
            'avg_profit': float(best_method['avg_profit'])
        }

    def configure_settings(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"""
        print("\n‚öôÔ∏è  –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í:")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"1. –ú–µ—Ç–æ–¥ —Ä–∞–∑–º–µ—Ç–∫–∏: {self.config.method}")
        print(f"2. Min profit target: {self.config.min_profit_target}")
        print(f"3. Hold bars: {self.config.hold_bars}")
        print("–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏")

    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É - –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"""
        stats = self.data_loader.get_data_stats()
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

    def _detect_data_quality_issues(self) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö - SQLAlchemy –≤–µ—Ä—Å–∏—è"""
        issues = []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.engine –≤–º–µ—Å—Ç–æ self.conn
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ç–æ–∫
            query_duplicates = """
                SELECT timestamp, COUNT(*) as cnt 
                FROM labeling_results 
                WHERE symbol = :symbol
                GROUP BY timestamp 
                HAVING COUNT(*) > 1
            """
            duplicates = pd.read_sql_query(
                query_duplicates,
                self.engine,
                params={'symbol': self.config.symbol}
            )
            if not duplicates.empty:
                issues.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ—Ç–æ–∫: {len(duplicates)} —Å–ª—É—á–∞–µ–≤")

            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º...
            # ...

        except Exception as err:
            issues.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞: {err}")

        return issues

    def _log_quality_analysis(self, analysis: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤"""
        logger.info("\n" + "=" * 60)
        logger.info("üìä –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê")
        logger.info("=" * 60)

        total_metrics = analysis.get('total_metrics', {})
        best_method = analysis.get('best_method', {})
        issues = analysis.get('data_quality_issues', [])

        total_pnl_value = total_metrics.get('total_pnl', 0)
        total_positive_pnl = total_metrics.get('total_positive_pnl', 0)
        total_negative_pnl = total_metrics.get('total_negative_pnl', 0)
        pnl_ratio = total_metrics.get('pnl_ratio', 0)
        avg_profit = total_metrics.get('avg_profit', 0)

        # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        logger.info(f"üìà –û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {total_metrics.get('success_rate', 0):.1%}")
        logger.info(f"üí∞ –û–±—â–∏–π PNL+: {total_positive_pnl:+.3f} ({total_positive_pnl * 100:+.1f}%)")
        logger.info(f"üí∏ –û–±—â–∏–π PNL-: {total_negative_pnl:+.3f} ({total_negative_pnl * 100:+.1f}%)")
        logger.info(f"üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ PNL: {pnl_ratio:.1f}:1" if pnl_ratio != float(
            'inf') else "üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ PNL: ‚àû (–Ω–µ—Ç —É–±—ã—Ç–∫–æ–≤)")
        logger.info(f"üèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method.get('method', 'N/A')} ({best_method.get('success_rate', 0):.1%})")
        logger.info(f"üìä –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {total_metrics.get('total_signals', 0)}")
        logger.info(f"üíµ –°—Ä–µ–¥–Ω–∏–π PnL: {avg_profit:.4f} ({avg_profit:.1%})")
        logger.info(f"üéØ –°–æ–≤–æ–∫—É–ø–Ω—ã–π PnL: {total_pnl_value:+.3f} ({total_pnl_value * 100:+.1f}%)")

        if issues:
            logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue in issues:
                logger.warning(f"   ‚Ä¢ {issue}")
        else:
            logger.info("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

        # ‚¨áÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ö–û–ù–°–û–õ–¨–ù–´–ô –í–´–í–û–î
        print(f"\nüéØ –ò–¢–û–ì–ò –ê–ù–ê–õ–ò–ó–ê:")
        print(f"   üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {total_metrics.get('success_rate', 0):.1%} —Å–¥–µ–ª–æ–∫")
        print(f"   üí∞ –ü—Ä–∏–±—ã–ª—å: {total_positive_pnl:+.3f} ({total_positive_pnl * 100:+.1f}%)")
        print(f"   üí∏ –£–±—ã—Ç–∫–∏: {total_negative_pnl:+.3f} ({total_negative_pnl * 100:+.1f}%)")
        print(f"   üìä –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å: {total_pnl_value:+.3f} ({total_pnl_value * 100:+.1f}%)")

        if pnl_ratio > 2:
            print(f"   ‚úÖ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (–æ—Ç–ª–∏—á–Ω–æ–µ)")
        elif pnl_ratio > 1:
            print(f"   ‚ö†Ô∏è  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ)")
        else:
            print(f"   ‚ùå –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {pnl_ratio:.1f}:1 (—Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è)")

    def export_feature_importance(self, *args, run_id: str | None = None, model_name: str = "unknown",
                                  top_n: int | None = None, **kwargs) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ SQLite (—Ç–∞–±–ª–∏—Ü–∞ training_feature_importance).
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫–∏–µ –≤—Ö–æ–¥—ã:
          ‚Ä¢ export_feature_importance(df, ...)            # df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['feature','importance'] –∏–ª–∏ –∏–Ω–¥–µ–∫—Å=feature
          ‚Ä¢ export_feature_importance(series, ...)        # pandas.Series: index=feature, values=importance
          ‚Ä¢ export_feature_importance(dict, ...)          # dict: feature -> importance
          ‚Ä¢ export_feature_importance(list_of_tuples, ...)# [(feature, importance), ...]
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
          run_id    ‚Äî –∫ –∫–∞–∫–æ–º—É —Å–Ω–∞–ø—à–æ—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏; –µ—Å–ª–∏ None, –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π READY –ø–æ symbol)
          model_name‚Äî –∏–º—è/–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'lgbm_v1')
          top_n     ‚Äî –ø—Ä–∏ –∑–∞–¥–∞–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ top-N –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: —á–∏—Å–ª–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫.
        """


        # 0) ensure DDL
        self._ensure_training_snapshot_tables()

        # 1) –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å run_id, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if run_id is None:
            with self.engine.begin() as conn:
                row = conn.execute(text("""
                    SELECT run_id
                      FROM training_dataset_meta
                     WHERE status='READY' AND symbol=:symbol
                  ORDER BY created_at DESC
                     LIMIT 1
                """), {"symbol": self.config.symbol}).mappings().first()
            if not row:
                raise RuntimeError("–ù–µ—Ç –≥–æ—Ç–æ–≤–æ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞ (status=READY). –£–∫–∞–∂–∏—Ç–µ run_id –≤—Ä—É—á–Ω—É—é.")
            run_id = row["run_id"]

        # 2) –∏–∑–≤–ª–µ—á—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ importance
        importance_obj = None
        if args:
            importance_obj = args[0]
        elif "importance" in kwargs:
            importance_obj = kwargs["importance"]
        elif "df" in kwargs:
            importance_obj = kwargs["df"]

        if importance_obj is None:
            raise ValueError("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á (df/series/dict/list).")

        # 3) –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–∞ -> DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['feature','importance']
        if isinstance(importance_obj, pd.DataFrame):
            df_imp = importance_obj.copy()
            # –¥–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω
            if "feature" not in df_imp.columns or "importance" not in df_imp.columns:
                if df_imp.shape[1] == 1:  # –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–µ–π, index = feature
                    df_imp = df_imp.reset_index()
                    df_imp.columns = ["feature", "importance"]
                elif df_imp.shape[1] >= 2:
                    # –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–∞–∫ feature/importance
                    cols = list(df_imp.columns)
                    df_imp = df_imp[[cols[0], cols[1]]].copy()
                    df_imp.columns = ["feature", "importance"]
        elif hasattr(importance_obj, "to_frame"):  # Series
            s = importance_obj
            df_imp = s.to_frame(name="importance").reset_index()
            if df_imp.columns[0] != "feature":
                df_imp.columns = ["feature", "importance"]
        elif isinstance(importance_obj, dict):
            df_imp = pd.DataFrame(list(importance_obj.items()), columns=["feature", "importance"])
        elif isinstance(importance_obj, (list, tuple)):
            df_imp = pd.DataFrame(importance_obj, columns=["feature", "importance"])
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≤—Ö–æ–¥–∞ –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á: {type(importance_obj)}")

        # —É–±—Ä–∞—Ç—å NaN/inf –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ —Ç–∏–ø—ã
        df_imp = df_imp.dropna(subset=["feature"]).copy()
        df_imp["importance"] = pd.to_numeric(df_imp["importance"], errors="coerce")
        df_imp = df_imp.replace([np.inf, -np.inf], np.nan).dropna(subset=["importance"])

        if df_imp.empty:
            raise ValueError("–¢–∞–±–ª–∏—Ü–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á –ø—É—Å—Ç–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")

        # –Ω–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Å—É–º–º—É=1 (–æ–±—â–µ–ø—Ä–∏–Ω—è—Ç–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏)
        ssum = float(df_imp["importance"].sum())
        if ssum > 0:
            df_imp["importance"] = df_imp["importance"] / ssum

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ç–æ–ø-N
        df_imp = df_imp.sort_values("importance", ascending=False).reset_index(drop=True)
        if top_n is not None and top_n > 0:
            df_imp = df_imp.head(int(top_n))

        df_imp["rank"] = np.arange(1, len(df_imp) + 1, dtype=int)
        created_at = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")
        df_imp["run_id"] = run_id
        df_imp["model_name"] = str(model_name)
        df_imp["created_at"] = created_at

        # –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
        out_cols = ["run_id", "model_name", "feature", "importance", "rank", "created_at"]
        df_out = df_imp[out_cols].copy()

        # 4) —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î (upsert –ø–æ PRIMARY KEY)
        inserted = 0
        with self.engine.begin() as conn:
            # —É–¥–∞–ª–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è (run_id, model_name), —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ç—å —Ä—è–¥—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º —ç–∫—Å–ø–æ—Ä—Ç–µ
            conn.execute(text("""
                DELETE FROM training_feature_importance
                 WHERE run_id=:rid AND model_name=:mname
            """), {"rid": run_id, "mname": model_name})

            df_out.to_sql("training_feature_importance", self.engine, if_exists="append", index=False)
            inserted = len(df_out)

        logger.info("‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: run_id=%s, model=%s, rows=%d", run_id, model_name, inserted)
        # –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ ‚Äî –ø–µ—á–∞—Ç—å top-10
        preview = df_out.sort_values("rank").head(10)
        print("\nüè∑Ô∏è TOP –≤–∞–∂–Ω–æ—Å—Ç–µ–π (–¥–æ 10 —Å—Ç—Ä–æ–∫):")
        for _, r in preview.iterrows():
            print(f"  {r['rank']:>2}. {r['feature']:<30} {r['importance']:.4f}")

        return inserted

    def cross_validation_split(self, n_splits: int = 5, test_size: float = 0.2) -> Dict[str, Any]:
        """
        –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤.
        –î–∞—Ç–∞—Å–µ—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏ —Å–Ω–∞–ø—à–æ—Ç–µ:
          - –º–µ—Ç–∫–∏ –∏–∑ labeling_results (reversal_label ‚àà {0,1,2})
          - —Ñ–∏—á–∏ –∏–∑ candles_* (load_indicators)
          - –∂—ë—Å—Ç–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (_validate_snapshot_frame)
        –†–∞–∑–±–∏–µ–Ω–∏–µ: time-based, –±–µ–∑ —É—Ç–µ—á–∫–∏ –±—É–¥—É—â–µ–≥–æ.
        """
        import pandas as pd
        from collections import Counter
        import traceback
        logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ {n_splits}-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏...")

        try:
            # 1) –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫–∞–∫ –≤ —Å–Ω–∞–ø—à–æ—Ç–µ
            raw_df, _meta = self._build_training_snapshot_dataframe()

            # 2) –ñ—ë—Å—Ç–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è/–æ—á–∏—Å—Ç–∫–∞
            df_clean, issues, nan_drop_rows, duplicates_count = self._validate_snapshot_frame(raw_df)
            if df_clean.empty:
                raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

            dataset = df_clean.reset_index(drop=True)

            # 3) –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            if "ts" in dataset.columns:
                try:
                    # –ü—Ä–æ–±—É–µ–º –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Binance)
                    dataset["datetime"] = pd.to_datetime(dataset["ts"], unit="ms", utc=True)
                except (pd.errors.OutOfBoundsDatetime, OverflowError):
                    try:
                        # –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ - –ø—Ä–æ–±—É–µ–º —Å–µ–∫—É–Ω–¥—ã
                        dataset["datetime"] = pd.to_datetime(dataset["ts"], unit="s", utc=True)
                    except (pd.errors.OutOfBoundsDatetime, OverflowError):
                        # –ö—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞
                        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ts, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é —à–∫–∞–ª—É")
                        dataset["datetime"] = pd.date_range(start="2020-01-01", periods=len(dataset), freq="5min",
                                                            tz="UTC")
            elif "datetime" not in dataset.columns:
                dataset["datetime"] = pd.date_range(start="2020-01-01", periods=len(dataset), freq="5min", tz="UTC")
            # 4) –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            dataset = dataset.sort_values("datetime").reset_index(drop=True)

            total_samples = len(dataset)
            if not (0 < test_size < 1):
                raise ValueError("test_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ (0,1)")
            test_samples = max(1, int(total_samples * test_size))

            # sanity-check –Ω–∞ –æ–±—ä—ë–º
            if total_samples < max(n_splits * 10, n_splits + test_samples):
                raise ValueError(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {total_samples} samples –¥–ª—è {n_splits} —Ñ–æ–ª–¥–æ–≤ (test_size={test_size})")

            logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è CV: {total_samples} —Å—Ç—Ä–æ–∫")

            # 5) –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã (blocked, –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è)
            splits = []
            # —à–∞–≥ —Å–º–µ—â–µ–Ω–∏—è –æ–∫–Ω–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –º—ã –ø–æ–ª—É—á–∏–ª–∏ n_splits —Ñ–æ–ª–¥–æ–≤
            step = (total_samples - test_samples) // n_splits
            step = max(step, 1)

            for i in range(n_splits):
                start_idx = i * step
                end_idx = start_idx + test_samples
                if end_idx > total_samples:
                    end_idx = total_samples
                    start_idx = max(0, end_idx - test_samples)

                test_indices = list(range(start_idx, end_idx))
                train_indices = list(range(0, start_idx)) + list(range(end_idx, total_samples))

                # –Ω–∞ –≤—Å—è–∫–∏–π: –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –∏ –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                if len(set(train_indices).intersection(test_indices)) != 0:
                    logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ñ–æ–ª–¥–µ {i + 1}")
                if len(set(train_indices + test_indices)) != total_samples:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ñ–æ–ª–¥–µ {i + 1}")

                split_info = {
                    "fold": i + 1,
                    "train_indices": train_indices,
                    "test_indices": test_indices,
                    "train_size": len(train_indices),
                    "test_size": len(test_indices),
                    "train_period": (
                        f"{dataset.iloc[train_indices[0]]['datetime']} ‚Üí {dataset.iloc[train_indices[-1]]['datetime']}"
                        if train_indices else "N/A"
                    ),
                    "test_period": (
                        f"{dataset.iloc[test_indices[0]]['datetime']} ‚Üí {dataset.iloc[test_indices[-1]]['datetime']}"
                        if test_indices else "N/A"
                    ),
                }
                splits.append(split_info)

            result = {
                "n_splits": n_splits,
                "test_size": test_size,
                "total_samples": total_samples,
                "splits": splits,
                "class_distribution": dict(Counter(dataset["reversal_label"])),
            }

            logger.info(f"‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {n_splits} —Ñ–æ–ª–¥–æ–≤, {total_samples} samples")
            print("\n‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –í—Å–µ–≥–æ samples: {total_samples}")
            print(f"üéØ –§–æ–ª–¥–æ–≤: {n_splits}")
            print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {result['class_distribution']}")
            return result

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {err}")
            logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            return {}

    def detect_label_leakage(self) -> Dict[str, Any]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ç–µ—á–∫–∏ –º–µ—Ç–æ–∫ ‚Äî —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        """
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –º–µ—Ç–æ–∫...")

        duplicate_pairs = []
        high_corr_features = []
        issues = []

        try:
            df_positives = self.data_loader.load_labeled_data()

            if df_positives.empty:
                return {
                    'leakage_detected': False,
                    'issues': ['–ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'],
                    'high_corr_features': [],
                    'duplicate_feature_pairs': [],
                    'total_positives': 0
                }

            logger.info(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(df_positives)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

            # –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
            print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–¢–û–ö:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ –º–µ—Ç–æ–∫: {len(df_positives)}")
            print(f"   ‚Ä¢ BUY (1): {len(df_positives[df_positives['reversal_label'] == 1])}")
            print(f"   ‚Ä¢ SELL (2): {len(df_positives[df_positives['reversal_label'] == 2])}")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º —Ä–∞–∑–º–µ—Ç–∫–∏
            if 'method' in df_positives.columns:
                method_stats = df_positives['method'].value_counts()
                print(f"   ‚Ä¢ –ü–æ –º–µ—Ç–æ–¥–∞–º: {method_stats.to_dict()}")

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫
            if 'pnl' in df_positives.columns:
                avg_pnl = df_positives['pnl'].mean()
                profitable = len(df_positives[df_positives['pnl'] > 0])
                success_rate = profitable / len(df_positives)
                print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} ({profitable}/{len(df_positives)})")
                print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {avg_pnl:.4f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ñ–∏—á —Å –º–µ—Ç–∫–æ–π
            available_features = [col for col in self.data_loader.feature_names if col in df_positives.columns]

            print(f"\nüîç –ê–ù–ê–õ–ò–ó –§–ò–ß ({len(available_features)} –¥–æ—Å—Ç—É–ø–Ω–æ):")

            feature_correlations = []
            for feature in available_features:
                if df_positives[feature].dtype not in [np.float64, np.int64]:
                    continue

                clean_data = df_positives[[feature, 'reversal_label']].dropna()
                if len(clean_data) < 10:  # –ú–∏–Ω–∏–º—É–º 10 samples
                    continue

                try:
                    correlation = clean_data[feature].corr(clean_data['reversal_label'])
                    if not np.isnan(correlation):
                        feature_correlations.append((feature, abs(correlation)))

                        if abs(correlation) > 0.8:
                            high_corr_features.append((feature, correlation))
                except:
                    continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏—á–∏ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            feature_correlations.sort(key=lambda x: x[1], reverse=True)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á
            if feature_correlations:
                print("   üèÜ –¢–æ–ø-5 —Ñ–∏—á –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –º–µ—Ç–∫–æ–π:")
                for feature, corr in feature_correlations[:5]:
                    leak_warning = " ‚ö†Ô∏è –£–¢–ï–ß–ö–ê!" if abs(corr) > 0.8 else ""
                    print(f"      ‚Ä¢ {feature}: {corr:.4f}{leak_warning}")

            if high_corr_features:
                issues.append(f"–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –º–µ—Ç–∫–æ–π: {len(high_corr_features)} —Ñ–∏—á")
                for feature, corr in high_corr_features:
                    logger.warning(f"   ‚ö†Ô∏è {feature}: {corr:.4f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ–∏—á–∏
            if len(available_features) > 1:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏
                    numeric_features = [f for f in available_features
                                        if df_positives[f].dtype in [np.float64, np.int64]]

                    if len(numeric_features) > 1:
                        corr_matrix = self.data_loader.safe_correlation_calculation(df_positives, numeric_features)

                        for i in range(len(corr_matrix.columns)):
                            for j in range(i + 1, len(corr_matrix.columns)):
                                if corr_matrix.iloc[i, j] > 0.95:
                                    duplicate_pairs.append((
                                        corr_matrix.columns[i],
                                        corr_matrix.columns[j],
                                        corr_matrix.iloc[i, j]
                                    ))

                        if duplicate_pairs:
                            issues.append(f"–î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ–∏—á–∏: {len(duplicate_pairs)} –ø–∞—Ä")
                            print(f"\nüîÅ –î–£–ë–õ–ò–†–£–Æ–©–ò–ï–°–Ø –§–ò–ß–ò:")
                            for pair in duplicate_pairs[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                                print(f"   ‚Ä¢ {pair[0]} ‚âà {pair[1]} (corr={pair[2]:.3f})")
                except Exception as corr_err:
                    issues.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {corr_err}")

            leakage_detected = len(high_corr_features) > 0

            result = {
                'leakage_detected': leakage_detected,
                'issues': issues,
                'high_corr_features': high_corr_features,
                'duplicate_feature_pairs': duplicate_pairs,
                'total_positives': len(df_positives),
                'available_features': len(available_features),
                'feature_correlations': feature_correlations[:10]  # –¢–æ–ø-10 —Ñ–∏—á
            }

            if leakage_detected:
                logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫!")
                print("\n‚ùå –í–´–í–û–î: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —É—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫!")
            else:
                logger.info("‚úÖ –£—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
                print("\n‚úÖ –í–´–í–û–î: –£—Ç–µ—á–∫–∞ –º–µ—Ç–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")

            return result

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Ç–µ—á–∫–∏: {err}")
            return {
                'leakage_detected': False,
                'issues': [f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {err}'],
                'high_corr_features': [],
                'duplicate_feature_pairs': [],
                'total_positives': 0
            }

    # =========================================================================
    # –°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ú–ï–¢–û–î–´ –ò–ó –ò–°–•–û–î–ù–û–ì–û –ö–û–î–ê (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    # =========================================================================

    def save_to_db(self, results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –û–®–ò–ë–û–ö"""
        from sqlalchemy import text

        insert_sql = text("""
            INSERT OR REPLACE INTO labeling_results 
            (symbol, timestamp, timeframe, reversal_label, reversal_confidence,
             labeling_method, labeling_params, extreme_index, extreme_price, extreme_timestamp,
             confirmation_index, confirmation_timestamp, price_change_after, features_json,
             is_high_quality, created_at)
            VALUES (:symbol, :timestamp, :timeframe, :reversal_label, :reversal_confidence,
                    :labeling_method, :labeling_params, :extreme_index, :extreme_price, :extreme_timestamp,
                    :confirmation_index, :confirmation_timestamp, :price_change_after, :features_json,
                    :is_high_quality, CURRENT_TIMESTAMP)
        """)

        successful_saves = 0
        with self.engine.connect() as conn:
            for res in results:
                try:
                    #   –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —á—Ç–æ timestamp –±—É–¥—É—Ç INTEGER
                    timestamp = int(res.get('timestamp', 0))
                    extreme_timestamp = int(res.get('extreme_timestamp', 0))
                    confirmation_timestamp = int(res.get('confirmation_timestamp', 0))

                    #   –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                    if timestamp == 0 or extreme_timestamp == 0:
                        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ timestamp: {res.get('symbol')}")
                        continue

                    conn.execute(insert_sql, {
                        'symbol': res['symbol'],
                        'timestamp': timestamp,
                        'timeframe': res['timeframe'],
                        'reversal_label': res['reversal_label'],
                        'reversal_confidence': res.get('reversal_confidence', 1.0),
                        'labeling_method': res['labeling_method'],
                        'labeling_params': json.dumps(res.get('labeling_params', {})),
                        'extreme_index': res.get('extreme_index'),
                        'extreme_price': res.get('extreme_price'),
                        'extreme_timestamp': extreme_timestamp,
                        'confirmation_index': res.get('confirmation_index'),
                        'confirmation_timestamp': confirmation_timestamp,
                        'price_change_after': res.get('price_change_after'),
                        'features_json': res.get('features_json'),
                        'is_high_quality': res.get('is_high_quality', 1)
                    })
                    successful_saves += 1

                except Exception as row_err:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ {res.get('symbol', 'N/A')}: {row_err}")
                    continue

            conn.commit()

        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {successful_saves}/{len(results)} –º–µ—Ç–æ–∫ —á–µ—Ä–µ–∑ SQLAlchemy")
        if successful_saves < len(results):
            logger.warning(f"‚ö†Ô∏è –ù–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(results) - successful_saves} –º–µ—Ç–æ–∫ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")

    def manual_mode(self):
        """–†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        df = self.load_data()
        print("\n=== –†–£–ß–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê ===")
        print("–§–æ—Ä–º–∞—Ç: <–∏–Ω–¥–µ–∫—Å>,<BUY/SELL>. 'done' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")

        while True:
            user_input = input(">> ").strip()
            if user_input.lower() == 'done':
                break
            if ',' not in user_input:
                print("–§–æ—Ä–º–∞—Ç: –∏–Ω–¥–µ–∫—Å,—Ç–∏–ø")
                continue
            try:
                idx_str, typ = user_input.split(',')
                idx = int(idx_str.strip())
                typ = typ.strip().upper()
                if typ not in ['BUY', 'SELL']:
                    print("–¢–æ–ª—å–∫–æ BUY –∏–ª–∏ SELL")
                    continue
                if idx < 0 or idx >= len(df):
                    print(f"0 ‚â§ –∏–Ω–¥–µ–∫—Å < {len(df)}")
                    continue

                exit_idx = min(idx + self.config.hold_bars, len(df) - 1)
                pnl, is_profitable = self._calculate_pnl_to_index(df, idx, typ, exit_idx)
                print(f"PnL: {pnl:.4f} (—Ç—Ä–µ–±—É–µ—Ç—Å—è: ‚â•{self.config.min_profit_target:.4f})")

                if not is_profitable:
                    print(f"‚ö†Ô∏è  –ú–µ—Ç–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ profit target! –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å? (y/n)")
                    confirm = input(">> ").strip().lower()
                    if confirm != 'y':
                        continue

                label = 1 if typ == 'BUY' else 2

                row_dict = df.iloc[idx].to_dict()
                for k, v in row_dict.items():
                    if pd.isna(v):
                        row_dict[k] = None
                    elif isinstance(v, pd.Timestamp):
                        row_dict[k] = v.isoformat()
                    elif isinstance(v, (np.integer, np.int64)):
                        row_dict[k] = int(v)
                    elif isinstance(v, (np.floating, np.float64)):
                        row_dict[k] = float(v)
                    elif isinstance(v, str):
                        row_dict[k] = v

                timestamp = int(df['ts'].iloc[idx])

                result = {
                    'symbol': self.config.symbol,
                    'timestamp': timestamp,  # ‚Üê INTEGER
                    'timeframe': self.config.timeframe,
                    'reversal_label': label,
                    'reversal_confidence': 1.0,
                    'labeling_method': 'MANUAL',
                    'extreme_index': idx,
                    'extreme_price': df['close'].iloc[idx],
                    'extreme_timestamp': timestamp,  # ‚Üê INTEGER
                    'confirmation_index': idx,
                    'confirmation_timestamp': timestamp,  # ‚Üê INTEGER
                    'price_change_after': pnl,
                    'features_json': json.dumps(row_dict),
                    'is_high_quality': 1 if is_profitable else 0
                }
                self.save_to_db([result])
                self.labels.append({'index': idx, 'type': typ, 'pnl': pnl})
                print(f"‚úÖ –ú–µ—Ç–∫–∞ {typ} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (PnL: {pnl:.4f})")
            except Exception as err:
                print(f"–û—à–∏–±–∫–∞: {err}")

    def analyze_pnl_distribution(self, method: str = None):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL —á–µ—Ä–µ–∑ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
        """
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
        except ImportError:
            print("‚ùå –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ PnL —Ç—Ä–µ–±—É–µ—Ç—Å—è matplotlib –∏ seaborn")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install matplotlib seaborn")
            return

        logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL...")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            query = """
                SELECT labeling_method, price_change_after as pnl, reversal_label
                FROM labeling_results 
                WHERE symbol = :symbol
            """

            if method:
                query += " AND labeling_method = :method"
                params = {'symbol': self.config.symbol, 'method': method}
            else:
                params = {'symbol': self.config.symbol}

            df_pnl = pd.read_sql_query(query, self.engine, params=params)

            if df_pnl.empty:
                logger.warning("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PnL")
                return

            print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê PnL:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df_pnl)}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {df_pnl['pnl'].mean():.4f}")
            print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ PnL: {df_pnl['pnl'].median():.4f}")
            print(f"   ‚Ä¢ Std PnL: {df_pnl['pnl'].std():.4f}")
            print(f"   ‚Ä¢ Min PnL: {df_pnl['pnl'].min():.4f}")
            print(f"   ‚Ä¢ Max PnL: {df_pnl['pnl'].max():.4f}")
            print(f"   ‚Ä¢ PnL > 0: {(df_pnl['pnl'] > 0).sum()} ({(df_pnl['pnl'] > 0).mean():.1%})")
            print(f"   ‚Ä¢ PnL < 0: {(df_pnl['pnl'] < 0).sum()} ({(df_pnl['pnl'] < 0).mean():.1%})")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º
            if 'labeling_method' in df_pnl.columns:
                print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ú–ï–¢–û–î–ê–ú:")
                method_stats = df_pnl.groupby('labeling_method').agg({
                    'pnl': ['count', 'mean', 'std', 'min', 'max'],
                }).round(4)
                print(method_stats)

            # –°–æ–∑–¥–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
            plt.figure(figsize=(12, 8))

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ 1: –û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            plt.subplot(2, 2, 1)
            plt.hist(df_pnl['pnl'], bins=50, alpha=0.7, edgecolor='black')
            plt.axvline(df_pnl['pnl'].mean(), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df_pnl["pnl"].mean():.4f}')
            plt.axvline(0, color='green', linestyle='-', label='–ù—É–ª–µ–≤–∞—è –æ—Ç–º–µ—Ç–∫–∞')
            plt.xlabel('PnL')
            plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ PnL (–≤—Å–µ –º–µ—Ç–æ–¥—ã)')
            plt.legend()
            plt.grid(True, alpha=0.3)

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ 2: –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ PnL
            plt.subplot(2, 2, 2)
            positive_pnl = df_pnl[df_pnl['pnl'] > 0]['pnl']
            if len(positive_pnl) > 0:
                plt.hist(positive_pnl, bins=30, alpha=0.7, color='green', edgecolor='black')
                plt.axvline(positive_pnl.mean(), color='red', linestyle='--',
                            label=f'–°—Ä–µ–¥–Ω–µ–µ: {positive_pnl.mean():.4f}')
                plt.xlabel('PnL (> 0)')
                plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö PnL')
                plt.legend()
                plt.grid(True, alpha=0.3)

            # Boxplot –ø–æ –º–µ—Ç–æ–¥–∞–º
            plt.subplot(2, 2, 3)
            if 'labeling_method' in df_pnl.columns and df_pnl['labeling_method'].nunique() > 1:
                df_pnl.boxplot(column='pnl', by='labeling_method', ax=plt.gca())
                plt.title('PnL –ø–æ –º–µ—Ç–æ–¥–∞–º —Ä–∞–∑–º–µ—Ç–∫–∏')
                plt.suptitle('')  # –£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                plt.xticks(rotation=45)

            # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            plt.subplot(2, 2, 4)
            sorted_pnl = np.sort(df_pnl['pnl'])
            cumulative = np.arange(1, len(sorted_pnl) + 1) / len(sorted_pnl)
            plt.plot(sorted_pnl, cumulative, linewidth=2)
            plt.axvline(0, color='green', linestyle='--', alpha=0.7, label='–ù—É–ª–µ–≤–∞—è –æ—Ç–º–µ—Ç–∫–∞')
            plt.xlabel('PnL')
            plt.ylabel('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
            plt.title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ PnL')
            plt.grid(True, alpha=0.3)
            plt.legend()

            plt.tight_layout()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pnl_analysis_{self.config.symbol}_{timestamp}.png"
            plt.savefig(filename, dpi=150, bbox_inches='tight')
            plt.show()

            print(f"\nüíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")

            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
            self._analyze_pnl_outliers(df_pnl)

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ PnL: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    def _analyze_pnl_outliers(self, df_pnl: pd.DataFrame):
        """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ PnL"""
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í PnL:")

        Q1 = df_pnl['pnl'].quantile(0.25)
        Q3 = df_pnl['pnl'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df_pnl[(df_pnl['pnl'] < lower_bound) | (df_pnl['pnl'] > upper_bound)]

        print(f"   ‚Ä¢ –í—ã–±—Ä–æ—Å—ã (IQR –º–µ—Ç–æ–¥): {len(outliers)} —Å–∏–≥–Ω–∞–ª–æ–≤")
        print(f"   ‚Ä¢ –ì—Ä–∞–Ω–∏—Ü—ã –≤—ã–±—Ä–æ—Å–æ–≤: [{lower_bound:.4f}, {upper_bound:.4f}]")

        if not outliers.empty and 'labeling_method' in outliers.columns:
            print(f"   ‚Ä¢ –í—ã–±—Ä–æ—Å—ã –ø–æ –º–µ—Ç–æ–¥–∞–º:")
            outlier_methods = outliers['labeling_method'].value_counts()
            for method, count in outlier_methods.items():
                print(f"      - {method}: {count} –≤—ã–±—Ä–æ—Å–æ–≤")

        # –ê–Ω–∞–ª–∏–∑ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if len(df_pnl) > 0:
            extreme_positive = df_pnl.nlargest(5, 'pnl')
            extreme_negative = df_pnl.nsmallest(5, 'pnl')

            print(f"\nüìà –¢–û–ü-5 —Å–∞–º—ã—Ö –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:")
            for _, row in extreme_positive.iterrows():
                method = row.get('labeling_method', 'N/A')
                print(f"   ‚Ä¢ {method}: {row['pnl']:.4f}")

            print(f"\nüìâ –¢–û–ü-5 —Å–∞–º—ã—Ö —É–±—ã—Ç–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:")
            for _, row in extreme_negative.iterrows():
                method = row.get('labeling_method', 'N/A')
                print(f"   ‚Ä¢ {method}: {row['pnl']:.4f}")

    def quick_pnl_analysis(self):
        """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ PnL –±–µ–∑ –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        query = """
            SELECT labeling_method, price_change_after as pnl 
            FROM labeling_results 
            WHERE symbol = :symbol
        """

        try:
            df_pnl = pd.read_sql_query(query, self.engine, params={'symbol': self.config.symbol})

            if df_pnl.empty:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return

            print(f"\nüìä –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó PnL:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df_pnl)}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π PnL: {df_pnl['pnl'].mean():.4f}")
            print(f"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ PnL: {df_pnl['pnl'].median():.4f}")
            print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {(df_pnl['pnl'] > 0).mean():.1%}")

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–µ—Ç–æ–¥–∞–º
            if 'labeling_method' in df_pnl.columns:
                print(f"\nüìà –ü–û –ú–ï–¢–û–î–ê–ú:")
                for method in df_pnl['labeling_method'].unique():
                    method_data = df_pnl[df_pnl['labeling_method'] == method]
                    success_rate = (method_data['pnl'] > 0).mean()
                    avg_pnl = method_data['pnl'].mean()
                    print(
                        f"   ‚Ä¢ {method}: {len(method_data)} —Å–∏–≥–Ω–∞–ª–æ–≤, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%}, —Å—Ä–µ–¥–Ω–∏–π PnL: {avg_pnl:.4f}")

        except Exception as err:
            print(f"‚ùå –û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ PnL: {err}")

    def clear_labeling_table(self, confirmation_required: bool = True):
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–∞–∑–º–µ—Ç–∫–∞–º–∏
        """
        try:
            if confirmation_required:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π
                stats_query = """
                    SELECT labeling_method, COUNT(*) as count 
                    FROM labeling_results 
                    WHERE symbol = :symbol 
                    GROUP BY labeling_method
                """
                stats = pd.read_sql_query(stats_query, self.engine, params={'symbol': self.config.symbol})

                if stats.empty:
                    print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ labeling_results —É–∂–µ –ø—É—Å—Ç–∞")
                    return

                print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë–£–î–ï–¢ –£–î–ê–õ–ï–ù–û –í–°–ï–• –ú–ï–¢–û–ö!")
                print(f"üìä –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {self.config.symbol}:")
                total_count = 0
                for _, row in stats.iterrows():
                    print(f"   ‚Ä¢ {row['labeling_method']}: {row['count']} –º–µ—Ç–æ–∫")
                    total_count += row['count']
                print(f"   ‚Ä¢ –í–°–ï–ì–û: {total_count} –º–µ—Ç–æ–∫")

                confirm = input(f"\n‚ùì –í—ã —É–≤–µ—Ä–µ–Ω—ã? –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–æ–±—Ä–∞—Ç–∏–º–æ! (y/N): ").strip().lower()
                if confirm != 'y':
                    print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
                    return

            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É
            delete_query = "DELETE FROM labeling_results WHERE symbol = :symbol"
            with self.engine.connect() as conn:
                result = conn.execute(text(delete_query), {'symbol': self.config.symbol})
                conn.commit()

            deleted_count = result.rowcount
            logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {deleted_count} –º–µ—Ç–æ–∫ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.config.symbol}")
            print(f"‚úÖ –û—á–∏—â–µ–Ω–æ {deleted_count} –º–µ—Ç–æ–∫")

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    def clear_all_labeling_tables(self):
        """
        –û—á–∏—Å—Ç–∫–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ç–∫–∏ (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã)
        """
        try:
            print(f"\n‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –í–ù–ò–ú–ê–ù–ò–ï!")
            print(f"   –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–∞ –í–°–Ø —Ç–∞–±–ª–∏—Ü–∞ labeling_results!")
            print(f"   –í—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –±—É–¥—É—Ç –ø–æ—Ç–µ—Ä—è–Ω—ã!")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats_query = "SELECT COUNT(*) as total_count FROM labeling_results"
            total_count = pd.read_sql_query(stats_query, self.engine).iloc[0]['total_count']
            print(f"   –í—Å–µ–≥–æ –º–µ—Ç–æ–∫ –≤ –±–∞–∑–µ: {total_count}")

            confirm1 = input(f"\n‚ùì –í—ã –∞–±—Å–æ–ª—é—Ç–Ω–æ —É–≤–µ—Ä–µ–Ω—ã? (yes/NO): ").strip().lower()
            if confirm1 != 'yes':
                print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
                return

            # –û—á–∏—â–∞–µ–º –≤—Å—é —Ç–∞–±–ª–∏—Ü—É
            delete_query = "DELETE FROM labeling_results"
            with self.engine.connect() as conn:
                result = conn.execute(text(delete_query))
                conn.commit()

            deleted_count = result.rowcount
            logger.info(f"üßπ –û—á–∏—â–µ–Ω–∞ –≤—Å—è —Ç–∞–±–ª–∏—Ü–∞ labeling_results: {deleted_count} –º–µ—Ç–æ–∫")
            print(f"‚úÖ –û—á–∏—â–µ–Ω–∞ –≤—Å—è —Ç–∞–±–ª–∏—Ü–∞: {deleted_count} –º–µ—Ç–æ–∫")

        except Exception as err:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {err}")
            print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

    def mark_unprofitable_ranges_as_negatives(self) -> int:
        """
        –ü–æ–º–µ—á–∞–µ—Ç —É–±—ã—Ç–æ—á–Ω—ã–µ BUY/SELL (price_change_after < threshold) –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç HOLD-—Ç–æ—á–∫–∏ –≤ –æ–∫–Ω–µ [—Ç–µ–∫—É—â–∏–π —Å–∏–≥–Ω–∞–ª; —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª):
          ‚Ä¢ –û–±–Ω—É–ª—è–µ—Ç price_change_after —É ¬´–ª—É–∑–µ—Ä–æ–≤¬ª –¥–æ 0.0 (–∫–∞–∫ —Ñ–ª–∞–≥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª–∞)
          ‚Ä¢ –í—Å—Ç–∞–≤–ª—è–µ—Ç HOLD-END (label=0) –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º –±–∞—Ä–µ –ü–ï–†–ï–î —Å–ª–µ–¥—É—é—â–∏–º —Å–∏–≥–Ω–∞–ª–æ–º
          ‚Ä¢ –í—Å—Ç–∞–≤–ª—è–µ—Ç HOLD-MID (label=0) –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –æ–∫–Ω–∞, –µ—Å–ª–∏ –æ–∫–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ –∏ –≤—ã–¥–µ—Ä–∂–∞–Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –¥–æ –∫–æ–Ω—Ü–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö ¬´–ª—É–∑–µ—Ä–æ–≤¬ª.
        """

        import pandas as pd
        from sqlalchemy import text

        # ------------------------ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏) ------------------------
        thr = float(getattr(self.config, "min_profit_target", 0.001))  # –ø–æ—Ä–æ–≥ —É–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏
        symbol = self.config.symbol
        tf = self.config.timeframe
        candles_table = f"candles_{tf}"

        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–∫–Ω–∞ (–≤ –±–∞—Ä–∞—Ö) –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ HOLD (MID)
        min_window_bars = int(getattr(self.config, "hold_min_window_bars", 6))
        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è (–≤ –±–∞—Ä–∞—Ö) –º–µ–∂–¥—É HOLD-MID –∏ HOLD-END
        min_mid_end_gap = int(getattr(self.config, "hold_min_mid_end_gap", 3))
        # —Å–º–µ—â–µ–Ω–∏—è –æ—Ç –∫—Ä–∞—ë–≤ –æ–∫–Ω–∞, –≤ –∫–æ—Ç–æ—Ä—ã—Ö MID –Ω–µ —Å—Ç–∞–≤–∏–º (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–∏–ª–∏–ø–∞–ª –∫ –∫—Ä–∞—è–º)
        margin_left = int(getattr(self.config, "hold_mid_margin_left", 1))
        margin_right = int(getattr(self.config, "hold_mid_margin_right", 1))

        logger.info(f"üîß –ü–æ–∏—Å–∫ —É–±—ã—Ç–æ—á–Ω—ã—Ö –º–µ—Ç–æ–∫: pnl < {thr} | {symbol} {tf} | MID+END HOLD")

        updated_count = 0
        inserted_rows = []

        with self.engine.begin() as conn:
            # 1) –£–±—ã—Ç–æ—á–Ω—ã–µ BUY/SELL –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞/TF
            losers = pd.read_sql(
                text("""
                    SELECT reversal_label, extreme_timestamp, price_change_after
                      FROM labeling_results
                     WHERE symbol=:symbol AND timeframe=:tf
                       AND reversal_label IN (1,2)      -- BUY/SELL
                       AND price_change_after < :thr
                """),
                conn,
                params={"symbol": symbol, "tf": tf, "thr": thr},
            )

            if losers.empty:
                logger.info(f"‚ÑπÔ∏è –ù–µ—Ç —É–±—ã—Ç–æ—á–Ω—ã—Ö –º–µ—Ç–æ–∫ (pnl<{thr})")
                return 0

            # 2) –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ HOLD (–¥–ª—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
            existing_holds = set()
            res = conn.execute(
                text("""
                    SELECT extreme_timestamp
                      FROM labeling_results
                     WHERE symbol=:symbol AND timeframe=:tf
                       AND reversal_label=0
                """),
                {"symbol": symbol, "tf": tf},
            )
            for r in res:
                existing_holds.add(int(r[0]))

            new_holds_in_batch = set()

            # 3) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ ¬´–ª—É–∑–µ—Ä–∞¬ª
            for _, row in losers.iterrows():
                ts_cur = int(row.extreme_timestamp)

                # 3.1) –û–±–Ω—É–ª—è–µ–º PnL —É —Å–∞–º–æ–≥–æ –ª—É–∑–µ—Ä–∞ (–∫–∞–∫ —Ñ–ª–∞–≥, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ)
                conn.execute(
                    text("""
                        UPDATE labeling_results
                           SET price_change_after = 0.0
                         WHERE symbol=:symbol AND timeframe=:tf
                           AND extreme_timestamp=:ts
                    """),
                    {"symbol": symbol, "tf": tf, "ts": ts_cur},
                )
                updated_count += 1

                # 3.2) –°–ª–µ–¥—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª (–≥—Ä–∞–Ω–∏—Ü–∞ –æ–∫–Ω–∞)
                next_sig = conn.execute(
                    text("""
                        SELECT MIN(extreme_timestamp)
                          FROM labeling_results
                         WHERE symbol=:symbol AND timeframe=:tf
                           AND extreme_timestamp > :ts
                    """),
                    {"symbol": symbol, "tf": tf, "ts": ts_cur},
                ).fetchone()

                if not next_sig or not next_sig[0]:
                    # –•–≤–æ—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏ ‚Äî –±–µ–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –Ω–µ —Å—Ç–∞–≤–∏–º HOLD (—á—Ç–æ–±—ã –Ω–µ —Å–º–æ—Ç—Ä–µ—Ç—å –≤ –±—É–¥—É—â–µ–µ)
                    continue

                ts_next = int(next_sig[0])

                # 3.3) –°–ø–∏—Å–æ–∫ –±–∞—Ä–æ–≤ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º –∏ —Å–ª–µ–¥—É—é—â–∏–º —Å–∏–≥–Ω–∞–ª–æ–º (–∏—Å–∫–ª—é—á–∞—è –∫—Ä–∞–π–Ω–∏–µ)
                bars = conn.execute(
                    text(f"""
                        SELECT ts
                          FROM {candles_table}
                         WHERE symbol=:symbol
                           AND ts > :ts_cur
                           AND ts < :ts_next
                         ORDER BY ts ASC
                    """),
                    {"symbol": symbol, "ts_cur": ts_cur, "ts_next": ts_next},
                ).fetchall()

                if not bars:
                    # –ú–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π —Å–≤–µ—á–∏ ‚Äî —Å—Ç–∞–≤–∏—Ç—å HOLD –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ
                    continue

                ts_list = [int(b[0]) for b in bars]
                n = len(ts_list)

                # ------------------------ HOLD-END (–≤—Å–µ–≥–¥–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ) ------------------------
                ts_end = ts_list[-1]  # –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä –ü–ï–†–ï–î —Å–ª–µ–¥—É—é—â–∏–º —Å–∏–≥–Ω–∞–ª–æ–º

                def _try_queue_hold(ts_hold: int, method_tag: str):
                    """–õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫: –¥–æ–±–∞–≤–∏—Ç—å HOLD, –µ—Å–ª–∏ –µ–≥–æ –µ—â—ë –Ω–µ—Ç."""
                    if ts_hold in existing_holds or ts_hold in new_holds_in_batch:
                        return False
                    inserted_rows.append({
                        "symbol": symbol,
                        "timestamp": ts_hold,
                        "timeframe": tf,
                        "reversal_label": 0,  # HOLD
                        "reversal_confidence": 1.0,
                        "labeling_method": method_tag,  # 'HOLD_AFTER_LOSS_END' –∏–ª–∏ 'HOLD_AFTER_LOSS_MID'
                        "extreme_timestamp": ts_hold,  # –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ
                        "price_change_after": 0.0,
                    })
                    new_holds_in_batch.add(ts_hold)
                    return True

                _try_queue_hold(ts_end, "HOLD_AFTER_LOSS_END")

                # ------------------------ HOLD-MID (–ø–æ —É—Å–ª–æ–≤–∏—è–º) ------------------------
                # –¢—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ–∫–Ω–∞, –æ—Ç—Å—Ç—É–ø—ã –æ—Ç –∫—Ä–∞—ë–≤ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–∏—Å—Ç–∞–Ω—Ü–∏—é –¥–æ END
                if n >= min_window_bars:
                    # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä –æ–∫–Ω–∞ —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏
                    left = margin_left
                    right = n - 1 - margin_right
                    if left <= right:
                        mid_idx = (left + right) // 2  # —Å–µ—Ä–µ–¥–∏–Ω–∞ —Å —É—á—ë—Ç–æ–º margins
                        ts_mid = ts_list[mid_idx]

                        # –î–∏—Å—Ç–∞–Ω—Ü–∏—è MID ‚Üí END –≤ –±–∞—Ä–∞—Ö
                        gap = (n - 1) - mid_idx
                        if gap >= min_mid_end_gap and ts_mid != ts_end:
                            _try_queue_hold(ts_mid, "HOLD_AFTER_LOSS_MID")

            # 4) –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ HOLD
            if inserted_rows:
                pd.DataFrame(inserted_rows).to_sql(
                    "labeling_results", conn, if_exists="append", index=False
                )

        logger.info(
            f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –ª—É–∑–µ—Ä–æ–≤: {updated_count} | –¥–æ–±–∞–≤–ª–µ–Ω–æ HOLD: {len(inserted_rows)} "
            f"(END –≤—Å–µ–≥–¥–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –æ–∫–Ω–∞; MID ‚Äî –ø—Ä–∏ n>={min_window_bars} –∏ gap>={min_mid_end_gap})"
        )
        return updated_count

    def _calculate_unprofitable_hold_ranges(self, df: pd.DataFrame, signals: List[Dict]) -> List[Dict]:
        """
        –î–∏–∞–ø–∞–∑–æ–Ω—ã –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ BUY/SELL, –≥–¥–µ PnL < –ø–æ—Ä–æ–≥–∞ (–∏–ª–∏ <0 ‚Äî —Å–º. —É—Å–ª–æ–≤–∏–µ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã: {start_index, end_index, pnl, signal_type, range_length}
        """
        if not signals or len(signals) < 2:
            return []

        # ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã
        clean = []
        ts_to_idx = {int(ts): i for i, ts in enumerate(df["ts"].astype(int))}
        for s in signals:
            rl = int(s.get("reversal_label", -1))
            if rl not in (1, 2):
                continue
            idx = s.get("extreme_index")
            if idx is None or not isinstance(idx, (int, np.integer)):
                ts = s.get("extreme_timestamp")
                if ts is None or int(ts) not in ts_to_idx:
                    continue
                idx = ts_to_idx[int(ts)]
            if 0 <= idx < len(df):
                clean.append({"extreme_index": int(idx), "reversal_label": rl})

        if len(clean) < 2:
            return []

        clean.sort(key=lambda x: x["extreme_index"])
        hold_ranges = []

        for i in range(len(clean) - 1):
            start_idx = clean[i]["extreme_index"]
            raw_end = clean[i + 1]["extreme_index"]
            # —Å—á–∏—Ç–∞–µ–º PnL –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ë–ê–†–ê –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —Å–∏–≥–Ω–∞–ª–æ–º
            end_idx = max(start_idx + 1, raw_end - 1)
            if end_idx <= start_idx or end_idx >= len(df):
                continue

            signal_type = 'BUY' if clean[i]["reversal_label"] == 1 else 'SELL'
            pnl, _ = self._calculate_pnl_to_index(df, start_idx, signal_type, end_idx)

            # –≤—ã–±–µ—Ä–∏ –æ–¥–Ω–æ –∏–∑ —É—Å–ª–æ–≤–∏–π:
            # if pnl < 0:                                  # —Å—Ç—Ä–æ–≥–æ —É–±—ã—Ç–æ—á–Ω—ã–µ
            if pnl < float(self.config.min_profit_target):  # ¬´–Ω–µ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ –ø–æ –ø–æ—Ä–æ–≥—É¬ª
                hold_ranges.append({
                    "start_index": start_idx,
                    "end_index": raw_end,  # –≤–∞–∂–Ω–æ: –ø–æ–ª—É–∏–Ω—Ç–µ—Ä–≤–∞–ª [start, raw_end)
                    "pnl": float(pnl),
                    "signal_type": signal_type,
                    "range_length": raw_end - start_idx
                })

        self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(hold_ranges)} —É–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –º–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏")
        return hold_ranges

    def _run_auto_with_method(self, strategy_func, method_name: str, df: pd.DataFrame = None):
        """–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Å —Ä–∞—Å—á–µ—Ç–æ–º PnL –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏"""
        if df is None:
            df = self.load_data()
        signals = strategy_func(df)
        results = []

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π –º–µ—Ç–∫–∏
        sorted_signals = sorted(signals, key=lambda x: x['index'])

        for i, sig in enumerate(sorted_signals):
            idx = sig['index']

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –≤—ã—Ö–æ–¥–∞: –ª–∏–±–æ —Å–ª–µ–¥—É—é—â–∞—è –º–µ—Ç–∫–∞, –ª–∏–±–æ hold_bars
            if i + 1 < len(sorted_signals):
                # –ï—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª - –≤—ã—Ö–æ–¥–∏–º –Ω–∞ –±–∞—Ä–µ –ø–µ—Ä–µ–¥ –Ω–∏–º
                next_idx = sorted_signals[i + 1]['index']
                exit_idx = max(idx + 1, next_idx - 1)  # –º–∏–Ω–∏–º—É–º +1 –±–∞—Ä –æ—Ç –≤—Ö–æ–¥–∞
            else:
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª - –∏—Å–ø–æ–ª—å–∑—É–µ–º hold_bars
                exit_idx = idx + self.config.hold_bars

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü
            if exit_idx >= len(df):
                exit_idx = len(df) - 1

            if exit_idx <= idx:  # –∑–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                continue

            # –†–∞—Å—á–µ—Ç PnL –¥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤—ã—Ö–æ–¥–∞
            pnl, is_profitable = self._calculate_pnl_to_index(df, idx, sig['type'], exit_idx)

            row_dict = df.iloc[idx].to_dict()
            for k, v in row_dict.items():
                if pd.isna(v):
                    row_dict[k] = None
                elif isinstance(v, pd.Timestamp):
                    row_dict[k] = v.isoformat()
                elif isinstance(v, (np.integer, np.int64)):
                    row_dict[k] = int(v)
                elif isinstance(v, (np.floating, np.float64)):
                    row_dict[k] = float(v)
                elif isinstance(v, str):
                    row_dict[k] = v

            timestamp = int(df['ts'].iloc[idx])
            extreme_timestamp = int(sig.get('extreme_timestamp', timestamp))
            confirmation_timestamp = int(sig.get('confirmation_timestamp', timestamp))

            # —Å–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
            labeling_params = {}
            for k, v in self.config.__dict__.items():
                if k in ['db_engine', 'tool']:  # –∏—Å–∫–ª—é—á–∞–µ–º –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã
                    continue
                try:
                    # –ü—Ä–æ–±—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                    json.dumps(v)
                    labeling_params[k] = v
                except (TypeError, ValueError):
                    # –ï—Å–ª–∏ –Ω–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    labeling_params[k] = str(v)

            results.append({
                'symbol': self.config.symbol,
                'timestamp': timestamp,
                'timeframe': self.config.timeframe,
                'reversal_label': 1 if sig['type'] == 'BUY' else 2,
                'reversal_confidence': sig['confidence'],
                'labeling_method': method_name,
                'labeling_params': json.dumps(labeling_params),
                'extreme_index': idx,
                'extreme_price': float(df['close'].iloc[idx]),
                'extreme_timestamp': extreme_timestamp,
                'confirmation_index': sig['confirmation_index'],
                'confirmation_timestamp': confirmation_timestamp,
                'price_change_after': pnl,
                'features_json': json.dumps(row_dict),
                'is_high_quality': 1 if is_profitable else 0
            })

        if results:
            self.save_to_db(results)
            profitable_count = sum(1 for r in results if r['is_high_quality'] == 1)
            total_count = len(results)
            print(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_count} —Å–∏–≥–Ω–∞–ª–æ–≤ ({profitable_count} –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö, {total_count - profitable_count} —É–±—ã—Ç–æ—á–Ω—ã—Ö)")
            return len(results)
        else:
            print(f"‚ùå –°–∏–≥–Ω–∞–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return 0

    def _run_auto(self, strategy_func, method_name: str):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return self._run_auto_with_method(strategy_func, method_name)

    def enhanced_main_menu(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        while True:
            print("\n" + "=" * 60)
            print("           ML LABELING TOOL v3 ‚Äî –†–ê–°–®–ò–†–ï–ù–ù–û–ï –ú–ï–ù–Æ")
            print("=" * 60)

            stats = self.data_loader.get_data_stats()
            print(
                f"üìä –°–∏–º–≤–æ–ª: {self.config.symbol} | –°–≤–µ—á–µ–π: {stats.get('total_candles', 'N/A')} | –ú–µ—Ç–æ–∫: {stats.get('total_labels', 'N/A')}")

            print("\nüéØ –†–µ–∂–∏–º—ã —Ä–∞–∑–º–µ—Ç–∫–∏:")
            print("[1] PELT Offline - –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä penalty (–∞–≤—Ç–æ)")
            print("[2] CUSUM (–∞–≤—Ç–æ)")
            print("[3] Min/Max —ç–∫—Å—Ç—Ä–µ–º—É–º—ã (–∞–≤—Ç–æ)")
            print("[4] CUSUM + Min/Max –≥–∏–±—Ä–∏–¥ (–∞–≤—Ç–æ)")
            print("[5] –†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞")

            print("\nüìà –ê–Ω–∞–ª–∏–∑ –∏ –∫–∞—á–µ—Å—Ç–≤–æ:")
            print("[6] –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞")
            print("[7] –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –º–µ—Ç–æ–∫")
            print("[8] –≠–∫—Å–ø–æ—Ä—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á")
            print("[9] –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è")
            print("[16] –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è PnL")
            print("[17] –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ PnL")

            print("\n‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏:")
            print("[11] –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            print("[13] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫")
            print("[14] –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            print("[18] –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç–æ–∫ (—Ç–µ–∫—É—â–∏–π —Å–∏–º–≤–æ–ª)")
            print("[19] –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç–æ–∫ –ø–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º")
            print("[20] –ü–æ–º–µ—Ç–∫–∞ —É–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∫–∞–∫ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤")
            print("[21] –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫")


            print("\n[0] –í—ã—Ö–æ–¥")

            choice = input("\n–í–∞—à –≤—ã–±–æ—Ä: ").strip()

            if choice == '1':
                self._run_auto(self._pelt_offline_reversals, "PELT_OFFLINE")
            elif choice == '2':
                self._run_auto(self._cusum_reversals, "CUSUM")
            elif choice == '3':
                self._run_auto(self._extremum_reversals, "EXTREMUM")
            elif choice == '4':
                self._run_auto(self._cusum_extremum_hybrid, "CUSUM_EXTREMUM")
            elif choice == '5':
                self.manual_mode()
            elif choice == '6':
                self.advanced_quality_analysis()
            elif choice == '7':
                self.detect_label_leakage()
            elif choice == '8':
                try:
                    print("\nüè∑Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á –≤ –ë–î (training_feature_importance)")
                    rid = input("run_id (–ø—É—Å—Ç–æ = –ø–æ—Å–ª–µ–¥–Ω–∏–π READY): ").strip() or None
                    model_name = input("–ò–º—è –º–æ–¥–µ–ª–∏ [unknown]: ").strip() or "unknown"
                    top_n_inp = input("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å top-N (–ø—É—Å—Ç–æ = –≤—Å–µ): ").strip()
                    top_n = int(top_n_inp) if top_n_inp else None

                    saved = self.export_feature_importance(run_id=rid, model_name=model_name, top_n=top_n)
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏: {saved}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '9':
                try:
                    n_splits = int(input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ [5]: ") or "5")
                    result = self.cross_validation_split(n_splits=n_splits)
                    if result:
                        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {n_splits} —Ñ–æ–ª–¥–æ–≤ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '11':
                self.configure_settings()
            elif choice == '13':
                self.show_stats()
            elif choice == '14':
                try:
                    print("\nüì¶ –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å–Ω–∞–ø—à–æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ë–î")
                    print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –≥–æ—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ –∏–∑ labeling_results")
                    print(f"   –ú–µ—Ç–æ–¥ —Ä–∞–∑–º–µ—Ç–∫–∏: {self.config.method}")
                    print(f"   –°–∏–º–≤–æ–ª: {self.config.symbol}, —Ç–∞–π–º—Ñ—Ä–µ–π–º: {self.config.timeframe}")

                    confirm = input("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ —Å–Ω–∞–ø—à–æ—Ç–∞? (y/N): ").strip().lower()
                    if confirm != 'y':
                        print("‚ùå –°–æ–∑–¥–∞–Ω–∏–µ —Å–Ω–∞–ø—à–æ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–æ")
                        continue

                    run_id = self.create_training_snapshot()
                    print(f"‚úÖ –°–Ω–∞–ø—à–æ—Ç —Å–æ–∑–¥–∞–Ω: run_id={run_id}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {err}")
            elif choice == '16':
                try:
                    method = input("–ú–µ—Ç–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ [–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ]: ").strip()
                    if method:
                        self.analyze_pnl_distribution(method=method)
                    else:
                        self.analyze_pnl_distribution()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '17':
                try:
                    self.quick_pnl_analysis()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '18':
                try:
                    self.clear_labeling_table()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '19':
                try:
                    self.clear_all_labeling_tables()
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '20':
                    try:
                        count = self.mark_unprofitable_ranges_as_negatives()
                        print(f"‚úÖ –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω—ã HOLD-–º–µ—Ç–∫–∏: {count}")
                    except Exception as err:
                        print(f"‚ùå –û—à–∏–±–∫–∞: {err}")
            elif choice == '21':
                try:
                    count = self.merge_conflicting_labels()
                    print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫: {count}")
                except Exception as err:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {err}")

            elif choice == '0':
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

    def _expand_hold_ranges(self, labels_df: pd.DataFrame, all_timestamps: set) -> dict:
        """
        –í–ï–†–°–ò–Ø –ë–ï–ó –†–ê–°–®–ò–†–ï–ù–ò–Ø HOLD.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º ts, –±–µ–∑ –∑–∞–ª–∏–≤–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–∫–æ–≤.
        –ù–∞–¥—ë–∂–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö –Ω–∞ –æ–¥–∏–Ω ts.
        """

        if labels_df is None or labels_df.empty:
            return {}

        # 1) —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º
        labels_df = labels_df.loc[:, ~labels_df.columns.duplicated()].copy()

        # 2) –Ω–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ 'ts' –∏ 'reversal_label' (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
        cols_lower = {c.lower(): c for c in labels_df.columns}
        ts_col = cols_lower.get('ts')
        lab_col = cols_lower.get('reversal_label')

        if ts_col is None or lab_col is None:
            self.logger.warning("–ù–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ 'ts' –∏/–∏–ª–∏ 'reversal_label' –≤ labels_df")
            return {}

        df = labels_df[[ts_col, lab_col]].copy()

        # 3) –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤—ã–º —Ç–∏–ø–∞–º; –≤—ã–±—Ä–æ—Å–∏—Ç—å NaN/–Ω–µ—á–∏—Å–ª–æ–≤—ã–µ
        df[ts_col] = pd.to_numeric(df[ts_col], errors='coerce')
        df[lab_col] = pd.to_numeric(df[lab_col], errors='coerce')
        df = df.dropna(subset=[ts_col, lab_col])

        # 4) –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ int (–ø–æ—Å–ª–µ dropna –±–µ–∑–æ–ø–∞—Å–Ω–æ)
        df[ts_col] = df[ts_col].astype(np.int64)
        df[lab_col] = df[lab_col].astype(np.int64)

        # 5) –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ ts, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ all_timestamps (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã)
        if all_timestamps:
            # all_timestamps –º–æ–∂–µ—Ç –±—ã—Ç—å set(int) ‚Äî –æ–∫
            df = df[df[ts_col].isin(all_timestamps)]

        if df.empty:
            return {}

        # 6) –µ—Å–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É ts –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–∫ ‚Äî –±–µ—Ä—ë–º –ü–û–°–õ–ï–î–ù–Æ–Æ (–ø–æ –ø–æ—Ä—è–¥–∫—É —Å—Ç—Ä–æ–∫)
        # (–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–≤—É—é ‚Äî –∑–∞–º–µ–Ω–∏—Ç–µ .last() –Ω–∞ .first())
        df = df.groupby(ts_col, as_index=False)[lab_col].last()

        # 7) –≤–µ—Ä–Ω—É—Ç—å –∫–∞–∫ dict{ts:int -> label:int}
        return dict(zip(df[ts_col].tolist(), df[lab_col].tolist()))

    def _build_training_snapshot_dataframe(self):
        import pandas as pd
        from sqlalchemy import text
        market_df = self.data_loader.load_indicators()
        if market_df is None or market_df.empty:
            raise RuntimeError("–ü—É—Å—Ç—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        if "ts" not in market_df.columns:
            raise RuntimeError("–í market_df –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'ts'")
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(market_df)} —Å–≤–µ—á–µ–π —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
        with self.engine.begin() as conn:
            rows = conn.execute(text("""
                SELECT extreme_timestamp AS ts, reversal_label
                FROM labeling_results
                WHERE symbol=:symbol AND timeframe=:timeframe AND reversal_label IN (0,1,2)
                ORDER BY extreme_timestamp
            """), {"symbol": self.config.symbol, "timeframe": self.config.timeframe}).fetchall()
        if not rows:
            raise RuntimeError("–ù–µ—Ç –º–µ—Ç–æ–∫ –≤ labeling_results")
        labels_df = pd.DataFrame(rows, columns=["ts", "reversal_label"])
        logger.info(
            f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(labels_df)} –º–µ—Ç–æ–∫ (HOLD: {(labels_df['reversal_label'] == 0).sum()}, BUY: {(labels_df['reversal_label'] == 1).sum()}, SELL: {(labels_df['reversal_label'] == 2).sum()})")
        all_timestamps = set(market_df['ts'].values)
        expanded_labels = self._expand_hold_ranges(labels_df, all_timestamps)
        hold_count = sum(1 for l in expanded_labels.values() if l == 0)
        logger.info(f"‚úÖ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è HOLD: {hold_count}")
        market_df['reversal_label'] = market_df['ts'].map(expanded_labels).fillna(-1).astype(int)
        market_df['reversal_label'] = market_df['reversal_label'].replace({-1: 0, 0: 0, 1: 1, 2: 2})
        class_counts_before = market_df['reversal_label'].value_counts().to_dict()
        logger.info(
            f"   –î–û downsample: NO_SIGNAL={class_counts_before.get(0, 0)}, BUY={class_counts_before.get(1, 0)}, SELL={class_counts_before.get(2, 0)}, HOLD={class_counts_before.get(3, 0)}")
        no_signal = market_df[market_df['reversal_label'] == 0]
        signals = market_df[market_df['reversal_label'] != 0]
        n_no_signal = 10000
        no_signal_sample = no_signal.sample(n=min(n_no_signal, len(no_signal)), random_state=42)
        logger.info(f"‚úÖ Downsample NO_SIGNAL: {len(no_signal)} ‚Üí {len(no_signal_sample)}")

        # –†–∞–∑–¥–µ–ª—è–µ–º signals –Ω–∞ HOLD –∏ BUY/SELL
        hold_signals = signals[signals['reversal_label'] == 3]
        trade_signals = signals[signals['reversal_label'].isin([1, 2])]

        # Downsample HOLD –¥–æ —Ä–∞–∑–º–µ—Ä–∞ BUY+SELL
        n_hold = len(trade_signals)
        hold_sample = hold_signals.sample(n=min(n_hold, len(hold_signals)), random_state=42) if len(
            hold_signals) > 0 else hold_signals
        logger.info(f"‚úÖ Downsample HOLD: {len(hold_signals)} ‚Üí {len(hold_sample)}")

        dataset_df = pd.concat([no_signal_sample, hold_sample, trade_signals], ignore_index=True).sort_values(
            'ts').reset_index(drop=True)
        class_counts_after = dataset_df['reversal_label'].value_counts().to_dict()
        total = len(dataset_df)
        logger.info(
            f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {total} (NO_SIGNAL={class_counts_after.get(0, 0)}, BUY={class_counts_after.get(1, 0)}, SELL={class_counts_after.get(2, 0)}, HOLD={class_counts_after.get(3, 0)})")
        max_count = max(class_counts_after.values())
        weights_map = {label: max_count / class_counts_after.get(label, 1) for label in [0, 1, 2]}
        dataset_df['sample_weight'] = dataset_df['reversal_label'].map(weights_map)
        allowed_columns = ['ts', 'reversal_label', 'sample_weight', 'datetime', 'cmo_14', 'volume', 'trend_acceleration_ema7',
                           'regime_volatility', 'bb_width', 'adx_14', 'plus_di_14','minus_di_14', 'atr_14_normalized',
                           'volume_ratio_ema3', 'candle_relative_body', 'upper_shadow_ratio', 'lower_shadow_ratio',
                           'price_vs_vwap', 'bb_position', 'cusum_1m_recent', 'cusum_1m_quality_score',
                           'cusum_1m_trend_aligned', 'cusum_1m_price_move', 'is_trend_pattern_1m',
                           'body_to_range_ratio_1m', 'close_position_in_range_1m']
        dataset_df = dataset_df[[c for c in allowed_columns if c in dataset_df.columns]]
        dataset_df['symbol'] = self.config.symbol
        dataset_df['run_id'] = None
        dataset_df['timeframe'] = self.config.timeframe
        dataset_df['created_at'] = None
        meta_info = {
            "class_dist": {"no_signal": int(class_counts_after.get(0, 0)), "buy": int(class_counts_after.get(1, 0)),
                           "sell": int(class_counts_after.get(2, 0)), "hold": int(class_counts_after.get(3, 0)),
                           "total": total}, "buffer_bars": getattr(self.config, "buffer_bars", None),
            "seed": getattr(self.config, "seed", None),
            "config_json": {"method": self.config.method, "timeframe": self.config.timeframe,
                            "symbol": self.config.symbol}, "issues": {}}
        return dataset_df, meta_info

    def create_training_snapshot(self, run_id: str | None = None) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ë–î-—Å–Ω–∞–ø—à–æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
          - –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü snapshot (DDL)
          - –ø–∏—à–µ—Ç meta(status=CREATING)
          - —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ–∑–∏—Ç–∏–≤—ã/–Ω–µ–≥–∞—Ç–∏–≤—ã, anti_trade_mask, sample_weight)
          - –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ
          - –ø–∏—à–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ training_dataset –∏ –∞–≥—Ä–µ–≥–∞—Ç—ã –≤ training_dataset_meta
          - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç meta(status=READY)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç run_id.
        """

        self._ensure_training_snapshot_tables()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ run_id
        created_at = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")
        if not run_id:
            payload = {
                "symbol": self.config.symbol,
                "timeframe": self.config.timeframe,
                "method": self.config.method,
            }
            sh = hashlib.sha1(json.dumps(payload, sort_keys=True).encode("utf-8")).hexdigest()[:8]
            stamp = datetime.now(UTC).strftime("%Y%m%d_%H%M")
            run_id = f"{self.config.symbol.replace('/', '_')}_{self.config.timeframe}_{stamp}_{sh}"
        # META: status=CREATING
        meta_defaults = {
            "run_id": run_id,
            "status": "CREATING",
            "error_msg": None,
            "symbol": self.config.symbol,
            "timeframe": self.config.timeframe,
            "created_at": created_at
        }
        with self.engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO training_dataset_meta(run_id,status,error_msg,symbol,timeframe,created_at)
                VALUES (:run_id,:status,:error_msg,:symbol,:timeframe,:created_at)
                ON CONFLICT(run_id) DO UPDATE SET status=excluded.status, error_msg=NULL
            """), meta_defaults)

        try:
            # 1) –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä)
            dataset_df, meta_info = self._build_training_snapshot_dataframe()

            # 2) –í–∞–ª–∏–¥–∞—Ü–∏—è/–æ—á–∏—Å—Ç–∫–∞
            try:
                df_clean, issues, nan_drop_rows, duplicates_count = self._validate_snapshot_frame(dataset_df)
            except Exception as e:
                logger.info("Snapshot validation failed: %s", e)
                raise

            # 3) –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –∏ –º–µ—Ç—Ä–∏–∫–∏
            range_start_ts = int(df_clean["ts"].min()) if len(df_clean) else None
            range_end_ts = int(df_clean["ts"].max()) if len(df_clean) else None

            if "anti_trade_mask" in df_clean.columns and len(df_clean) > 0:
                try:
                    issues["anti_trade_coverage"] = float(df_clean["anti_trade_mask"].mean())
                except Exception:
                    pass

            rows_total = int(len(df_clean))
            pos_count = int((df_clean["reversal_label"].isin([1, 2])).sum()) if rows_total else 0
            hold_bars = int((df_clean["reversal_label"] == 0).sum()) if rows_total else 0
            neg_count = 0  # –ø–æ –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–µ –Ω–µ–≥–∞—Ç–∏–≤—ã –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º

            # 4) –ó–∞–ø–∏—Å—å —Å—Ç—Ä–æ–∫ snapshot
            df_clean = df_clean.copy()
            df_clean["run_id"] = run_id
            df_clean["symbol"] = self.config.symbol
            df_clean["timeframe"] = self.config.timeframe
            df_clean["created_at"] = created_at

            # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (29 –∫–æ–ª–æ–Ω–æ–∫)
            required_columns = [
                'run_id', 'symbol', 'timeframe', 'ts', 'datetime', 'reversal_label', 'sample_weight',
                'cmo_14','volume', 'trend_acceleration_ema7', 'regime_volatility', 'bb_width', 'adx_14',
                'plus_di_14', 'minus_di_14', 'atr_14_normalized', 'volume_ratio_ema3', 'candle_relative_body',
                'upper_shadow_ratio', 'lower_shadow_ratio', 'price_vs_vwap', 'bb_position',
                'cusum_1m_recent', 'cusum_1m_quality_score', 'cusum_1m_trend_aligned',
                'cusum_1m_price_move', 'is_trend_pattern_1m', 'body_to_range_ratio_1m',
                'close_position_in_range_1m', 'created_at'
            ]

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ DDL
            final_columns = [col for col in required_columns if col in df_clean.columns]
            df_for_db = df_clean[final_columns].copy()

            # batch insert
            df_for_db.to_sql("training_dataset", self.engine, if_exists="append", index=False)

            # 5) –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ META (READY) ‚Äî –±–µ–∑ featureset/source_hash/feature_names
            meta_payload = {
                "run_id": run_id,
                "status": "READY",
                "error_msg": None,
                "rows_total": rows_total,
                "pos_count": pos_count,
                "neg_count": neg_count,
                "hold_bars": hold_bars,
                "class_dist_json": json.dumps(meta_info.get("class_dist", {})),
                "buffer_bars": meta_info.get("buffer_bars"),
                "seed": meta_info.get("seed"),
                "labeling_method": self.config.method,
                "config_json": json.dumps(meta_info.get("config_json", {})),
                "nan_drop_rows": int(nan_drop_rows),
                "issues_json": json.dumps(issues, ensure_ascii=False),
                "range_start_ts": range_start_ts,
                "range_end_ts": range_end_ts,
            }
            with self.engine.begin() as conn:
                conn.execute(text("""
                    UPDATE training_dataset_meta
                       SET status=:status,
                           error_msg=:error_msg,
                           rows_total=:rows_total,
                           pos_count=:pos_count,
                           neg_count=:neg_count,
                           class_dist_json=:class_dist_json,
                           hold_bars=:hold_bars,
                           buffer_bars=:buffer_bars,
                           seed=:seed,
                           labeling_method=:labeling_method,
                           config_json=:config_json,
                           nan_drop_rows=:nan_drop_rows,
                           issues_json=:issues_json,
                           range_start_ts=:range_start_ts,
                           range_end_ts=:range_end_ts
                     WHERE run_id=:run_id
                """), meta_payload)

            logger.info(
                "‚úÖ Snapshot READY | run_id=%s | rows=%s | pos=%s | neg=%s | anti_trade=%s | range=[%s..%s] | nan_drop=%s",
                run_id, rows_total, pos_count, neg_count,
                (f"{issues.get('anti_trade_coverage'):.4f}" if isinstance(issues.get("anti_trade_coverage"),
                                                                          float) else "N/A"),
                range_start_ts, range_end_ts, nan_drop_rows
            )
            return run_id

        except Exception as e:
            err = str(e)
            logger.info("‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–Ω–∞–ø—à–æ—Ç–∞: %s", err)
            with self.engine.begin() as conn:
                conn.execute(text("""
                    UPDATE training_dataset_meta
                       SET status='FAILED', error_msg=:err
                     WHERE run_id=:run_id
                """), {"run_id": run_id, "err": err})
            raise

    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        try:
            if hasattr(self, "data_loader") and hasattr(self.data_loader, "db_engine"):
                self.data_loader.db_engine.dispose()
                logger.info("üîå SQLAlchemy engine –∑–∞–∫—Ä—ã—Ç.")
            if hasattr(self, "engine"):
                self.engine.dispose()
                logger.info("üßπ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ.")
        except Exception as err:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {err}")


# === –ó–ê–ü–£–°–ö ===
if __name__ == '__main__':
    tool = None
    try:
        # –ò–°–ü–û–õ–¨–ó–£–ï–ú –ù–ê–°–¢–†–û–ô–ö–ò –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ –ò–ó LabelingConfig
        # –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        config = LabelingConfig(
            symbol="ETHUSDT",
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ defaults –∫–ª–∞—Å—Å–∞ LabelingConfig
        )

        tool = AdvancedLabelingTool(config)
        tool.enhanced_main_menu()

    except KeyboardInterrupt:
        print("\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(0)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

        logger.error(f"–î–µ—Ç–∞–ª–∏: {traceback.format_exc()}")
        sys.exit(1)
    finally:
        if tool is not None:
            tool.close()